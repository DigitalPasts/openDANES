[
  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "How to Annotate Cuneiform Texts",
      "authors" : "Matthew Ong",
      "tags"     : "cuneiform, annotation, linguistics",
      "doi"       : "",
      "submit-date" : "2023-10-02",
      "publish-date" : "2023-10-06",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/openDANES/pages/How%20to%20Annotate%20Cuneiform%20Texts.html",
      "type"     : "tutorial",
      "summary"     : "A high level overview of how to annotate a cuneiform text for linguistic content.",
      "content"     : "Matthew Ong                                                                                                      1                                                                                                                                                                                                                   1 UC Berkeley                      Submitted on: October 02, 2023      Published on: October 06, 2023                                    Under peer review                                          Summary: A high level overview of how to annotate a cuneiform text for linguistic content.                                      #cuneiform                  #annotation                  #linguistics                                              Difficulty level: beginner                    Lesson OverviewThis tutorial presents a high level overview of how to annotate acuneiform text for linguistic content. Exactly what we mean by‘annotation’ will be discussed shortly. The tutorial does not focus on aparticular language or a particular genre, and instead deals withgeneral features of the cuneiform writing system that a person will needto deal with when annotating. A number of applications of annotatingcuneiform texts will also be discussed.Structure of a cuneiform text Cuneiform is a writing system developed in southernMesopotamia towards the endof the fourth millennium BCE. It consists of wedge-like signs made uponclay tablets (and other material like stone or metal), where aparticular sign can represent an entire word (i.e. alogogram or a syllable such asba, ak, or ram). Some cuneiform signs can also serve as adeterminative (alsoknown as a classifier), i.e. a special unspoken marker telling thereader what class of objects the following word belongs to. Thus thereare determinatives indicating that the following word is a kind ofperson, a human settlement, a river, an object made out of wood, or akind of drink. Note that the cuneiform writing system was used toexpress several spoken languages of the ancient Middle East, includingSumerian,Akkadian,Hittite,Elamite, andHurrian.Because it is sometimes difficult or unnecessary to represent the exactvisual appearance of a cuneiform table in modern print media (say, viaphotographs or hand drawings), we often represent the written contentusing the western alphabet in what is known as transliteration.Transliteration tells the reader either what a particular sign on thetablet ‘is’ (i.e. what it looks like), how to pronounce the sign incontext (sometimes called the reading of the sign), or both. When a signis transcribed using capital letters, it means we are only commenting onwhat the sign looks like. When lower case letters are used, it indicateshow the sign is to be pronounced. If we write within square or curlybrackets, it means the sign is either broken or a determinative,respectively.For example, the sign transcribed as KUR looks like three trianglewedges arranged in the shape of a mountain. But this sign can bepronounced different ways depending on context. It can be read ‘mat’,‘kur’, ‘nat’, and ‘šad’, among other things. When we write {KUR}, wemean that the KUR sign functions as a determinative signaling that theword following it is a type of land or mountainous area.Drawing of the KUR signWe usually use dashes between transliterated signs to indicate theybelong to the same word. Thus the sequence a-me-el šar-ri representstwo words, and in Akkadian it means ‘man of the king’.Finally, note that when we want to express the exact pronunciation ofthe words in a cuneiform text, we take the transliteration and convertit to normalization. This format also uses the Latin alphabet, but itdispenses with the dash signs telling us which signs are used to write aword, and instead just gives us the word itself. Using the aboveexample, the normalization of a-me-el šar-ri would be amēl šarri. Itis a reconstructed linguistic form, based on writing standards andscholarly conventions. Thus, for example, the macron sign above thevowel ‘e’ indicates this vowel is long.The Purpose of AnnotatingAnnotation is when a human editor adds notes or other markers to a textto indicate additional linguistic information about it, whether thatdeals with semantics (i.e. meaning), morphology (the structure ofwords), syntax (the structure of phrases and sentences), or phonology(the sound of words). Generally it is understood that the editorprovides the same level of information for all the words in a text andnot just a few.Providing annotations for a cuneiform text serves a number of uses, suchas      Providing linguistic data for machinelearning algorithmsseeking to model properties of the underlying language.        Illustrating for students of the language how to grammaticallyanalyze a text, or as a check against the students’ own analyses.        Providing empirical data to a researcher who wants to ask questionsinvolving the systematic recording of all the linguistic data in alarge number of texts, or questions involving linguistic patternsthat emerge only upon processing the data by a machine.        Giving the annotator themselves an opportunity to work through acorpus in detail, much like an editor does when preparing a newprint edition of a text.  The Stages of AnnotatingPreprocessing the Raw TextAnnotating a cuneiform text generally requires that it first beavailable in transliteration or normalization (depending on the specificpurposes of the annotator). In what follows we will speak only oftransliterations, with the understanding that we mean bothtransliterations and normalizations unless otherwise noted. If acuneiform text has been edited in a print journal or other scholarlypublication, it will come with a transliteration. Increasingly,transliterations of cuneiform texts are also available online as part ofa digital database. Sometimes the database simply displays the text toyou in your browser (i.e. as part of an HTML file) and you will have toscrape the transliteration from thesite using a variety of data processing tools. But often times thedatabase also allows you to download the transliteration in a text file.In fact, you should check if the database has their entire collection oftransliterations available for download as a ZIP archive (see OpenDANESresourcesfor downloadable datasets), as annotation is usually a process appliedto an entire group of texts rather than just one.Here is an example of transliteration and metadata for a thirdmillennium BCE Sumerian economic text (DC II/2, pl. 41 no. 5), as foundin the Database of Neo-Sumerian Texts(BDTNS).The basic file format needed for the annotation is a plain text filewith only the transliteration (hence no HTLM markup or other meta-data).You need to make sure that each word in the text (which may includedeterminatives) is separated by spaces from the preceding and followingword, which allows the annotation program to split up the text file intoword-size units. This important preliminary step is calledtokenization, and the broken-up items in the text are called tokens.Note that depending on your purposes, you may choose to reduce oreliminate symbols in the text denoting unreadable signs (e.g. the symbol‘x’) or other comments indicating layout of the text and tablet (such as‘rest of tablet broken’).An example of how your transliteration should look is given below. Thetext SAA 5, 114 is an Akkadian letterfrom the archives of Sargon II (r. 722-705 BCE), and was publishedonline with other texts from the Neo-Assyrian royal archives as anOracc project. A normalized version ofthe text is also given for comparison:  a-na LUGAL EN-ia ARAD-ka {m}gab-bu-ana-aš-šur {KUR}u₂-ra-ar-ṭa-a.ae-mu-qe-šu₂ ina {KUR}u₂-a-za-na up-tah-hi-ir o be₂-et pa-ni-šu₂-ni laaš₂-me {m}me-la-ar-ṭu-a {LU₂}A-šu₂ {m}a-ba-li-u₂-qu-nu {LU₂}EN.NAM ša{KUR}x x+x-pa a-di e-mu-qe-šu₂-nu x x+x x x x x x x x x KUR-MEŠi-za-qu-pu LUGAL be-li₂ lu-u u₂-da LUGAL EN lu-u la i-qab-bi ma-a ki-itaš-mu-u-ni ma-a a-ta-a la taš-pu-ra  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazanauptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša{KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupušarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lātašpuraNote in this example that we have removed all special marks and commentsdescribing breakage in the tablet or reconstructed parts of the text.This makes later stages of the annotation and machine learning processsimpler. We should understand, however, that this is a choice we need toexplain when presenting the results of our annotations. If your projectneeds treat these features of online editions differently, you may haveto adjust later stages of your workflow accordingly.If you are annotating multiple cuneiform texts, a good practice to haveeach text in its own file, where the file is titled in a way that you(or the computer, if using a processing script) can easily identify whatthe text is. This may mean using numerical indices in the file nameswhose interpretation you record in a separate list. Alternatively, youcan often put all of the transliterations in a single file, separated byempty lines and special comments identifying the text. Each approach hasits advantages depending on your purpose for annotating and the toolsyou use.Making the AnnotationsTo create the annotation meta-data for a transliteration you need aprogram that will allow you to view the text and add special symbols andnotation to it. By now there are a number of free programs aimed athumanists seeking to add all sorts of meta-data to digitized texts,freely available for download or use online as a web application. Someof these programs are mainly used for highlighting thematic relationsbetween passages or phrases in a text or for connecting entitiesmentioned in the text to an external specialized vocabulary such as adatabase of maps or biographies. In our case (doing linguisticannotations), you want to make sure your tool can annotate lemmas,syntactic dependencies, and morphological features at the minimum.Hopefully it will also allow you to create your own set of labels aspart of a custom semantic annotation framework. The first three itemsare the bread and butter of what basic linguistic annotation consistsof, and are briefly explained below.Lemmas, roughly speaking, are the basic stems of words which remainafter one removes prefixes and suffixes used to denote things likeplurality and grammatical case (in the case of nouns) or person andtense (in the case of verbs). In English, the lemma of cats is catbecause we removed the -s ending signalling plurality. For similarreasons, the lemma of the verb liked is like. Note that in generalwe are not seeking to do a full etymological reduction of a word to abasic root (such as prevention -&amp;gt; ‘vent’ or skyscraper -&amp;gt;scrape), but rather seek the basic form of a word abstracted from allthe ways it can appear in different grammatical contexts while stillbeing regarded as the ‘same’ word. Thus the lemma of a noun abstractsaway whether the noun is singular or plural, or the subject or object ofa verb, and the lemma of a verb abstracts away whether that verb appearsin present or past tense, or the first person or third, or as animperative or an indicative form, etc.This admittedly vague definition of a lemma reflects its primary use asa lexicographic tool (allowing a human or computer to group disparateword forms under a single heading) rather than a precise morphologicalone. A certain amount of language-specific convention is involved, andif you are unsure how to lemmatize the items appearing in your cuneiformtexts, you should consult other annotation projects in the relevantlanguage.Syntactic dependencies indicate grammatical dependencies among words andphrases such as subject and object of a verb, an adjective modifying anoun, or a conjunction connecting two full sentences. The term‘dependency’ is used because it signals an asymmetric relation betweentwo words or phrases, one of them the head and the other thedependent. One may think of annotating syntactic dependencies asessentially constructing a directed graph or tree over a sentence orlarger span of text, and a good annotation tool allows you to specifydependencies between tokens in a text by connecting them with arrows.Nevertheless, at the level of the annotation file the program usuallyencodes dependencies using numerical indices assigned to each token toindicate their position in the sentence. Thus each token not only getsan index telling you where it is in the sentence (first word in thesentence, or second word, or third word, etc.) but also a second indextelling you which other token in the sentence is its head.  You must take care when reading the annotation file that you know what such an index means, i.e. whether it signs a head token or dependent token!Note also that we have been vague about exactly what grammaticalframework one should use when creating syntactic dependencies. This isbecause there is more than one framework available, and even within oneframework, technical revisions are periodically made by experts whichentail changing the name. Because it was designed to reflectcross-linguistic commonalities in syntactic structure, the UniversalDependencies (UD) framework isfrequently used by annotators who wish their corpus to be compatiblewith other projects regardless of language. You should check what otherpeople annotating in your language are doing before deciding upon aframework.Graph showing syntactic dependencies in the Universal Dependencies (UD) format.Morphological feature labelling is the task of identifying certainmorphemes, or parts of a word, conveying specific grammatical andsemantic information. The goal is to itemize aspects of the shape of theword with what it means. Annotators generally think in terms offeature-value pairs, whereby a feature is a certain category such asgrammatical number or gender, verb tense, or definiteness, and a valueis what variant of the category the word expresses via a particularmorpheme. Thus in the English word cats, the plural suffix -s signalsthe value ‘plural’ within the category of grammatical number. Inliked, the -d suffix indicates the value ‘past’ in the category oftense. When one uses an annotation tool, feature-value pairs are oftenencoded via a specific label set involving an equal sign, such as‘Gender=Masculine’ or ‘Tense=Past’, or simply a plus sign (e.g. ‘+Past’means a verb in the past tense). Nevertheless, in the context oflanguages written in cuneiform, there are still no exact conventions forhow to represent these features and their values in a label set forannotation. Thus if you wish your annotation scheme to be compatiblewith other projects, you should check to see what they use and matchtheir schema before beginning annotation.With these things in mind, an annotation program with a graphicalinterface is practically essential for doing annotations efficiently.One currently popular program isInception, a downloadable Javaapplet that works offline through your internet browser. Besidesallowing both syntactic and morphological annotation, Inception alsoallows semantic role labelling and user-defined label schemes which canbe exported in various formats. Other tools you may consider areBrat,WebAnno (which is essentially aweb-based version of Inception), andWebLicht,another online annotating platform more geared towards well-documentedlanguages like English. All of these programs are easy to set-up anduse, though you should note the data formats they use for input andoutput, as this may affect other parts of your work-flow.Issues to Consider When AnnotatingIf you have not previously annotated a cuneiform text in the specificlanguage at hand, it would be good to check how others have annotated inthat language, both in the hopes of making your work compatible withtheirs, as well as becoming familiar with the difficulties they faced inapplying their annotation scheme. Unless you are highly experienced withthe language using modern linguistic categories, you may find that yourinitial label set for morphological features turns out to beinefficient, or your understanding of a certain syntactic structureturns out to be wrong (or at least problematic). Looking at what othershave done may save you time and effort in the long run. At the sametime, no annotation scheme currently used for a cuneiform language fitsfor the grammatical particularities of that language perfectly, whetherat the level of theoretical description or practical implementation.Depending on the type of text you are working with, you may finddeviating from the conventions of other annotators or even needing toinvent conventions yourself. What is most important is that you havereasons you can cite for what you do and that you explain those reasonssomewhere in the documentation that accompanies your work. Not only willthis make it easier for others to understand what you did, it will alsohelp you to remember your own policies and be consistent in your work.Here are some examples of projects that use linguistic annotations in different languages written in cuneiform: on Anatolian languages like Luwian and Hittite see the eDiAna project and Hittite Festival Rituals, respectively; for Akkadian, Sumerian, Persian and Urartian see the ORACC lemmatization guidlines, and for Sumerian specifically see the ETCSL project conventions.A second issue is whether to approach annotation sub-task by sub-task ordocument by document. If you intend to annotate many documents forseveral features (e.g. syntactic dependencies and morphologicaldependencies and lemmatization), you can either annotate all of thedocuments for a single feature first, and then go back and go over thedocuments for the second feature, etc., or you can do all of thefeatures for a single document while it is still in front of you beforegoing on to the second document, etc. Depending on the tools andknowledge you bring to your work, one method may be more efficient thanthe other. If the syntactic dependencies of a corpus are particularlydifficult to do, you may opt to first go through and do the morphologybefore tackling the syntax alone. If you are working with someone whodoes not know the grammar of the language well but can find the lemmasfor all the tokens in the text on their own, you could delegate the taskof lemmatization to them while you do the dependencies and morphology.While these kinds of considerations are relevant to annotating manysorts of languages beyond cuneiform ones, in the case of cuneiformlanguages there is one thing to keep in mind. Unless the type of textyou are annotating is particularly simple morphosyntactically, it willlikely take you some effort to understand what the text is saying evenin transliteration (or normalization). Thus going over the text multipletimes for different annotation tasks may be less efficient that doingeverything at once while the meaning of the text is still fresh in yourmind.Finally, be aware of accompanying datasets or machine learning toolsthat can accelerate your work. If your transliterations come from anonline database, they may also come with lemmatization data as part of aJSON file or separate glossary. If you already had to extract the rawtext from such a JSON file, it is only a little more work to grab thelemmas that go with the tokens as well. Similarly, if you are makingannotations of a corpus to train a natural language processing model on it (say a syntactic parser or morphologizer), you can actually start training your model on the portion of the corpus you have annotated early on and thenapply its predictions to the rest of the corpus. Going through andcorrecting the model’s predictions is often faster than going throughthe whole raw corpus unaided. This technique, known as bootstrapping,is particularly effective when repeated multiple times, early on, for alarge corpus.Choosing How to Present ResultsUnless you are annotating cuneiform texts for private purposes and donot want to share you data, you should consider how to share your workwith others on the internet. GitHub is an online platformmainly designed for people writing code they wish to share with othersin a controlled, version-specific way. As an annotator, you can use itas a convenient place to store your data and any associated processingscripts, keeping your data project private, open to all, or only toinvited users. The system is designed to synchronize with particularfolders on your local computer, so that the process of backing up dataor uploading newer versions is easy and allows you to compare currentand older versions of files. GitHub is a good place to provide your dataif you envision yourself working on multiple projects in the future, orif you have scripts or other associated code that you need to presentalongside the annotations themselves.If you have annotated your texts in the Universal Dependencies format,you can also make your data available on the UDwebsite (which actually stores its data onGitHub) alongside annotated corpora from dozens of other languages. Youdo need to make sure your annotations conform to their formatspecifications. This website already features corpora of Akkadian,Biblical Hebrew, and Hittite.Final ObservationsWhile annotating itself has some immediate uses, it is often done aspart of a larger language processing task, research program, orpedagogical project. We mentioned some of these uses earlier on, amongwhich were applications to machine learning and language modeldevelopment. It is worth noting that many of the cutting-edge computeralgorithms and research efforts in natural language processing aregeared towards modern languages with large online data sets such asEnglish, even if they are not restricted to only those languages. Nocuneiform language comes close to the size of such corpora in terms ofraw text, let alone text that has been properly transliterated orotherwise made suitable for annotation. This means that the challengesof applying machine learning and developing language models forcuneiform languages are somewhat different from someone working onEnglish. There are fewer pre-existing annotated corpora that one can useto jump-start ones own annotations. Many natural language processingpackages assume that you are interested in working with a popular modernlanguage, and often come with large data sets from those languagesbuilt-in, without clearly explaining how to apply their code to alow-resource language starting from the ground up. Unless you are anexperienced programmer, you will likely find it very helpful to haveguidance from someone more experienced in natural language processing ordata science, or to look at some of the online proceedings from NLPworkshops aimed at humanists and specialists in less common languages.One recent workshop at Princetonillustrates how to develop an NLP project for new languages usingspacy. On a broader level, one may also considerthe Digital Humanities Summer Institute with itsvarious course offerings. What may be the most helpful in the beginning,however, is finding someone else who has begun an annotation project inyour language by searching GitHub and getting guidance from them."
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts - Challenges and Comparisons of Python and R Resources",
      "authors" : "chatGPT",
      "tags"     : "python, R, pre-processing",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-07",
      "publish-date" : "2023-04-07",
      "peer-review-date" : "2023-04-07",
      "update-date" : "",
      "url"      : "/openDANES/pages/paper1.html",
      "type"     : "white-paper",
      "summary"     : "This is an example paper created by chatGPT.",
      "content"     : "chatGPT                                                                                                      1                                                                                                                                                               1 openAI                      Submitted on: April 07, 2023      Published on: April 07, 2023              Peer reviewed: April 07, 2023                                  Reviewed by:         chatGPT                                                                                            doi                                                    Summary: This is an example paper created by chatGPT.                                      #python                  #R                  #pre-processing                        About this paperAll the text contained in this paper, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write an example white paper that will include all possible markdown syntax? (different heading types, tip prompts, images, etc.). This will be used as a template example.  The example white paper should be about current challenges in preprocessing ancient texts using available online resources, particularly comparing current resources in python versus current resources in R.  As this is a demo, please include jokes to make it funny and entertaining!Then, for the peer-review process, a new conversation with chatGPT was started where the following prompt was given:  I need you to perform an academic peer review for a white paper submitted to openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Please critically answer the following questions:      should the paper be published in openDANES?    are there any corrections you would suggest to the text, which would make it clearer or strengthen the points the author is making? Please write them out.    are the jokes funny enough? If not, can you improve them?  Then the original version of this paper was copy-pasted.ChatGPT provided the following answers:  Overall, the paper is well-written, informative, and engaging. It addresses an important and timely topic, and provides a useful comparison of Python and R resources for preprocessing ancient texts. Below are my answers to the three questions posed:  (1) Yes, the paper should be published in openDANES. It meets the platform’s criteria of providing tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  (2) There are no major corrections that I would suggest. However, I have a few minor suggestions that may strengthen the points the author is making:      The introduction could benefit from a more specific statement of the problem or research question that the paper addresses.    In the Python section, the author mentions sentiment analysis and questions its usefulness for ancient texts. It may be helpful to provide an example or explanation of why this is the case.    In the R section, the author could expand on the benefits of the tidytext and quanteda packages. What specific features or functionalities do these packages offer?    (3) The jokes are light-hearted and add to the overall tone of the paper. I don’t think they detract from the content, and therefore don’t need to be improved.  Overall, the paper is a valuable contribution to the field of ancient Near Eastern studies and computational methodologies.Then, we asked chatGPT to rewrite the paper with the suggested corrections. The text below is the result of that peer-review.The text has not been adapted whatsoever by a human. This is purely meant as an example of how a formatted paper looks like.Preprocessing Ancient Texts: Challenges and Comparisons of Python and R ResourcesIntroductionAs the field of ancient Near Eastern studies becomes increasingly reliant on computational methodologies, scholars are faced with the challenge of preprocessing ancient texts using available online resources. In this white paper, we will explore the current challenges of preprocessing ancient texts and compare the available resources in Python and R. Specifically, we aim to answer the question: which language is better for preprocessing ancient texts - Python or R?The Challenge of Preprocessing Ancient TextsPreprocessing ancient texts presents a unique set of challenges that are not encountered when working with modern texts. For one thing, ancient texts often come in a variety of languages, many of which are no longer spoken. Additionally, ancient texts may be damaged or incomplete, making them difficult to read or understand. And let’s not forget the challenge of deciphering ancient scripts!But fear not, intrepid scholars! With the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world.Python Resources for Preprocessing Ancient TextsPython has a wealth of resources available for preprocessing ancient texts, including the Natural Language Toolkit (NLTK) and the TextBlob library. These tools can be used to tokenize text and perform part-of-speech tagging. However, while Python also has the capability to perform sentiment analysis, it is unlikely to be useful for ancient texts since we have no way of knowing how the authors were feeling.Of course, as with any programming language, there are some quirks to be aware of. As the saying goes, “Python is the second-best language for everything.” But don’t let that discourage you - with a little perseverance, you can accomplish great things in Python.R Resources for Preprocessing Ancient TextsR is another popular language for preprocessing ancient texts, with resources like the tidytext package and the quanteda package. These tools can be used to perform a variety of text analysis tasks, such as word frequency analysis and topic modeling.The tidytext package offers functionality for performing sentiment analysis specifically for texts in English, which may be useful for some ancient texts written in English. The quanteda package provides a variety of text processing and analysis functionalities, including tokenization, part-of-speech tagging, and keyword extraction. It also offers support for many different languages and scripts.However, R has its own quirks and challenges. As one wise person once said, “R is like a drug. Once you start using it, you can’t stop.” So be careful when delving into the world of R - you may find yourself unable to escape!Comparing Python and R ResourcesAs we have seen, both Python and R offer valuable resources for preprocessing ancient texts. Which language is better for this task ultimately depends on your specific needs and preferences. Python may be more intuitive for those who are new to programming, while R may be better suited for those who are already familiar with statistics and data analysis.ConclusionIn conclusion, preprocessing ancient texts presents a unique set of challenges, but with the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world. Python and R both have their own strengths and quirks, so the best choice depends on your specific needs and preferences.And remember, when in doubt, just keep laughing - because as the great scholar Confucius once said, “Everything has beauty, but not everyone sees it.”"
    } ,

  

  

  

    
    

    {
      "title"    : "Challenges in OCR for Cuneiform Texts",
      "authors" : "Shai Gordin, Avital Romach",
      "tags"     : "cuneiform, OCR",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-16",
      "publish-date" : "2023-04-16",
      "peer-review-date" : "2023-04-16",
      "update-date" : "",
      "url"      : "/openDANES/pages/paper2.html",
      "type"     : "white-paper",
      "summary"     : "An overview of the current challenges in cuneiform OCR",
      "content"     : "Shai Gordin                                                                                                      1                                                                                                                                                                                   2                                                                                                                                                                                                                                                                     ,             Avital Romach                                                                                                                                                                                                                                                      2                                                                                                                       3                                                                                                                                                                                             1 Ariel University                      2 Digital Pasts Lab                      3 Yale University                      Submitted on: April 16, 2023      Published on: April 16, 2023              Peer reviewed: April 16, 2023                                  Reviewed by:         chatGPT                                                                                            doi                                                    Summary: An overview of the current challenges in cuneiform OCR                                      #cuneiform                  #OCR                        Optical Character Recognition (OCR) has made significant progress in recent years, but it still faces significant challenges when it comes to cuneiform texts. Cuneiform was the first system of writing developed by humans, used by ancient civilizations in Mesopotamia, and is a complex script with over 1,000 characters. In this paper, we will discuss the challenges of OCR for cuneiform texts, as well as some potential solutions.Character RecognitionCuneiform characters are typically composed of wedge-shaped marks impressed into clay tablets. OCR software faces difficulties in recognizing these characters due to the complexity of the shapes, which can have many variations. Additionally, the script has no clear distinction between upper and lowercase letters, making it difficult for OCR software to differentiate between them. The lack of standardization in character shape and size also poses a significant challenge for OCR.Contextual AnalysisCuneiform texts often contain ligatures, where two or more characters are merged into a single glyph. This merging can create ambiguity in the interpretation of individual characters, which can lead to errors in OCR. Contextual analysis is essential for resolving these ambiguities, but it can be challenging to implement in OCR software.Training DataOCR software relies heavily on training data to recognize characters accurately. However, obtaining a large dataset of cuneiform characters for training can be difficult due to the scarcity of cuneiform texts. Additionally, the variation in character shapes and sizes makes it challenging to create a comprehensive training set.SolutionsSeveral potential solutions have been proposed to address these challenges in OCR for cuneiform texts. One solution is to create specialized OCR software specifically for cuneiform texts, which can incorporate contextual analysis and machine learning algorithms trained on a small dataset of cuneiform characters. Another solution is to develop software that combines OCR with manual verification by experts in cuneiform texts."
    } ,

  

  

  

    
    

    {
      "title"    : "Advantages and Disadvantages of Stylometric Analysis for Cuneiform Texts",
      "authors" : "Avital Romach, Shai Gordin",
      "tags"     : "cuneiform, stylometry, python",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-16",
      "publish-date" : "2023-04-16",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/openDANES/pages/paper3.html",
      "type"     : "white-paper",
      "summary"     : "An opinion piece on what insights can be gained from stylometric analysis into texts written using the cuneiform writing system, and what insights cannot be gained.",
      "content"     : "Avital Romach                                                                                                      1                                                                                                                                                                                   2                                                                                                                                                                                                                                           ,             Shai Gordin                                                                                                                                                                                                                                                      2                                                                                                                       3                                                                                                                                                                                                                       1 Yale University                      2 Digital Pasts Lab                      3 Ariel University                      Submitted on: April 16, 2023      Published on: April 16, 2023                                    Under peer review                                          Summary: An opinion piece on what insights can be gained from stylometric analysis into texts written using the cuneiform writing system, and what insights cannot be gained.                                      #cuneiform                  #stylometry                  #python                        Stylometry is the study of an author’s writing style, and it has become an increasingly popular method for analyzing cuneiform texts. However, as with any method, stylometric analysis has its advantages and disadvantages. In this paper, we will discuss the benefits and drawbacks of using stylometric analysis for cuneiform texts.AdvantagesAuthor AttributionOne of the significant advantages of stylometric analysis is that it can be used to attribute authorship to texts. This is particularly useful in cuneiform studies, where many texts are anonymous or have uncertain authorship. By analyzing the style of writing, scholars can determine whether multiple texts were written by the same author or whether a specific text can be attributed to a particular author.Dating TextsStylometric analysis can also be used to estimate the dates of texts. Cuneiform texts were produced over a period of thousands of years, and dating them accurately can be challenging. However, by analyzing the style of writing, scholars can place texts into different time periods and gain insights into the evolution of cuneiform writing.Identification of Textual RelationshipsAnother advantage of stylometric analysis is that it can be used to identify relationships between texts. By analyzing the style of writing, scholars can determine whether two texts were written by the same author, or whether they were written by different authors with similar writing styles.DisadvantagesLimitations of the MethodStylometric analysis is not a perfect method for analyzing texts, and it has its limitations. For example, it assumes that an author’s writing style remains consistent over time, which may not always be the case. Additionally, it can be challenging to determine the appropriate statistical thresholds for identifying similarities and differences in writing styles.Limited DataStylometric analysis relies heavily on having a significant amount of data available for analysis. However, cuneiform texts are scarce, and it can be challenging to obtain enough texts to conduct meaningful stylometric analysis. This limitation can lead to inaccuracies in the analysis and limit the potential insights gained from the analysis.Difficulty of InterpretationFinally, stylometric analysis can be challenging to interpret, particularly when it comes to cuneiform texts. Cuneiform writing is complex, and its evolution over time can make it difficult to determine whether two texts were written by the same author or whether they have similar writing styles.ConclusionIn conclusion, stylometric analysis has many advantages when it comes to analyzing cuneiform texts, such as author attribution, dating texts, and identifying textual relationships. However, it also has limitations, such as the assumption of consistency in writing style, the need for significant amounts of data, and the difficulty of interpretation. As with any method, it is essential to consider the advantages and disadvantages of stylometric analysis carefully and to use it in conjunction with other analytical methods for a comprehensive understanding of cuneiform texts."
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)",
      "authors" : "chatGPT",
      "tags"     : "python, CLTK, ancient Greek",
      "doi"       : "",
      "submit-date" : "2023-04-06",
      "publish-date" : "2023-04-06",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/openDANES/pages/tutorial1.html",
      "type"     : "tutorial",
      "summary"     : "This is a demo tutorial created by chatGPT.",
      "content"     : "{%- include author_card.html authors=page.authors date=page.submit-date publish-date=page.publish-date peer-review-date=page.peer-review-date update-date=page.update-date tags=page.tags summary=page.summary reviewers=page.reviewers difficulty=page.difficulty doi=page.doi deprecated=page.deprecated -%}About this tutorialAll the text contained in this tutorial, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write a demo tutorial that will include all possible markdown syntax? (all heading types, code blocks, images, etc.). This will be used as a template example.  The demo tutorial should be about preprocessing ancient texts using the The Classical Language Toolkit (CLTK) python library.  As this is a demo, please include jokes to make it funny and entertaining!The code in this tutorial has not been validated and the text itself has not been edited only the link to the image displayed in the tutorial has been adapted to display correctly. This is purely meant as an example of how a formatted tutorial looks like.Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)Are you tired of manually preprocessing ancient texts for your research in Near Eastern studies? Fear not! The Classical Language Toolkit (CLTK) is here to help!What is CLTK?The CLTK is a Python library that provides natural language processing (NLP) tools for ancient languages, including Greek, Latin, Sanskrit, and Old English. With the CLTK, you can tokenize, lemmatize, and part-of-speech tag texts, as well as perform other common NLP tasks.Installing CLTKBefore we get started, make sure you have Python 3 installed. Then, you can install the CLTK using pip:pip install cltkLoading TextsOnce you have the CLTK installed, you can start loading texts. Let’s load the Iliad by Homer:import cltkfrom cltk.corpus.utils.importer import CorpusImportercorpus_importer = CorpusImporter(&#39;greek&#39;)corpus_importer.import_corpus(&#39;tlg&#39;) # import the TLG corpuscorpus_importer.import_corpus(&#39;phi5&#39;) # import the PHI5 corpusfrom cltk.corpus.readers import get_corpus_readerreader = get_corpus_reader(corpus_name=&#39;tlg&#39;, language=&#39;greek&#39;)iliad = reader.raw(&#39;tlg0012.tlg001&#39;) # load the IliadTokenizing TextsNow that we have the Iliad loaded, let’s tokenize it:from cltk.tokenize.word import WordTokenizertokenizer = WordTokenizer(&#39;greek&#39;)tokens = tokenizer.tokenize(iliad)Part-of-Speech TaggingWe can also perform part-of-speech tagging on the tokens:from cltk.tag.pos import POSTagtagger = POSTag(&#39;greek&#39;)tagged_tokens = tagger.tag_ngram_123_backoff(tokens)Lemmatizing TextsFinally, we can lemmatize the tokens:from cltk.lemmatize.backoff import BackoffGreekLemmatizerlemmatizer = BackoffGreekLemmatizer()lemmas = [lemmatizer.lemmatize(token) for token in tokens]And that’s it! With just a few lines of code, we’ve tokenized, part-of-speech tagged, and lemmatized an ancient Greek text. Now, go forth and preprocess with ease!{% include fig_legend.html content=”Figure 1: This is the logo of CLTK” %}  Why did the linguist break up with the CLTK? Because it kept tokenizing everything!Note: This demo tutorial used various markdown syntaxes including headers, code blocks, images, and blockquotes. Use these features to make your tutorials more organized and visually appealing. And don’t forget to add a touch of humor to keep your readers engaged!"
    } ,

  

  

  

    
    

    {
      "title"    : "Introduction to Linked Open Data for Ancient Near Eastern Studies",
      "authors" : "Shai Gordin",
      "tags"     : "LOD, ontology",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-16",
      "publish-date" : "2023-04-16",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/openDANES/pages/tutorial2.html",
      "type"     : "tutorial",
      "summary"     : "A tutorial introducing the main concepts and resources for applying linked open data to your databases and research pipelines.",
      "content"     : "{%- include author_card.html authors=page.authors date=page.submit-date publish-date=page.publish-date peer-review-date=page.peer-review-date update-date=page.update-date tags=page.tags summary=page.summary reviewers=page.reviewers difficulty=page.difficulty doi=page.doi deprecated=page.deprecated -%}Linked Open Data (LOD) is a method of publishing and interlinking structured data on the internet. In recent years, LOD has gained traction in the field of ancient Near Eastern studies, with several projects using LOD to connect disparate datasets and provide new insights into the ancient world. In this paper, we will provide an overview of LOD and its potential applications in ancient Near Eastern studies.What is Linked Open Data?Linked Open Data is a method of publishing data on the internet that allows for easy sharing and integration with other datasets. The key features of LOD are:  URI identification: Data is identified using a unique URI (Uniform Resource Identifier), allowing it to be easily referenced and linked to other datasets.  Structured data: Data is structured using RDF (Resource Description Framework) or other standards, allowing for easy parsing and integration with other datasets.  Open access: Data is made available under an open license, allowing for easy reuse and sharing.LOD allows for the creation of a “web of data,” where different datasets are linked together to create a more comprehensive view of a particular subject.Applications in Ancient Near Eastern StudiesLOD has many potential applications in ancient Near Eastern studies, including:Integration of DatasetsOne of the primary advantages of LOD is its ability to integrate datasets from different sources. For example, the Cuneiform Digital Library Initiative (CDLI) has created an LOD version of its database, allowing researchers to easily integrate CDLI data with other datasets, such as the Open Richly Annotated Cuneiform Corpus (ORACC)1. This integration allows for new insights into the ancient world by combining data from different sources.Visualization of DataLOD can also be used to create visualizations of data, allowing for easier analysis and understanding of complex datasets. For example, the Neo-Assyrian Text Corpus Project (NATCP) has created an LOD version of its corpus, which can be visualized using tools such as the Open Semantic Network Analysis Platform (OSNAP)2. This visualization allows researchers to see connections between texts and themes, providing new insights into the Neo-Assyrian Empire.Publication of DataFinally, LOD allows for the easy publication of data, making it accessible to a wider audience. For example, the OpenAIRE project has created an LOD version of the Open Access Digital Library of Information Science and Technology (OADLIST)3. This allows for easy integration of OADLIST data with other datasets, as well as easier access to OADLIST data for researchers and the general public.ConclusionIn conclusion, Linked Open Data has many potential applications in ancient Near Eastern studies, including integration of datasets, visualization of data, and publication of data. By creating a “web of data,” LOD allows for new insights into the ancient world and easier access to data for researchers and the general public.Footnotes:            S. Tinnefeld et al., “Integrating Cuneiform Texts and Images: The Cuneiform Digital Library Initiative and the Open Richly Annotated Cuneiform Corpus,” in Proceedings of the Digital Humanities Conference, pp. 411-414, 2016. &amp;#8617;              J. Dahl et al., “Semantic Networks for Ancient Assyria: The Neo-Assyrian Text Corpus Project and the Open Semantic Network Analysis Platform,” in Journal of Ancient Near Eastern History, vol. 3, pp. 65-89, 2016. &amp;#8617;              A. Ioannidis et al., “OpenAIRE LOD Services: Scholarly Communication Data as Linked Data,” in Proceedings of the International Conference on Dublin Core and Metadata Applications, pp. &amp;#8617;"
    } ,

  

  

  

    
    

    {
      "title"    : "Quick guide to the Ancient Language Processing Python Library",
      "authors" : "Avital Romach",
      "tags"     : "python, ALP, cuneiform",
      "doi"       : "",
      "submit-date" : "2023-04-16",
      "publish-date" : "2023-04-16",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/openDANES/pages/tutorial3.html",
      "type"     : "tutorial",
      "summary"     : "A tutorial introducing the main function currently available in the ALP python library and how to use them to preprocess and tokenize cuneiform texts.",
      "content"     : "{%- include author_card.html authors=page.authors date=page.submit-date publish-date=page.publish-date peer-review-date=page.peer-review-date update-date=page.update-date tags=page.tags summary=page.summary reviewers=page.reviewers difficulty=page.difficulty doi=page.doi deprecated=page.deprecated -%}The Ancient Language Processing (ALP) Python library provides a set of tools for processing and analyzing ancient texts. In this tutorial, we will introduce the main functions currently available in the ALP library and show how to use them to preprocess and tokenize cuneiform texts.InstallationTo install the ALP library, you can use pip:pip install alpPreprocessingThe first step in processing a cuneiform text is to preprocess it. This involves removing any unwanted characters or formatting, such as line breaks or page numbers. The preprocess function in the ALP library takes a string as input and returns a cleaned version of the string.import alptext = &quot;1. [...]\n2. [...]\n3. [...]&quot;clean_text = alp.preprocess(text)print(clean_text)Output:[...][...][...]TokenizationOnce the text has been preprocessed, the next step is to tokenize it into individual words or signs. The tokenize function in the ALP library takes a string as input and returns a list of tokens.import alptext = &quot;[...] [x] [...] [y]&quot;tokens = alp.tokenize(text)print(tokens)Output:[&#39;[...]&#39;, &#39;[x]&#39;, &#39;[...]&#39;, &#39;[y]&#39;]The tokenize function can also handle multi-line texts and remove duplicates:import alptext = &quot;1. [...] [x]\n2. [...] [y]\n3. [...] [x]&quot;tokens = alp.tokenize(text, remove_duplicates=True)print(tokens)Output:[&#39;[...]&#39;, &#39;[x]&#39;, &#39;[y]&#39;]Part-of-Speech TaggingThe ALP library also provides a function for part-of-speech (POS) tagging of cuneiform texts. The pos_tag function takes a list of tokens as input and returns a list of tuples, where each tuple contains a token and its corresponding POS tag.import alptokens = [&#39;[...]&#39;, &#39;[x]&#39;, &#39;[...]&#39;, &#39;[y]&#39;]pos_tags = alp.pos_tag(tokens)print(pos_tags)Output:[(&#39;[...]&#39;, &#39;X&#39;), (&#39;[x]&#39;, &#39;N&#39;), (&#39;[...]&#39;, &#39;X&#39;), (&#39;[y]&#39;, &#39;N&#39;)]ConclusionIn this tutorial, we have introduced the main functions available in the ALP Python library and shown how to use them to preprocess and tokenize cuneiform texts. The ALP library provides a set of tools for processing and analyzing ancient texts that can be used in various research projects."
    } ,

  

  

  
    {} ,

  

  

  
    {} 

  

  
]