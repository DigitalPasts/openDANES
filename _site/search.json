[
  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Digital Ancient Near Eastern Studies - A Transition to Arts and Crafts",
      "authors" : "Shai Gordin, Avital Romach, Eliese-Sophia Lincke, Hubert Mara, Aleksi Sahala, Marine Béranger",
      "tags"     : "opinion, open access, DANES network",
      "doi"       : "",
      "submit-date" : "2024-02-16",
      "publish-date" : "2024-02-16",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Digital%20Ancient%20Near%20Eastern%20Studies%20-%20A%20Transition%20to%20Arts%20and%20Crafts.html",
      "type"     : "white-paper",
      "summary"     : "A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.",
      "content"     : "Shai Gordin                                                                                                                      1                                                                                                                                                                                                           2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ;               Avital Romach                                                                                                                                                                                                                                                                                                                      3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ;               Eliese-Sophia Lincke                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                 ;               Hubert Mara                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      5                                                                                                                                                                                                                                                                             ;               Aleksi Sahala                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6                                                                                                                                                                             ;               Marine Béranger                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                                                                                                                                         1 Ariel University              ;                 2 Open University of Israel              ;                 3 Yale University              ;                 4 FU Berlin              ;                 5 MLU Halle-Wittenberg              ;                 6 University of Helsinki                                                      Submitted on: February 16, 2024          Published on: February 16, 2024                                                                          Under peer review                                          Summary: A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.                                      #opinion                  #open access                  #DANES network                          “The use of electronic data-processing devices for research in the fields of linguistics and philology is by now fairly common.” (Hans G. Güterbock, “The Hittite Computer Analysis Project”, from the 1967-1968 annual report of the Oriental Institute, currently the Institute for the Study of Ancient Cultures)Digital studies of the ancient Near East are not new. If anything, within the fields of assyriology, egyptology, and archaeology, one can find some of the pioneers embracing the advent of the computer age. Today, there is new interest in applying computational methods to the humanities and social sciences, and to the study of the ancient world specifically, given the potential of such interdisciplinary initiatives. However, there are questions that are fair to ask:  can the computer really teach us something new about our fragmentary, complicated texts and artifacts? Is this a movement of new arts and crafts, providing us with fresh perspectives, or are these arts and crafts a trending hobby, not really worth the scholarly effort?In order to apply computational methodologies, there is a need for shared language, interdisciplinary work, and a willingness to constantly learn outside of your field of specialty. Exploration of texts and artifacts as data, with the plethora of tools and techniques available today, is fundamental, yet figuring out where to start can be daunting. There is a need to design experiments, that might fail, and show no results except (bitter) experience. Even the significance of successful results are not always appreciated by your peers. Where computational analysis ends or should end and humanistic inference begins can at times be opaque. Lack of standards and resources for computational research in ancient Near Eastern (ANE) studies and neighboring disciplines makes the interpretation and significance of such work vague and unclear to the non-specialists.Let us give such an example. Below you can see graphs generated from the network of the Mesopotamian Ancient Placenames Almanac (MAPA); for further information see Gordin, Clark and Romach (2022) and Clark and Gordin (2023). The points on the graph are toponyms attested in texts from the hinterland of the city of Uruk in the first millennium BCE. Their position on the graph is a result of their Degree in the network—how central they are as hubs—and their Betweenness Centrality score—how important they are as a bridge to get from one location to another. Without minimal background in graph theory, it is hard to understand the value of such visualizations, and whether they add new information to our traditional ways of understanding connections and links between geographical locations. The transformation of the texts into data—from sentences to word lists, to toponyms represented in a network, to a graph of their scores—distances us from the artifacts, the original texts. These transformations are so substantial, it is the equivalent of looking at a sword and trying to understand how it was forged without seeing the process or knowing anything about the tools and methods used.In this piece, we argue that it is worthwhile to dedicate the time to learn these arts and crafts. We begin with what resources were created before us, where we stand today, and what we hope to gain in the future. We outline the initiatives of the Digital Ancient Near Eastern Studies (DANES) network, whose purpose is to increase the number of apprentices, by providing both basic literacy to understand computational methods, and advanced practical applications to those who want to become artisans in the field.FoundationsThe early pioneers created large databases which are the baselines for many computational works performed today, such as (in alphabetical order) the Coptic Scriptorium, Cuneiform Digital Library Initiative (CDLI), Hethitologie-Portal Mainz (HPM), the Open Richly Annotated Cuneiform Corpus (ORACC), Papyri.info, the Perseus digital library, and Thesaurus Linguae Aegyptiae (TLA), to name a few. Most of these efforts were placed at digitizing ancient sources. This was a colossal effort with the technology of the time, when storage space was limited, when punch-cards were frustrating, when non-Latin characters were not supported.Criticism was not lacking towards these revolutionary endeavors. Were digitization and computational studies worth the effort? Was new knowledge obtained that could not have been gathered otherwise? Such setbacks conceptually made it seem as though the computer revolution has not fundamentally touched upon the fields of the ancient Near East. There are still general misunderstandings regarding the inherent differences between print publications (which include PDF versions), and plain-text digital scholarly editions that are also published on online platforms (see Sahle 2016).From the early 2010s, the AI revolution reinvigorated the use of computational methods for the study of the ancient Near East. Breakthroughs in computer science, increased storage capabilities for big data, the development of the internet, the creation of programming languages and environments that are more user-friendly, and the extension of the Unicode standard, opened new lines of research.At the current moment, we think a conscious choice needs to be made on how we envision the field going forward. If we want to promote new methodologies, we cannot dismiss established ones. If we want to promote such new methodologies alongside established ones, computational research needs to be communicated in a way that is still understandable to all relevant fields. We need a shared language that includes basic literacy of terms, common problems and possible solutions (see e.g. Homburg et al. 2023). As basic digital literacy becomes more common, more scholars and students will desire to expand their knowledge, use digital tools, gain more programming experience, etc.This is particularly important as computational methods are shared across the growing field of digital humanities. A shared set of methodologies is a powerful link between fields, creating a shared jargon, and it is important that we propel the study of ancient cultures to current research avenues.Training in the PresentAll the above requires training—both of humans and machine learning algorithms. The barriers to computational studies today for those coming from the humanities, compared to the challenges of the 20th century, are very different. Yet, the big difference is that today it is easier to lower said barriers, due to the ease of sharing information, tools, and knowledge online. There are many digital humanities’ initiatives, platforms, and resources, to train and aid humanists to learn anything from basic digital literacy to developing advanced machine learning applications (such as the Programming Historian, The Digital Orientalist, or DARIAH-Campus). These general resources are invaluable, but sometimes they are insufficient when applying new methodologies on ancient Near Eastern data, either in the form of text, object, image, or landscape.Furthermore, current technologies, especially generative AI models, offer new possibilities. Large language models (LLMs) have transformed the ways in which we interact with texts only in the last few months. Many are wondering what is the place of humans in this future, now that generative models such as GPT and image generation models are revolutionizing the way we write and create. Barriers such as the knowledge of how to write code may seem obsolete, now that anyone can use chatGPT with natural language input to program. Nevertheless, that does not mean that anyone can write useful and functional code, or gain meaningful results.It is easy to trick or ask ChatGPT questions the model was not trained for, and receive incorrect results. When asked to translate a pivotal sentence in Hittite that was used for its identification as an Indo-European language, the translation is only partially correct, with many caveats elaborated upon by the model, such as the incorrect statement that Hittite and Latin are from two different language families!For humanists to be able to understand how to work with such tools, and gain meaningful results from AI models, digital literacy is a prerequisite. In the above example, knowing more about what language models are to begin with, what data they are trained on and how it affects the model’s results, allows humanists to judge their results accordingly and also teaches humanists how to give only reasonable tasks to models to begin with.On the other side of the coin, those with computational background are often not familiar enough with the intricacies of ancient cultures, languages, writing systems, and artifacts in order to design models that will provide historically sound results or meaningful conclusions. Conceptual understanding and structured ontologies are imperative for scholars from the humanities to better contribute to the methodological discussions in the interface between computational methods and humanistic approaches.Furthermore, despite the foundational works of the 20th century, we are still facing issues that are waiting for state-of-the-art innovations when studying ancient cultures computationally. We should update our best-practice methods for preserving and digitizing our objects of study. While the vast majority of image data are photographs and flatbed scans, the number of acquired and available 2D+ and 3D models is increasing. 3D models specifically provide precise representations that allow the exact measurements of minute details such as fingerprints, seals and damaged characters. Standardized high-resolution, high-contrast representations have also been shown to improve AI-based approaches to optical character recognition tasks (Stötzner et al. 2023). To take advantage of these new technological possibilities, 3D scanning needs to be put on the agenda, and its benefits for research and object preservation need to be clearly communicated to the wider research community.For fully transparent research and seamless workflow, these material remains need to be linked to metadata, or text editions if there is any writing on the objects. Linked open data (LOD) is a set of standards for linking data (be it text, artifact, metadata, and more) over the web in certain formats, following specific ontologies agreed upon by a community of experts. The use of LOD in ancient studies is growing (e.g. Pelagios network), and it requires consensus and discussions to keep the data interlinked.Digital monolingualism and the primacy of the Latin script have led to software development lagging behind a fundamental need for full Unicode and font support in order to include non-European scripts and languages. Natural language processing (NLP) models designed primarily for English and maybe some other European languages require adaptations when applied to ancient languages. The need for a lobby of ancient Near Eastern studies and other disciplines sharing these concerns is obvious (see for example the DHd Multilingual Digital Humanities working group), and ancient language processing (ALP) is becoming a growing initiative within the NLP and machine learning community (see e.g. Anderson et al. 2023, Bin and Gordin 2023, and the ML4AL workshop @ ACL 2024).In the next section, we elaborate on the work of the DANES community to furnish some of the prerequisites necessary for meaningful computational and collaborative research, and to promote research on some of the current issues presented above.Current and Future ActivitiesThe DANES network actively creates spaces for the above goals to become a reality. The efforts of the network, which is inclusive and welcoming to all interested scholars and students in relevant fields, are manifest through the following initiatives:ConferencesThe first annual DANES conference took place between 19th-21st of February 2023, organized by three institutions: the Digital Pasts Lab at Ariel University, the TAD AI and Data Science Center at Tel Aviv University, and the School of Computer Science and Engineering at the Hebrew University of Jerusalem. The conference covered many topics, including optical character recognition (OCR) for cuneiform documents, data visualization methods, critical discussion of data analysis and machine learning, computational stylistics, natural language processing models, linguistic annotation, networks, and new online tools and environments and their sustainability. One can view the talks of the first day on YouTube, read the abstracts on Zenodo, and view the conference posters on OpenDANES. The proceedings of the conference are going to be published as a special issue, iDANES, in the it - Information Technology journal.Just as important as the talks themselves, were the discussions in-between sessions, the discovery of shared problems, and arriving at joint solutions. These conversations led to more avenues of further development and collaboration. Such was the main purpose of the conference: not only giving a venue to present interdisciplinary research, but also a space for scholars that usually have few people around them that understand every aspect of their work.The conference included a Q&amp;amp;A session with some of the experts in the field, Eliese-Sophia Lincke, Niek Veldhuis, Hendrik Hameeuw, and Hubert Mara, who received questions from the audience and online participants on some of the current burning questions. Especially important were the round-table discussions, whose purpose was to establish the goals of the new DANES network, common points of contention, future areas of research, and what is needed for the network and this field to thrive in the next decade.DANES conferences are going to regularly occur every year. The exact date of the next conference is still in the works. The following conferences will continue to have a combination of lectures with active participation: hands-on workshops, round-tables, and Q&amp;amp;A sessions.Working groupsThe DANES working groups are usually once-per-month online meetings to discuss or implement a specific computational methodology. In between meetings, the DANES discord server has sub-channels for each group for further discussions. They are meant as an entry point for those who want to learn about computational methodologies and do not know where to start or whom to ask. They are usually led by at least two people from the network.In 2023, the OCR group, led by Hendrik Hameeuw and Eliese-Sophia Lincke, met to discuss current challenges. The ancient language processing (ALP) group, led by Katrien De Graef and Shai Gordin, had guest lectures of various experts in the field to discuss their ongoing projects; the lectures and the recordings are available. The Interoperability and annotation group, led by Adam Anderson and Timo Homburg, discussed the importance of linked open data and how to apply it.Specific topics for groups depend on the initiators. Anyone is welcome to lead a group! Current active groups include a continuation of the ALP group, this year focusing on how to create a framework of under-resourced ANE languages, with a case-study on Elamite. Further activities of the Pedagogy and Gamification group and OCR groups are planned for 2024—sign up for the mailing list or join the Discord for updates!Discord and mailing listThe channels of communication for members of the DANES network is through Discord and a mailing list. To join the mailing list or Discord, send an email to danes@listserv.dfn.de or digpasts@gmail.com. Discord is a communication platform that allows users to create and join communities, known as servers, where they can chat with text, voice, and video. It was originally designed for gamers to communicate while playing online games, but it has since expanded to cater to various interests and communities. The DANES discord server includes various channels for communication of the DANES working groups and general discussions. It is meant as the primary space for anyone in the community to ask questions, give updates, and generally have conversations on DANES and related fields and methodologies.The mailing list is the more formal communication channel of the community. It is used to send out the monthly newsletter and give updates on meetings of the DANES working groups.NewsletterThe mailing list sends out a monthly newsletter, at the beginning of each month, which includes updates on the world of DANES. The newsletter summarizes, while explicating jargon, important articles that were published in the past month, and highlights important previous publications related to DANES. It informs the DANES community of relevant conferences, both in the ANE field and the digital humanities and computer science fields. The purpose of the newsletter is to establish DANES in both the field of computer science and ANE studies, by developing joint language and terminology around important publications, and encourage network members to participate in conferences relating to both worlds.Furthermore, after the publication of the newsletter there is a regular happy-hour meeting on the DANES discord channel for members to discuss and chat about what’s new in the community.OpenDANES platformThe OpenDANES platform is an open access publication platform for pedagogical materials. It aims to create content that will be useful and informative to any who want to learn how to combine computational methods with ancient artifacts, regardless of their previous knowledge. It publishes tutorials, which provide step-by-step instructions on how to apply computational methodologies for beginner, intermediate, and advanced levels. White papers, such as this one, are published on the platform as well. Those can be anything from opinion pieces, updating the community on projects, introducing the community to general initiatives, or efforts of groups and individuals working on digital and computational studies of the ancient world. All contributions go through a peer-review process to ensure high-quality and usefulness to the community, as well as to acknowledge contributors’ academic work. Upon final publication, all contributions receive a DOI.Additionally, OpenDANES includes the DANES resources, a dataset which collects free, online resources which can aid the DANES community members. It is a constantly growing dataset, and anyone is invited to contribute resources! See the instructions on the DANES Resources page.Final NotesTime will tell how the efforts of the DANES network will fare. The goals and activities will likely change and adapt as times goes by, and as technology changes and develops. The one thing that will not change is our overarching goal: being a resource and a hub for the community of those who study or want to study the ancient world computationally. Our focus will remain on sharing knowledge and expertise with the community, spreading the crafts and training new generations of artisans.For that reason, we call out to all who want to take part in the community, whether as passive observers or active participants. Considering our past, present, and future, it is hard to imagine the field not changing drastically in the next decade. We want the ANE community to thrive in new research possibilities, without losing academic rigor to enigmatic methods and trendy AI jargon. In order to build a shared language and decide on our field’s future, we want as many as possible to join us: from the masters of the craft, to those on the fence on whether to become apprentices or not, to those who never thought they could have anything to do with this art. There is a place for everyone within the DANES community.ReferencesAnderson, Adam, Shai Gordin, Stav Klein, Bin Li, Yudong Liu &amp;amp; Marco C. Passarotti (eds.). 2023. Proceedings of the Ancient Language Processing Workshop (RANLP-ALP 2023). Shoumen, Bulgaria: INCOMA Ltd. https://aclanthology.org/volumes/2023.alp-1Clark, Shmuel &amp;amp; Shai Gordin. 2023. ‘The Mesopotamian Ancient Place-Names Almanac (MAPA): A Gazetteer of the Uruk Urbanscape in the Age of Empires.’ Journal of Open Humanities Data 9(1), p. 20. https://doi.org/10.5334/johd.146Gordin, Shai, Shmuel Clark &amp;amp; Avital Romach. 2022. ‘MAPA: A Linked Open Data Gazetteer of the Southern Babylonian Landscape’. Interdisciplinary Digital Engagement in Arts &amp;amp; Humanities 3(2). https://doi.org/10.21428/f1f23564.8d442eeaHomburg, Timo, Tim Brandes, Eva-Maria Huber &amp;amp; Michael A. Hedderich. 2023. ‘From an Analog to a Digital Workflow: An Introductory Approach to Digital Editions in Assyriology’. Cuneiform Digital Library Bulletin 2023 (4). https://cdli.mpiwg-berlin.mpg.de/articles/cdlb/2023-4Li, Bin &amp;amp; Shai Gordin (eds.). 2023. Proceedings of ALT2023: First Workshop on Ancient Language Translation (ALT). Macau, China: Asia-Pacific Association for Machine Translation. https://aclanthology.org/2023.alt-1.pdfSahle, Patrick. 2016. ‘What Is a Scholarly Digital Edition?’ In Digital Scholarly Editing: Theories and Practices, edited by Matthew James Driscoll &amp;amp; Elena Pierazzo, 19–39. Digital Humanities Series. Cambridge: Open Book Publishers. https://books.openedition.org/obp/3397Stötzner, Ernst, Timo Homburg &amp;amp; Hubert Mara. 2023. CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation.  IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). https://doi.org/10.1109/ICCVW60793.2023.00183"
    } ,

  

  

  

    
    

    {
      "title"    : "How to Annotate Cuneiform Texts",
      "authors" : "Matthew Ong",
      "tags"     : "cuneiform, annotation, linguistics",
      "doi"       : "",
      "submit-date" : "2023-10-02",
      "publish-date" : "2023-10-06",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/How%20to%20Annotate%20Cuneiform%20Texts.html",
      "type"     : "tutorial",
      "summary"     : "A high level overview of how to annotate a cuneiform text for linguistic content.",
      "content"     : "Matthew Ong                                                                                                                      1                                                                                                                                                                                                                                                                       1 UC Berkeley                                                      Submitted on: October 02, 2023          Published on: October 06, 2023                                                                          Under peer review                                          Summary: A high level overview of how to annotate a cuneiform text for linguistic content.                                      #cuneiform                  #annotation                  #linguistics                                              Difficulty level: beginner                    Lesson OverviewThis tutorial presents a high level overview of how to annotate acuneiform text for linguistic content. In general, annotation is an important first step if one wants to make a written language corpus amenable to more sophisticated linguistic analysis than just key word searches. It is also usually necessary to annotate the corpus if one wants to train a language model on it which can parse new sentences according to the language’s grammar. Exactly what we mean by ‘annotation’, and other reasons why one might want to make annotations of a corpus, will be discussed shortly. The tutorial will then go through an illustrative example to show the important aspects of annotation.This tutorial uses examples from Akkadian, but the general techniques can adapted to other ancient language corpora.Basic facts about cuneiformCuneiform is a writing system developed in southernMesopotamia towards the endof the fourth millennium BCE. It uses logograms, syllabograms and determinatives. Cuneiform was used to express several spoken languages of the ancient Middle East, includingSumerian,Akkadian,Hittite,Elamite, andHurrian. The most common way to represent cuneiform signsin modern publications is via transliteration. When a signis transcribed using capital letters, it means we are only commenting onwhat the sign looks like. When lower case letters are used, it indicateshow the sign is to be pronounced. If we write within square or curlybrackets, it means the sign is either broken or a determinative,respectively.For example, the sign transcribed as KUR looks like three trianglewedges arranged in the shape of a mountain. But this sign can bepronounced different ways depending on context. It can be read ‘mat’,‘kur’, ‘nat’, and ‘šad’, among other things. When we write {KUR}, wemean that the KUR sign functions as a determinative signaling that theword following it is a type of land or mountainous area.Drawing of the KUR signWe usually use dashes between transliterated signs to indicate theybelong to the same word. Thus the sequence a-me-el šar-ri representstwo words, and in Akkadian it means ‘man of the king’.Normalization takes transliteration and expresses the actual phonological forms behind it. It dispenses with dashes and uses macrons to indicate vowel length. Using the aboveexample, the normalization of a-me-el šar-ri would be amēl šarri. The macron sign above the vowel ‘e’ indicates this vowel is long.The Purpose of AnnotatingAnnotation allows a human editor to add information to a text that indicates its linguistic structure, whether thatdeals with semantics, morphology, syntax, or phonology. Generally it is understood that the editor provides the same level of information for all the words in a text andnot just a few.Providing such linguistic information about a text via annotation has a number of uses, suchas      Providing linguistic data for machinelearning algorithmsseeking to model properties of the underlying language.        Illustrating for students of the language how to grammaticallyanalyze a text, or as a check against the students’ own analyses.        Providing empirical data to a researcher who wants to ask questionsinvolving the systematic recording of all the linguistic data in alarge number of texts, or questions involving linguistic patternsthat emerge only upon processing the data by a machine.        Giving the annotators themselves an opportunity to work through acorpus in detail, much like an editor does when preparing a newprint edition of a text.  The Stages of AnnotatingPreprocessing the Raw TextAnnotating a cuneiform text generally requires that it first beavailable in transliteration or normalization (depending on the specificpurposes of the annotator). If acuneiform text has been edited in a print journal or other scholarlypublication, it will be represented in transliteration. Increasingly,transliterations of cuneiform texts are also available online as part ofa digital database. Sometimes the database simply displays the text toyou in your browser (i.e. as part of an HTML file) and you will have toscrape the transliteration from thesite using a variety of data processing tools. But often times thedatabase also allows you to download the transliteration in a text file.In fact, you should check if the database has their entire collection oftransliterations available for download as a ZIP archive (see OpenDANESresourcesfor downloadable datasets), as annotation is usually a process appliedto an entire group of texts rather than just one.Some online cuneiform corpora, such as those found on Oracc, also include normalized versions of cuneiform texts which can be obtained through suitable preprocessing. In what follows we will use a normalized text for illustration.The basic file format needed for the annotation is a plain text filewith only the normalized text (hence no HTLM markup or other meta-data).You need to make sure that each word in the text is separated by spaces from the preceding and followingword, which allows the annotation program to split up the text file intoword-size units. This important preliminary step is calledtokenization, and the broken-up items in the text are called tokens.Note that depending on your purposes, you may choose to reduce oreliminate symbols in the text denoting unreadable signs (e.g. the symbol‘x’) or other comments indicating layout of the text and tablet (such as‘rest of tablet broken’).An example of how your normalized text should look is given below. Thetext SAA 5, 114 is an Akkadian letterfrom the archives of Sargon II (r. 722-705 BCE), and was publishedonline with other texts from the Neo-Assyrian royal archives as anOracc project.  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazanauptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša{KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupušarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lātašpuraNote in this example that we have removed all special marks and commentsdescribing breakage in the tablet or reconstructed parts of the text. This makes later stages of the annotation and machine learning processsimpler. We should understand, however, that this is a choice we need toexplain when presenting the results of our annotations. If your projectneeds treat these features of online editions differently, you may haveto adjust later stages of your workflow accordingly. Note also that unclear signs or partial words are still left in transliterated form with dashes and brackets. You may wish to remove these tokens according to your needs.If you are annotating multiple cuneiform texts, a good practice to haveeach text in its own file, where the file is titled in a way that you(or the computer, if using a processing script) can easily identify whatthe text is. This may mean using numerical indices in the file nameswhose interpretation you record in a separate list. Alternatively, youcan often put all of the texts in a single file, separated byempty lines and special comments identifying the text. Each approach hasits advantages depending on your purpose for annotating and the toolsyou use.Making the AnnotationsTo create the annotation metadata for a text you need aprogram that will allow you to view the text and add special symbols andnotation to it. By now there are a number of free programs aimed athumanists seeking to add all sorts of meta-data to digitized texts,freely available for download or use online as a web application. Someof these programs are mainly used for highlighting thematic relationsbetween passages or phrases in a text or for connecting entitiesmentioned in the text to an external specialized vocabulary such as adatabase of maps or biographies. In our case (doing linguisticannotations), you want to make sure your tool can annotate lemmas,syntactic dependencies, and morphological features at the minimum.One very reasonable annotator program we recommend for Akkadian is Inception. It is freely downloadable as a Javaapplet that works directly through your internet browser. It is capable of handling lemmas, syntactic dependencies, and morphological feature specification in a fairly intuitive manner. The following screen shot shows what it looks like when annotating the normalized version of SAA 5, 114 introduced above.Annotating SAA 5, 114 in Inception.Lemmas, roughly speaking, are the basic stems of words which remainafter one removes prefixes and suffixes used to denote things likeplurality and grammatical case (in the case of nouns) or person andtense (in the case of verbs). In English, the lemma of cats is catbecause we removed the -s ending signalling plurality. For similarreasons, the lemma of the verb liked is like. Note that in generalwe are not seeking to do a full etymological reduction of a word to abasic root (such as prevention -&amp;gt; ‘vent’ or skyscraper -&amp;gt;scrape), but rather seek the basic form of a word abstracted from allthe ways it can appear in different grammatical contexts while stillbeing regarded as the ‘same’ word. Thus the lemma of a noun abstractsaway whether the noun is singular or plural, or the subject or object of a verb, and the lemma of a verb abstracts away whether that verb appears in present or past tense, or the first person or third, or as an imperative or an indicative form, etc.This admittedly vague definition of a lemma reflects its primary use asa lexicographic tool (allowing a human or computer to group disparateword forms under a single heading) rather than a precise morphologicalone. A certain amount of language-specific convention is involved, andif you are unsure how to lemmatize the items appearing in your cuneiformtexts, you should consult other annotation projects in the relevantlanguage.In the example figure above, the lemma for a given token is given in the orange box right above the token. For instance, the lemma for the form uptahhir is pahāru. You can manually specify the lemma for a given token by selecting the Lemma layer in the upper right and clicking on the token you want to lemmatize.In terms of syntax, Inception uses the UniversalDependencies (UD) framework because it is well-suited for working with many different languages. Unlike other grammatical formalisms, UD is based on the idea of syntactic dependency.Syntactic dependencies indicate grammatical dependencies among words andphrases such as subject and object of a verb, an adjective modifying anoun, or a conjunction connecting two full sentences. Overall, one makes semantically ‘light’ terms  like prepositions and particles dependent on semantically more important terms like nouns and verbs. Syntactic dependencies are asymmetric relations betweentwo words or phrases, one of them being the head and the other thedependent. One may think of annotating the syntactic dependencies of a sentence asessentially constructing a directed graph or tree, where the dependencies between tokens in a text are expressed by directed arrows.In the second sentence within the example figure, there is an arrow going from the verb uptahhir to the noun Urarṭaya ‘the Urartian’. The arrow’s direction indicates that the noun is dependent on the verb, and the label ‘nsubj’ indicates the noun is the nominal subject of the verb. Creating such a dependency is a simple manner of selecting the Syntax layer and drawing an arrow from the head token to the dependency token. You select the label for the dependency in the lower right.Morphological feature labelling is the task of identifying certainmorphemes, or parts of a word, conveying specific grammatical andsemantic information. The goal is to itemize aspects of the shape of theword with what it means. Annotators generally think in terms offeature-value pairs, whereby a feature is a certain category such asgrammatical number or gender, verb tense, or definiteness, and a valueis what variant of the category the word expresses via a particularmorpheme. Thus in the English word cats, the plural suffix -s signalsthe value ‘plural’ within the category of grammatical number. Inliked, the -d suffix indicates the value ‘past’ in the category oftense. When one uses an annotation tool, feature-value pairs are oftenencoded via a specific label set involving an equal sign, such as‘Gender=Masculine’ or ‘Tense=Past’, or simply a plus sign (e.g. ‘+Past’means a verb in the past tense). Nevertheless, in the context oflanguages written in cuneiform, there are still no exact conventions forhow to represent these features and their values in a label set forannotation. Thus if you wish your annotation scheme to be compatiblewith other projects, you should check to see what they use and matchtheir schema before beginning annotation.In the second sentence of the example figure, you can see the Features window showing the morphological feature specification of the form uptahhir. It begins ‘Gender=Masc|Mood=Ind|…’, which means the form is masculine and in the indicative mood. The vertical bars separate the feature-value pairs, and the left side of the equation always specifies the feature, while the right side the value of that particular feature. You can manually specify the feature string for a token by selecting the Morphology layer, clicking on the token, and typing in the feature string.The Output FileAfter you have finished annotated a text in Inception you need to output the results to a file. Although Inception allows for a number of output formats, the most convenient is CONLLU-format. This format convenient represents the lemmas, syntactic dependencies, and morphological features. It is a text file where each row represents a single token, and with ten tab-separated columns each of which specifies linguistic information about that token. The details of the encoding are found at the UniversalDependencies website.The CONLLU file for the annotation of SAA 5, 114 looks like:CONLLU file for SAA 5, 114.Note that the CONLLU format encodes token position within a sentence in the first column. Dependency relations are given in the seventh column, and dependency type in the eighth. Thus for the second sentence, the fourth row represents the token Wazana, while the fifth row represents uptahhir. The seventh column of the fourth row has value 5 and the eight column is ‘obl’. That means the token Wazana is syntactically dependent on token uptahhir and the dependency relation is oblique.  You must take care when reading a CONLLU file that you remember the seventh column represents the syntactic head, not the dependent. These are easy to confuse!Issues to Consider When AnnotatingIf you have not previously annotated a cuneiform text in the specificlanguage at hand, it would be good to check how others have annotated inthat language, both in the hopes of making your work compatible withtheirs, as well as becoming familiar with the difficulties they faced inapplying their annotation scheme. Unless you are highly experienced withthe language using modern linguistic categories, you may find that yourinitial label set for morphological features turns out to beinefficient, or your understanding of a certain syntactic structureturns out to be wrong (or at least problematic). Looking at what othershave done may save you time and effort in the long run. At the sametime, no annotation scheme currently used for a cuneiform language fits the grammatical particularities of that language perfectly, whetherat the level of theoretical description or practical implementation.Depending on the type of text you are working with, you may finddeviating from the conventions of other annotators or even needing toinvent conventions yourself. What is most important is that you havereasons you can cite for what you do and that you explain those reasonssomewhere in the documentation that accompanies your work. Not only willthis make it easier for others to understand what you did, it will alsohelp you to remember your own policies and be consistent in your work.Here are some examples of projects that use linguistic annotations in different languages written in cuneiform: on Anatolian languages like Luwian and Hittite see the eDiAna project and Hittite Festival Rituals, respectively; for Akkadian, Sumerian, Persian and Urartian see the ORACC lemmatization guidlines, and for Sumerian specifically see the ETCSL project conventions.A second issue is whether to approach annotation sub-task by sub-task ordocument by document. If you intend to annotate many documents forseveral features (e.g. syntactic dependencies and morphologicaldependencies and lemmatization), you can either annotate all of thedocuments for a single feature first, and then go back and go over thedocuments for the second feature, etc., or you can do all of thefeatures for a single document while it is still in front of you beforegoing on to the second document, etc. Depending on the tools andknowledge you bring to your work, one method may be more efficient thanthe other. If the syntactic dependencies of a corpus are particularlydifficult to do, you may opt to first go through and do the morphologybefore tackling the syntax alone. If you are working with someone whodoes not know the grammar of the language well but can find the lemmasfor all the tokens in the text on their own, you could delegate the taskof lemmatization to them while you do the dependencies and morphology.While these kinds of considerations are relevant to annotating manysorts of languages beyond cuneiform ones, in the case of cuneiformlanguages there is one thing to keep in mind. Unless the type of textyou are annotating is particularly simple morphosyntactically, it willlikely take you some effort to understand what the text is saying evenin transliteration (or normalization). Thus going over the text multipletimes for different annotation tasks may be less efficient that doingeverything at once while the meaning of the text is still fresh in yourmind.Finally, be aware of accompanying datasets or machine learning toolsthat can accelerate your work. If your transliterations come from anonline database, they may also come with lemmatization data as part of aJSON file or separate glossary. If you already had to extract the rawtext from such a JSON file, it is only a little more work to grab thelemmas that go with the tokens as well. Similarly, if you are makingannotations of a corpus to train a natural language processing model on it (say a syntactic parser or morphologizer), you can actually start training your model on the portion of the corpus you have annotated early on and thenapply its predictions to the rest of the corpus. Going through andcorrecting the model’s predictions is often faster than going throughthe whole raw corpus unaided. This technique, known as bootstrapping,is particularly effective when repeated multiple times, early on, for alarge corpus.Choosing How to Present ResultsUnless you are annotating cuneiform texts for private purposes and donot want to share you data, you should consider how to share your workwith others on the internet. GitHub is an online platformmainly designed for people writing code they wish to share with othersin a controlled, version-specific way. As an annotator, you can use itas a convenient place to store your data and any associated processingscripts, keeping your data project private, open to all, or only toinvited users. The system is designed to synchronize with particularfolders on your local computer, so that the process of backing up dataor uploading newer versions is easy and allows you to compare currentand older versions of files. GitHub is a good place to provide your dataif you envision yourself working on multiple projects in the future, orif you have scripts or other associated code that you need to presentalongside the annotations themselves.If you have annotated your texts in the Universal Dependencies format,you can also make your data available on the UDwebsite (which actually stores its data onGitHub) alongside annotated corpora from dozens of other languages. Youdo need to make sure your annotations conform to their formatspecifications. This website already features corpora of Akkadian,Biblical Hebrew, and Hittite.Final ObservationsWhile annotating itself has some immediate uses, it is often done aspart of a larger language processing task, research program, orpedagogical project. We mentioned some of these uses earlier on, amongwhich were applications to machine learning and language modeldevelopment. It is worth noting that many of the cutting-edge computeralgorithms and research efforts in natural language processing aregeared towards modern languages with large online datasets such asEnglish, even if they are not restricted to only those languages. Nocuneiform language comes close to the size of such corpora in terms ofraw text, let alone text that has been properly transliterated orotherwise made suitable for annotation. This means that the challengesof applying machine learning and developing language models forcuneiform languages are somewhat different from someone working onEnglish. There are fewer pre-existing annotated corpora that one can useto jump-start ones own annotations. Many natural language processingpackages assume that you are interested in working with a popular modernlanguage, and often come with large datasets from those languagesbuilt-in, without clearly explaining how to apply their code to alow-resource language starting from the ground up. Unless you are anexperienced programmer, you will likely find it very helpful to haveguidance from someone more experienced in natural language processing ordata science, or to look at some of the online proceedings from NLPworkshops aimed at humanists and specialists in less common languages.One recent workshop at Princetonillustrates how to develop an NLP project for new languages usingspacy. On a broader level, one may also considerthe Digital Humanities Summer Institute with itsvarious course offerings. What may be the most helpful in the beginning,however, is finding someone else who has begun an annotation project inyour language by searching GitHub and getting guidance from them."
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts - Challenges and Comparisons of Python and R Resources",
      "authors" : "chatGPT",
      "tags"     : "python, R, pre-processing",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-07",
      "publish-date" : "2023-04-07",
      "peer-review-date" : "2023-04-07",
      "update-date" : "",
      "url"      : "/pages/paper1.html",
      "type"     : "white-paper",
      "summary"     : "This is an example paper created by chatGPT.",
      "content"     : "chatGPT                                                                                                                      1                                                                                                                                                                                                           1 openAI                                                      Submitted on: April 07, 2023          Published on: April 07, 2023                      Peer reviewed: April 07, 2023                                                                        Reviewed by:         chatGPT                                                                                            doi                                                    Summary: This is an example paper created by chatGPT.                                      #python                  #R                  #pre-processing                        About this paperAll the text contained in this paper, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write an example white paper that will include all possible markdown syntax? (different heading types, tip prompts, images, etc.). This will be used as a template example.  The example white paper should be about current challenges in preprocessing ancient texts using available online resources, particularly comparing current resources in python versus current resources in R.  As this is a demo, please include jokes to make it funny and entertaining!Then, for the peer-review process, a new conversation with chatGPT was started where the following prompt was given:  I need you to perform an academic peer review for a white paper submitted to openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Please critically answer the following questions:      should the paper be published in openDANES?    are there any corrections you would suggest to the text, which would make it clearer or strengthen the points the author is making? Please write them out.    are the jokes funny enough? If not, can you improve them?  Then the original version of this paper was copy-pasted.ChatGPT provided the following answers:  Overall, the paper is well-written, informative, and engaging. It addresses an important and timely topic, and provides a useful comparison of Python and R resources for preprocessing ancient texts. Below are my answers to the three questions posed:  (1) Yes, the paper should be published in openDANES. It meets the platform’s criteria of providing tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  (2) There are no major corrections that I would suggest. However, I have a few minor suggestions that may strengthen the points the author is making:      The introduction could benefit from a more specific statement of the problem or research question that the paper addresses.    In the Python section, the author mentions sentiment analysis and questions its usefulness for ancient texts. It may be helpful to provide an example or explanation of why this is the case.    In the R section, the author could expand on the benefits of the tidytext and quanteda packages. What specific features or functionalities do these packages offer?    (3) The jokes are light-hearted and add to the overall tone of the paper. I don’t think they detract from the content, and therefore don’t need to be improved.  Overall, the paper is a valuable contribution to the field of ancient Near Eastern studies and computational methodologies.Then, we asked chatGPT to rewrite the paper with the suggested corrections. The text below is the result of that peer-review.The text has not been adapted whatsoever by a human. This is purely meant as an example of how a formatted paper looks like.Preprocessing Ancient Texts: Challenges and Comparisons of Python and R ResourcesIntroductionAs the field of ancient Near Eastern studies becomes increasingly reliant on computational methodologies, scholars are faced with the challenge of preprocessing ancient texts using available online resources. In this white paper, we will explore the current challenges of preprocessing ancient texts and compare the available resources in Python and R. Specifically, we aim to answer the question: which language is better for preprocessing ancient texts - Python or R?The Challenge of Preprocessing Ancient TextsPreprocessing ancient texts presents a unique set of challenges that are not encountered when working with modern texts. For one thing, ancient texts often come in a variety of languages, many of which are no longer spoken. Additionally, ancient texts may be damaged or incomplete, making them difficult to read or understand. And let’s not forget the challenge of deciphering ancient scripts!But fear not, intrepid scholars! With the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world.Python Resources for Preprocessing Ancient TextsPython has a wealth of resources available for preprocessing ancient texts, including the Natural Language Toolkit (NLTK) and the TextBlob library. These tools can be used to tokenize text and perform part-of-speech tagging. However, while Python also has the capability to perform sentiment analysis, it is unlikely to be useful for ancient texts since we have no way of knowing how the authors were feeling.Of course, as with any programming language, there are some quirks to be aware of. As the saying goes, “Python is the second-best language for everything.” But don’t let that discourage you - with a little perseverance, you can accomplish great things in Python.R Resources for Preprocessing Ancient TextsR is another popular language for preprocessing ancient texts, with resources like the tidytext package and the quanteda package. These tools can be used to perform a variety of text analysis tasks, such as word frequency analysis and topic modeling.The tidytext package offers functionality for performing sentiment analysis specifically for texts in English, which may be useful for some ancient texts written in English. The quanteda package provides a variety of text processing and analysis functionalities, including tokenization, part-of-speech tagging, and keyword extraction. It also offers support for many different languages and scripts.However, R has its own quirks and challenges. As one wise person once said, “R is like a drug. Once you start using it, you can’t stop.” So be careful when delving into the world of R - you may find yourself unable to escape!Comparing Python and R ResourcesAs we have seen, both Python and R offer valuable resources for preprocessing ancient texts. Which language is better for this task ultimately depends on your specific needs and preferences. Python may be more intuitive for those who are new to programming, while R may be better suited for those who are already familiar with statistics and data analysis.ConclusionIn conclusion, preprocessing ancient texts presents a unique set of challenges, but with the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world. Python and R both have their own strengths and quirks, so the best choice depends on your specific needs and preferences.And remember, when in doubt, just keep laughing - because as the great scholar Confucius once said, “Everything has beauty, but not everyone sees it.”"
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)",
      "authors" : "chatGPT",
      "tags"     : "python, CLTK, ancient Greek",
      "doi"       : "",
      "submit-date" : "2023-04-06",
      "publish-date" : "2023-04-06",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/tutorial1.html",
      "type"     : "tutorial",
      "summary"     : "This is a demo tutorial created by chatGPT.",
      "content"     : "{%- include author_card.html title=page.title item_type=page.type banner-image-source=page.banner-image-source authors=page.authors date=page.submit-date publish-date=page.publish-date peer-review-date=page.peer-review-date update-date=page.update-date tags=page.tags summary=page.summary reviewers=page.reviewers difficulty=page.difficulty doi=page.doi deprecated=page.deprecated -%}About this tutorialAll the text contained in this tutorial, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write a demo tutorial that will include all possible markdown syntax? (all heading types, code blocks, images, etc.). This will be used as a template example.  The demo tutorial should be about preprocessing ancient texts using the The Classical Language Toolkit (CLTK) python library.  As this is a demo, please include jokes to make it funny and entertaining!The code in this tutorial has not been validated and the text itself has not been edited only the link to the image displayed in the tutorial has been adapted to display correctly. This is purely meant as an example of how a formatted tutorial looks like.Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)Are you tired of manually preprocessing ancient texts for your research in Near Eastern studies? Fear not! The Classical Language Toolkit (CLTK) is here to help!What is CLTK?The CLTK is a Python library that provides natural language processing (NLP) tools for ancient languages, including Greek, Latin, Sanskrit, and Old English. With the CLTK, you can tokenize, lemmatize, and part-of-speech tag texts, as well as perform other common NLP tasks.Installing CLTKBefore we get started, make sure you have Python 3 installed. Then, you can install the CLTK using pip:pip install cltkLoading TextsOnce you have the CLTK installed, you can start loading texts. Let’s load the Iliad by Homer:import cltkfrom cltk.corpus.utils.importer import CorpusImportercorpus_importer = CorpusImporter(&#39;greek&#39;)corpus_importer.import_corpus(&#39;tlg&#39;) # import the TLG corpuscorpus_importer.import_corpus(&#39;phi5&#39;) # import the PHI5 corpusfrom cltk.corpus.readers import get_corpus_readerreader = get_corpus_reader(corpus_name=&#39;tlg&#39;, language=&#39;greek&#39;)iliad = reader.raw(&#39;tlg0012.tlg001&#39;) # load the IliadTokenizing TextsNow that we have the Iliad loaded, let’s tokenize it:from cltk.tokenize.word import WordTokenizertokenizer = WordTokenizer(&#39;greek&#39;)tokens = tokenizer.tokenize(iliad)Part-of-Speech TaggingWe can also perform part-of-speech tagging on the tokens:from cltk.tag.pos import POSTagtagger = POSTag(&#39;greek&#39;)tagged_tokens = tagger.tag_ngram_123_backoff(tokens)Lemmatizing TextsFinally, we can lemmatize the tokens:from cltk.lemmatize.backoff import BackoffGreekLemmatizerlemmatizer = BackoffGreekLemmatizer()lemmas = [lemmatizer.lemmatize(token) for token in tokens]And that’s it! With just a few lines of code, we’ve tokenized, part-of-speech tagged, and lemmatized an ancient Greek text. Now, go forth and preprocess with ease!{% include fig_legend.html content=”Figure 1: This is the logo of CLTK” %}  Why did the linguist break up with the CLTK? Because it kept tokenizing everything!Note: This demo tutorial used various markdown syntaxes including headers, code blocks, images, and blockquotes. Use these features to make your tutorials more organized and visually appealing. And don’t forget to add a touch of humor to keep your readers engaged!"
    } ,

  

  

  
    {} ,

  

  

  
    {} 

  

  
]