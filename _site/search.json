[
  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Digital Ancient Near Eastern Studies - A Transition to Arts and Crafts",
      "authors" : "Shai Gordin, Avital Romach, Eliese-Sophia Lincke, Hubert Mara, Aleksi Sahala, Marine Béranger",
      "tags"     : "opinion, open access, DANES network",
      "doi"       : "",
      "submit-date" : "2024-02-16",
      "publish-date" : "2024-02-16",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Digital%20Ancient%20Near%20Eastern%20Studies%20-%20A%20Transition%20to%20Arts%20and%20Crafts.html",
      "type"     : "white-paper",
      "summary"     : "A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.",
      "content"     : "Shai Gordin                                                                                                                      1                                                                                                                                                                                                           2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ;               Avital Romach                                                                                                                                                                                                                                                                                                                      3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ;               Eliese-Sophia Lincke                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                 ;               Hubert Mara                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      5                                                                                                                                                                                                                                                                             ;               Aleksi Sahala                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6                                                                                                                                                                             ;               Marine Béranger                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                                                                                                                                         1 Ariel University              ;                 2 Open University of Israel              ;                 3 Yale University              ;                 4 FU Berlin              ;                 5 MLU Halle-Wittenberg              ;                 6 University of Helsinki                                                      Submitted on: February 16, 2024          Published on: February 16, 2024                                                                          Under peer review                                          Summary: A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.                                      #opinion                  #open access                  #DANES network                          “The use of electronic data-processing devices for research in the fields of linguistics and philology is by now fairly common.” (Hans G. Güterbock, “The Hittite Computer Analysis Project”, from the 1967-1968 annual report of the Oriental Institute, currently the Institute for the Study of Ancient Cultures)Digital studies of the ancient Near East are not new. If anything, within the fields of assyriology, egyptology, and archaeology, one can find some of the pioneers embracing the advent of the computer age. Today, there is new interest in applying computational methods to the humanities and social sciences, and to the study of the ancient world specifically, given the potential of such interdisciplinary initiatives. However, there are questions that are fair to ask:  can the computer really teach us something new about our fragmentary, complicated texts and artifacts? Is this a movement of new arts and crafts, providing us with fresh perspectives, or are these arts and crafts a trending hobby, not really worth the scholarly effort?In order to apply computational methodologies, there is a need for shared language, interdisciplinary work, and a willingness to constantly learn outside of your field of specialty. Exploration of texts and artifacts as data, with the plethora of tools and techniques available today, is fundamental, yet figuring out where to start can be daunting. There is a need to design experiments, that might fail, and show no results except (bitter) experience. Even the significance of successful results are not always appreciated by your peers. Where computational analysis ends or should end and humanistic inference begins can at times be opaque. Lack of standards and resources for computational research in ancient Near Eastern (ANE) studies and neighboring disciplines makes the interpretation and significance of such work vague and unclear to the non-specialists.Let us give such an example. Below you can see graphs generated from the network of the Mesopotamian Ancient Placenames Almanac (MAPA); for further information see Gordin, Clark and Romach (2022) and Clark and Gordin (2023). The points on the graph are toponyms attested in texts from the hinterland of the city of Uruk in the first millennium BCE. Their position on the graph is a result of their Degree in the network—how central they are as hubs—and their Betweenness Centrality score—how important they are as a bridge to get from one location to another. Without minimal background in graph theory, it is hard to understand the value of such visualizations, and whether they add new information to our traditional ways of understanding connections and links between geographical locations. The transformation of the texts into data—from sentences to word lists, to toponyms represented in a network, to a graph of their scores—distances us from the artifacts, the original texts. These transformations are so substantial, it is the equivalent of looking at a sword and trying to understand how it was forged without seeing the process or knowing anything about the tools and methods used.In this piece, we argue that it is worthwhile to dedicate the time to learn these arts and crafts. We begin with what resources were created before us, where we stand today, and what we hope to gain in the future. We outline the initiatives of the Digital Ancient Near Eastern Studies (DANES) network, whose purpose is to increase the number of apprentices, by providing both basic literacy to understand computational methods, and advanced practical applications to those who want to become artisans in the field.FoundationsThe early pioneers created large databases which are the baselines for many computational works performed today, such as (in alphabetical order) the Coptic Scriptorium, Cuneiform Digital Library Initiative (CDLI), Hethitologie-Portal Mainz (HPM), the Open Richly Annotated Cuneiform Corpus (ORACC), Papyri.info, the Perseus digital library, and Thesaurus Linguae Aegyptiae (TLA), to name a few. Most of these efforts were placed at digitizing ancient sources. This was a colossal effort with the technology of the time, when storage space was limited, when punch-cards were frustrating, when non-Latin characters were not supported.Criticism was not lacking towards these revolutionary endeavors. Were digitization and computational studies worth the effort? Was new knowledge obtained that could not have been gathered otherwise? Such setbacks conceptually made it seem as though the computer revolution has not fundamentally touched upon the fields of the ancient Near East. There are still general misunderstandings regarding the inherent differences between print publications (which include PDF versions), and plain-text digital scholarly editions that are also published on online platforms (see Sahle 2016).From the early 2010s, the AI revolution reinvigorated the use of computational methods for the study of the ancient Near East. Breakthroughs in computer science, increased storage capabilities for big data, the development of the internet, the creation of programming languages and environments that are more user-friendly, and the extension of the Unicode standard, opened new lines of research.At the current moment, we think a conscious choice needs to be made on how we envision the field going forward. If we want to promote new methodologies, we cannot dismiss established ones. If we want to promote such new methodologies alongside established ones, computational research needs to be communicated in a way that is still understandable to all relevant fields. We need a shared language that includes basic literacy of terms, common problems and possible solutions (see e.g. Homburg et al. 2023). As basic digital literacy becomes more common, more scholars and students will desire to expand their knowledge, use digital tools, gain more programming experience, etc.This is particularly important as computational methods are shared across the growing field of digital humanities. A shared set of methodologies is a powerful link between fields, creating a shared jargon, and it is important that we propel the study of ancient cultures to current research avenues.Training in the PresentAll the above requires training—both of humans and machine learning algorithms. The barriers to computational studies today for those coming from the humanities, compared to the challenges of the 20th century, are very different. Yet, the big difference is that today it is easier to lower said barriers, due to the ease of sharing information, tools, and knowledge online. There are many digital humanities’ initiatives, platforms, and resources, to train and aid humanists to learn anything from basic digital literacy to developing advanced machine learning applications (such as the Programming Historian, The Digital Orientalist, or DARIAH-Campus). These general resources are invaluable, but sometimes they are insufficient when applying new methodologies on ancient Near Eastern data, either in the form of text, object, image, or landscape.Furthermore, current technologies, especially generative AI models, offer new possibilities. Large language models (LLMs) have transformed the ways in which we interact with texts only in the last few months. Many are wondering what is the place of humans in this future, now that generative models such as GPT and image generation models are revolutionizing the way we write and create. Barriers such as the knowledge of how to write code may seem obsolete, now that anyone can use chatGPT with natural language input to program. Nevertheless, that does not mean that anyone can write useful and functional code, or gain meaningful results.It is easy to trick or ask ChatGPT questions the model was not trained for, and receive incorrect results. When asked to translate a pivotal sentence in Hittite that was used for its identification as an Indo-European language, the translation is only partially correct, with many caveats elaborated upon by the model, such as the incorrect statement that Hittite and Latin are from two different language families!For humanists to be able to understand how to work with such tools, and gain meaningful results from AI models, digital literacy is a prerequisite. In the above example, knowing more about what language models are to begin with, what data they are trained on and how it affects the model’s results, allows humanists to judge their results accordingly and also teaches humanists how to give only reasonable tasks to models to begin with.On the other side of the coin, those with computational background are often not familiar enough with the intricacies of ancient cultures, languages, writing systems, and artifacts in order to design models that will provide historically sound results or meaningful conclusions. Conceptual understanding and structured ontologies are imperative for scholars from the humanities to better contribute to the methodological discussions in the interface between computational methods and humanistic approaches.Furthermore, despite the foundational works of the 20th century, we are still facing issues that are waiting for state-of-the-art innovations when studying ancient cultures computationally. We should update our best-practice methods for preserving and digitizing our objects of study. While the vast majority of image data are photographs and flatbed scans, the number of acquired and available 2D+ and 3D models is increasing. 3D models specifically provide precise representations that allow the exact measurements of minute details such as fingerprints, seals and damaged characters. Standardized high-resolution, high-contrast representations have also been shown to improve AI-based approaches to optical character recognition tasks (Stötzner et al. 2023). To take advantage of these new technological possibilities, 3D scanning needs to be put on the agenda, and its benefits for research and object preservation need to be clearly communicated to the wider research community.For fully transparent research and seamless workflow, these material remains need to be linked to metadata, or text editions if there is any writing on the objects. Linked open data (LOD) is a set of standards for linking data (be it text, artifact, metadata, and more) over the web in certain formats, following specific ontologies agreed upon by a community of experts. The use of LOD in ancient studies is growing (e.g. Pelagios network), and it requires consensus and discussions to keep the data interlinked.Digital monolingualism and the primacy of the Latin script have led to software development lagging behind a fundamental need for full Unicode and font support in order to include non-European scripts and languages. Natural language processing (NLP) models designed primarily for English and maybe some other European languages require adaptations when applied to ancient languages. The need for a lobby of ancient Near Eastern studies and other disciplines sharing these concerns is obvious (see for example the DHd Multilingual Digital Humanities working group), and ancient language processing (ALP) is becoming a growing initiative within the NLP and machine learning community (see e.g. Anderson et al. 2023, Bin and Gordin 2023, and the ML4AL workshop @ ACL 2024).In the next section, we elaborate on the work of the DANES community to furnish some of the prerequisites necessary for meaningful computational and collaborative research, and to promote research on some of the current issues presented above.Current and Future ActivitiesThe DANES network actively creates spaces for the above goals to become a reality. The efforts of the network, which is inclusive and welcoming to all interested scholars and students in relevant fields, are manifest through the following initiatives:ConferencesThe first annual DANES conference took place between 19th-21st of February 2023, organized by three institutions: the Digital Pasts Lab at Ariel University, the TAD AI and Data Science Center at Tel Aviv University, and the School of Computer Science and Engineering at the Hebrew University of Jerusalem. The conference covered many topics, including optical character recognition (OCR) for cuneiform documents, data visualization methods, critical discussion of data analysis and machine learning, computational stylistics, natural language processing models, linguistic annotation, networks, and new online tools and environments and their sustainability. One can view the talks of the first day on YouTube, read the abstracts on Zenodo, and view the conference posters on OpenDANES. The proceedings of the conference are going to be published as a special issue, iDANES, in the it - Information Technology journal.Just as important as the talks themselves, were the discussions in-between sessions, the discovery of shared problems, and arriving at joint solutions. These conversations led to more avenues of further development and collaboration. Such was the main purpose of the conference: not only giving a venue to present interdisciplinary research, but also a space for scholars that usually have few people around them that understand every aspect of their work.The conference included a Q&amp;amp;A session with some of the experts in the field, Eliese-Sophia Lincke, Niek Veldhuis, Hendrik Hameeuw, and Hubert Mara, who received questions from the audience and online participants on some of the current burning questions. Especially important were the round-table discussions, whose purpose was to establish the goals of the new DANES network, common points of contention, future areas of research, and what is needed for the network and this field to thrive in the next decade.DANES conferences are going to regularly occur every year. The exact date of the next conference is still in the works. The following conferences will continue to have a combination of lectures with active participation: hands-on workshops, round-tables, and Q&amp;amp;A sessions.Working groupsThe DANES working groups are usually once-per-month online meetings to discuss or implement a specific computational methodology. In between meetings, the DANES discord server has sub-channels for each group for further discussions. They are meant as an entry point for those who want to learn about computational methodologies and do not know where to start or whom to ask. They are usually led by at least two people from the network.In 2023, the OCR group, led by Hendrik Hameeuw and Eliese-Sophia Lincke, met to discuss current challenges. The ancient language processing (ALP) group, led by Katrien De Graef and Shai Gordin, had guest lectures of various experts in the field to discuss their ongoing projects; the lectures and the recordings are available. The Interoperability and annotation group, led by Adam Anderson and Timo Homburg, discussed the importance of linked open data and how to apply it.Specific topics for groups depend on the initiators. Anyone is welcome to lead a group! Current active groups include a continuation of the ALP group, this year focusing on how to create a framework of under-resourced ANE languages, with a case-study on Elamite. Further activities of the Pedagogy and Gamification group and OCR groups are planned for 2024—sign up for the mailing list or join the Discord for updates!Discord and mailing listThe channels of communication for members of the DANES network is through Discord and a mailing list. To join the mailing list or Discord, send an email to danes@listserv.dfn.de or digpasts@gmail.com. Discord is a communication platform that allows users to create and join communities, known as servers, where they can chat with text, voice, and video. It was originally designed for gamers to communicate while playing online games, but it has since expanded to cater to various interests and communities. The DANES discord server includes various channels for communication of the DANES working groups and general discussions. It is meant as the primary space for anyone in the community to ask questions, give updates, and generally have conversations on DANES and related fields and methodologies.The mailing list is the more formal communication channel of the community. It is used to send out the monthly newsletter and give updates on meetings of the DANES working groups.NewsletterThe mailing list sends out a monthly newsletter, at the beginning of each month, which includes updates on the world of DANES. The newsletter summarizes, while explicating jargon, important articles that were published in the past month, and highlights important previous publications related to DANES. It informs the DANES community of relevant conferences, both in the ANE field and the digital humanities and computer science fields. The purpose of the newsletter is to establish DANES in both the field of computer science and ANE studies, by developing joint language and terminology around important publications, and encourage network members to participate in conferences relating to both worlds.Furthermore, after the publication of the newsletter there is a regular happy-hour meeting on the DANES discord channel for members to discuss and chat about what’s new in the community.OpenDANES platformThe OpenDANES platform is an open access publication platform for pedagogical materials. It aims to create content that will be useful and informative to any who want to learn how to combine computational methods with ancient artifacts, regardless of their previous knowledge. It publishes tutorials, which provide step-by-step instructions on how to apply computational methodologies for beginner, intermediate, and advanced levels. White papers, such as this one, are published on the platform as well. Those can be anything from opinion pieces, updating the community on projects, introducing the community to general initiatives, or efforts of groups and individuals working on digital and computational studies of the ancient world. All contributions go through a peer-review process to ensure high-quality and usefulness to the community, as well as to acknowledge contributors’ academic work. Upon final publication, all contributions receive a DOI.Additionally, OpenDANES includes the DANES resources, a dataset which collects free, online resources which can aid the DANES community members. It is a constantly growing dataset, and anyone is invited to contribute resources! See the instructions on the DANES Resources page.Final NotesTime will tell how the efforts of the DANES network will fare. The goals and activities will likely change and adapt as times goes by, and as technology changes and develops. The one thing that will not change is our overarching goal: being a resource and a hub for the community of those who study or want to study the ancient world computationally. Our focus will remain on sharing knowledge and expertise with the community, spreading the crafts and training new generations of artisans.For that reason, we call out to all who want to take part in the community, whether as passive observers or active participants. Considering our past, present, and future, it is hard to imagine the field not changing drastically in the next decade. We want the ANE community to thrive in new research possibilities, without losing academic rigor to enigmatic methods and trendy AI jargon. In order to build a shared language and decide on our field’s future, we want as many as possible to join us: from the masters of the craft, to those on the fence on whether to become apprentices or not, to those who never thought they could have anything to do with this art. There is a place for everyone within the DANES community.ReferencesAnderson, Adam, Shai Gordin, Stav Klein, Bin Li, Yudong Liu &amp;amp; Marco C. Passarotti (eds.). 2023. Proceedings of the Ancient Language Processing Workshop (RANLP-ALP 2023). Shoumen, Bulgaria: INCOMA Ltd. https://aclanthology.org/volumes/2023.alp-1Clark, Shmuel &amp;amp; Shai Gordin. 2023. ‘The Mesopotamian Ancient Place-Names Almanac (MAPA): A Gazetteer of the Uruk Urbanscape in the Age of Empires.’ Journal of Open Humanities Data 9(1), p. 20. https://doi.org/10.5334/johd.146Gordin, Shai, Shmuel Clark &amp;amp; Avital Romach. 2022. ‘MAPA: A Linked Open Data Gazetteer of the Southern Babylonian Landscape’. Interdisciplinary Digital Engagement in Arts &amp;amp; Humanities 3(2). https://doi.org/10.21428/f1f23564.8d442eeaHomburg, Timo, Tim Brandes, Eva-Maria Huber &amp;amp; Michael A. Hedderich. 2023. ‘From an Analog to a Digital Workflow: An Introductory Approach to Digital Editions in Assyriology’. Cuneiform Digital Library Bulletin 2023 (4). https://cdli.mpiwg-berlin.mpg.de/articles/cdlb/2023-4Li, Bin &amp;amp; Shai Gordin (eds.). 2023. Proceedings of ALT2023: First Workshop on Ancient Language Translation (ALT). Macau, China: Asia-Pacific Association for Machine Translation. https://aclanthology.org/2023.alt-1.pdfSahle, Patrick. 2016. ‘What Is a Scholarly Digital Edition?’ In Digital Scholarly Editing: Theories and Practices, edited by Matthew James Driscoll &amp;amp; Elena Pierazzo, 19–39. Digital Humanities Series. Cambridge: Open Book Publishers. https://books.openedition.org/obp/3397Stötzner, Ernst, Timo Homburg &amp;amp; Hubert Mara. 2023. CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation.  IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). https://doi.org/10.1109/ICCVW60793.2023.00183"
    } ,

  

  

  

    
    

    {
      "title"    : "How to Annotate Cuneiform Texts",
      "authors" : "Matthew Ong",
      "tags"     : "cuneiform, annotation, linguistics",
      "doi"       : "10.5281/zenodo.10825347",
      "submit-date" : "2023-10-02",
      "publish-date" : "2023-10-06",
      "peer-review-date" : "2024-03-15",
      "update-date" : "",
      "url"      : "/pages/How%20to%20Annotate%20Cuneiform%20Texts.html",
      "type"     : "tutorial",
      "summary"     : "A high level overview of how to annotate a cuneiform text for linguistic content.",
      "content"     : "Matthew Ong                                                                                                                      1                                                                                                                                                                                                                                                                       1 UC Berkeley                                                      Submitted on: October 02, 2023          Published on: October 06, 2023                      Peer reviewed: March 15, 2024                                                                        Reviewed by:         Heather D. Baker                                                                          Chiara Palladino                                                                                                                                                doi                                                    Summary: A high level overview of how to annotate a cuneiform text for linguistic content.                                      #cuneiform                  #annotation                  #linguistics                                              Difficulty level: beginner                    Lesson OverviewThis tutorial presents a high level overview of how to annotate acuneiform text for linguistic content. In general, linguistic annotation is an important first step if one wants to make a written language corpus amenable to more sophisticated linguistic analysis than just key word searches. It is also usually necessary to annotate the corpus if one wants to train a language model on it which can parse new sentences according to the language’s grammar. Exactly what we mean by ‘annotation’, and other reasons why one might want to make annotations of a corpus, will be discussed the Purpose of Annotating. The tutorial will then go through an illustrative example to show the important aspects of annotation.This tutorial uses examples from Akkadian, but the general techniques can adapted to other ancient language corpora.Basic facts about cuneiformCuneiform is a writing system developed in southernMesopotamia towards the endof the fourth millennium BCE. It uses logograms, syllabograms and determinatives. Cuneiform was used to express several languages of the ancient Middle East, includingSumerian,Akkadian,Hittite,Elamite, andHurrian. The most common way to represent cuneiform signsin modern publications is via transliteration. When a signis transcribed using capital letters, it indicates a logogram. When lower case letters are used, it indicateshow the sign is to be pronounced in a syllabic reading. If we write a sign within half-square brackets (‘⸢KUR⸣’), it is only partially visible on the tablet, while if it is written within square brackets (‘[KUR]’) it means the sign is no longer visible at all, and is reconstructed by assyriologists. When we write a sign as a superscript (for print editions) or within curlybrackets (for online editions), it means the sign serves as a determinative.For example, the sign transcribed as KUR looks like three trianglewedges arranged in the shape of a mountain. When written logographically, it usually stands as the word for ‘mountain’ or ‘land’. But this sign can bepronounced in different ways depending on context. It can be read ‘mat’,‘kur’, ‘nat’, and ‘šad’, among other things. When we write {KUR} (in digital editions) or KUR (in print editions), itmeans that the KUR sign functions as a determinative signaling that theword following it is a type of land or mountainous area.Drawing of the KUR sign.We usually use dashes between transliterated signs to indicate theybelong to the same word. Thus the sequence a-me-el šar-ri representstwo words, and in Akkadian it means ‘man of the king’.Normalization takes transliteration and expresses the actual phonological forms behind it. It dispenses with dashes and uses macrons and circumflexes to indicate vowel length. Using the aboveexample, the normalization of a-me-el šar-ri would be amēl šarri. The macron sign above the vowel ‘e’ indicates this vowel is long.The Purpose of AnnotatingLinguistic annotation allows a human editor to add information to a text relating to its linguistic structure, whether thatdeals with semantics, morphology, syntax, or phonology. Providing such linguistic information about a text via annotation has a number of uses, suchas:      Providing linguistic data for machinelearning algorithmsseeking to model properties of the underlying language;        Illustrating for students of the language how to grammaticallyanalyze a text, or as a check against the students’ own analyses;        Providing empirical data to a researcher who wants to ask questionsinvolving the systematic recording of all the linguistic data in alarge number of texts, or questions involving linguistic patternsthat emerge only upon processing the data by a machine;        Giving the annotator themselves an opportunity to work through acorpus in detail, creating an index, much like an editor does when preparing a newprint edition of a text.  The Stages of AnnotatingWhen linguistically annotating a cuneiform text it is convenient to divide the work into a preprocessing stage and an actual annotation stage. We will discuss each of these stages in order.Preprocessing the Raw TextAnnotating a cuneiform text generally requires that it first beavailable in transliteration or transcription/normalization (depending on the specificpurposes of the annotator). If acuneiform text has been edited in a print journal or other scholarlypublication, it will be represented in transliteration. Increasingly,transliterations of cuneiform texts are also available online as part ofdigital databases. Sometimes the database simply displays the text toyou in your browser (i.e. as part of an HTML file) and you will have toscrape the transliteration from thesite using a variety of data processing tools. But often times thedatabase also allows you to download the transliteration in a text file.In fact, you should check if the database has their entire collection oftransliterations available for download (see OpenDANESresourcesfor downloadable datasets), as annotation is usually a process appliedto an entire group of texts rather than just one. One repository that does this is the CDLI.Some online cuneiform corpora, such as those found in Oracc, also include normalized versions of cuneiform texts which can be obtained through suitable preprocessing. Readers interested in learning how to download large sets of normalized texts from Oracc can use the Jupyter notebooks of Niek Veldhuis, which come with substantial documentation. This notebook in particular will allow you to obtain normalized versions of texts from one of the State Archives of Assyria volumes online.In what follows we will use portions of a normalized text obtained from Oracc via the above notebooks for illustrating preprocessing and annotation. This text is SAA 5, 114, an Akkadian letterfrom the royal archives of Sargon II (r. 721-705 BCE).The basic file format of the normalized text you want to ultimately annotate should be a plain text file, with no HTML markup or other metadata. In particular, there should be no punctuation markers like periods or quotation marks. You need to make sure that each word in the text is separated by spaces (ideally one) from the preceding and followingwords, which allows the annotation program to split up the text file intoword-size units. This important preliminary step is calledtokenization, and the broken-up items in the text are called tokens.Note that depending on your purposes, you may choose to reduce oreliminate symbols in the text denoting unreadable signs (e.g. the symbol‘x’) or other comments indicating layout of the text and tablet (such as‘rest of tablet broken’). Generally, removing broken or uninterpretable symbols makes little difference for training morphosyntactic language models. But if you wish to have your annotations retain more fidelity to the original text (e.g. by indicating how far apart fragments of linguistically parsable material are from each other), it is better to retain these tokens.In our case (SAA 5, 114), our desired plain text file looks like  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazanauptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša{KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupušarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lātašpuraNote that in contrast to the online edition of this text, there are no special marks and commentsdescribing line numbers, breakage in the tablet, obverse and reverse, or reconstructed parts of the text. This is because our interest in the text is linguistic, without regard to where the words appear on the tablet. Moreover, such clean up makes the resulting annotation more useful for machine learning purposes. However, depending on the annotation program you use it is often possible to include special comment lines at the top of the file preceded by a special symbol, such as ‘#’. Such lines can be useful in allowing you or a processing script to identify certain features of the text (such as publication information).In general, we should understand that what to remove from an online edition of the text is a choice we need toexplain when presenting the results of our annotations. If your projectneeds to treat these features of online editions differently, you may haveto adjust later stages of your workflow accordingly. Note in general that it might be harder for you to read and understand the text if you remove markers of the physical aspects of the tablet (e.g. where the obverse/reverse begins, rulings, line breaks). Thus it is probably a good idea to have an edition of the text handy for you to consult while parsing the text.Note also that unclear signs or partial words like {KUR}x and x+x-pa are still left in transliterated form with dashes and brackets to indicate which signs function as determinatives or go together to form a word. According to Assyriological convention, a single ‘x’ sign generally refers to one illegible sign on the tablet. You may wish to remove these tokens according to your needs. Sometimes the fragmentary tokens are still useful for linguistic annotation, particularly if they come with a determinatives. In that case one can often deduce the part of speech or semantic class of the original token, a useful piece of information for annotation. In our example, the fragmentary token {KUR}x has the determiner for a land or country. Thus we know to mark it as a (proper) noun during the annotation phase.We need to make two more comments about the preprocessing stage. First, the above text file for SAA 5, 114 consists of one line, even though the text itself consists of multiple sentences. Keeping the entire text as a single line somewhat simplifies the annotation stage. But depending on the length of the text you are annotating or its particular layout, you may find it convenient to break up the text into several lines based on sentence breaks. This can make the annotation phase somewhat simpler since the annotation program we recommend below will regard each line as its own unit, allowing you to analyse the entire text piece by piece. An example of how this could be done is given below.  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazana uptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša {KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupu šarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lā tašpuraIn particular, if the text you are annotating is in verse form, where each line represents its own sentence, introducing such line divisions into the text file may be a judicious choice. Alternatively, introducing line breaks where the cuneiform tablet has a line ruling (indicating a strong section break) or where there is a clear shift in topics within the text (which tends to be represented in English translation by a paragraph indentation) can also be judicious. In general, however, the choice of where to break up a prose cuneiform text into sentences can be somewhat subjective. Furthermore, once you start annotating separate lines of a text you cannot easily ‘merge’ the lines back together (say, if you made a mistake in where to divide the text) without having to start over from scratch again. For this reason, we recommend being conservative in introducing line breaks in the text file.If you are annotating multiple cuneiform texts, a good practice is to haveeach text in its own file, where the file is titled in a way that you(or the computer, if using a processing script) can easily identify whatthe text is. This may mean using numerical indices in the file nameswhose interpretation you record in a separate list. The image below illustrates this schema. It is a directory listing of many files, each of which containes a single cuneiform text in transcription. Each file is labelled with the P-number that uniquely identifies the text it contains within the CDLI catalogue.Example of files names.Alternatively, you can often put all of the texts in a single file, separated byempty lines and special comment lines identifying the text.Each approach has its advantages depending on your purpose for annotating and the toolsyou use. Generally if you are scraping your texts from an online repository it is easier to store them in separate files.Making the AnnotationsTo create the annotation metadata for a text you need aprogram that will allow you to view the text and add special symbols andnotation to it. By now there are a number of free programs aimed athumanists seeking to add all sorts of metadata to digitized texts,freely available for download or use online as a web application. Someof these programs are mainly used for highlighting thematic relationsbetween passages or phrases in a text or for connecting entitiesmentioned in the text to an external specialized vocabulary such as adatabase of maps or biographies. In our case (doing linguisticannotations), you want to make sure your tool can annotate lemmas,syntactic dependencies, and morphological features at the minimum.One very reasonable annotator program we recommend for Akkadian is Inception. It is freely downloadable as a Javaapplet that works directly through your internet browser. It is capable of handling lemmas, syntactic dependencies, and morphological feature specification in a fairly intuitive manner. The following screen shot shows what it looks like when annotating the normalized version of SAA 5, 114 introduced above.Annotating SAA 5, 114 in Inception.The rest of this tutorial will discuss annotations done in Inception.Generally, if you are doing your own morpho-syntactic annotations from the ground up, by hand, you start by importing the raw text file of the cuneiform text in transcription into Inception. This looks something like this:A raw transcription file imported into Inception.Annotation layersOne should think of annotating a text as a collection of more or less disjoint labeling tasks. Each task can be thought of as applying a ‘layer’ of labels to all of the tokens of the text, and indeed, Inception uses the term ‘layer’ to describe the various annotation tasks you can use the program for (including custom labelling tasks). For the sake of efficiency, you want to tackle each specific labeling task in turn rather than trying to do them all at the same time. For example, you first go through and assign the part of speech tags to each token in the text. Then you go back and assign lemmas to all the tokens. Then you provide morphological parses for all the tokens, etc. This ‘serial-wise’ way of doing things in Inception is easier because you must switch layers in the graphical interface whenever you wish to switch from one labeling task to another.Visually, Inception displays the labels you apply as colored layers stacked above each token. In the SAA 5, 114 example above, the lemma for a given token is given in the orange box right above the token. For instance, the lemma for the form uptahhir is pahāru. Above that are the morphological parses, and then the part of speech tags. Syntactic dependencies are indicated by directed arrows among the tokens.Some of the default layers used in Inception will now be discussed in turn.Part of speech layerInception provides two fields for part of speech tags. The first is the universal part of speech tag (UPOS). It is based on the UniversalDependencies (UD) framework and is meant to apply for all languages. It appears on the left-hand cell of the part of speech tag above a token. The second kind of tag is provided for language-specific or more customized part of speech tags and is abbreviated XPOS. It appears on the right-hand side of the part of speech tag. In the case of Akkadian and Sumerian, a convenient set of XPOS tags to use is provided by Oracc and is listed here (along with corresponding UPOS tags).Syntactic layerIn terms of syntax, Inception uses the UD framework because it is well-suited for working with many different languages. Unlike other grammatical formalisms, UD is based on the idea of syntactic dependency.Syntactic dependencies indicate grammatical dependencies among words andphrases such as subject and object of a verb, an adjective modifying anoun, or a conjunction connecting two full sentences. Overall, one makes semantically ‘light’ terms like prepositions and particles dependent on semantically more important terms like nouns and verbs. Syntactic dependencies are asymmetric relations betweentwo words or phrases, one of them being the head and the other thedependent. One may think of annotating the syntactic dependencies of a sentence asessentially constructing a directed graph or tree, where the dependencies between tokens in a text are expressed by directed arrows. Inception reflects this fact visually in an intuitive manner.In the second sentence within the SAA 5, 144 example figure, there is an arrow going from the verb uptahhir to the noun Urarṭaya ‘the Urartian’. The arrow’s direction indicates that the noun is dependent on the verb, and the label ‘nsubj’ indicates the noun is the nominal subject of the verb. Creating such a dependency is a simple manner of selecting the Syntax layer and drawing an arrow from the head token to the dependency token. You select the label for the dependency in the lower right.Although the program will not prevent you from doing so, it is considered semantically anomalous in the basic UD framework to have a form syntactically dependent on more than one head form. This is because of what syntactic dependence means in UD (putting aside the so-called enhanced dependencies system). In Inception, this anomaly would be represented by having arrows from two different forms leading into a third. Although the program will not prevent you from doing this, such ‘improper’ graph structures will not only be unclear to human readers, it will also create problems for machine learning algorithms using your annotations for training data. You should thus strive to avoid making such mistakes in your annotation (remembering that it is perfectly normal for a single form to have multiple forms dependent on it, i.e. to have multiple arrows leading out of it). A more subtle problem arises when you have a cycle in your annotations, i.e. where there is a chain of arrows starting from one form and ultimately leading back to the same form. This type of error can arise when you are annotating a very complex or long sentence. It, too, creates problems for machine learning algorithms but can be hard to discover until you gain experience in ‘reading’ Inception dependency graphs.Note also that because most cuneiform texts do not mark sentence divisions, it is up to the annotator to decide whether they want to indicate sentence divisions at the layer of the base text file (by separating the text into separate lines) before annotating. If you wish to annotate the whole text in Inception without first making line breaks, the only clear marker of sentences or clauses will be where one annotation graph stops and the other begins. In other words, sentences are defined graph-theoretically by connected components. What you should remember is if you have broken your base text into separate lines (where each line represents its own sentence), you cannot draw arrows across sentence boundaries in Inception. If you initially made a line break in the text thinking it was a sentence boundary but while annotating realize you made a mistake, you cannot easily undo your mistake. You must either go back and fix the base text and start annotating from scratch again, or tolerate the error in Inception and continue. For this reason, it is sometimes better to be conservative and not make breaks in the underlying text unless you are sure there is a sentence break.Morphological layerWhen doing annotations for morphological forms in Inception, you should think in terms offeature-value pairs. A feature is a certain grammatical or semantic category such asgrammatical number or gender, verb tense, or definiteness, and a valueis what variant of the category the word expresses via a particularmorpheme. Thus in the English word cats, the plural suffix -s signalsthe value ‘plural’ within the category of grammatical number. Inliked, the -d suffix indicates the value ‘past’ in the category oftense. In Inception, a morphological parse for a token consists of a text string made up of feature-value pairs, where a given pair consists of the feature label on the left, the value label on the right, and an equal sign joining the two sides together. Examples include‘Gender=Masculine’ or ‘Tense=Past’ or ‘Subjunctive=Yes’. The feature-value pairs are concatenated together by the bar sign ‘|’. For example, a noun in Akkadian might have feature-value string ‘Gender=Masculine|Number=Singular|Case=Nominative’. For cuneiform there are currently no strict conventions for what label sets to use for morphological annotation. However, if you wish your annotation scheme to be compatible with other annotation projects, you should check to see what they use and match their schema (or establish some one to one correspondence in labels) before beginning annotation.In Inception, feature-value strings are input on the right side of the screen once you’ve designated a token for morphological annotation. In the second sentence of the SAA 5, 114 figure, you can see the Features window showing the morphological feature specification of the form uptahhir. It begins ‘Gender=Masc|Mood=Ind|…’, which means the form is masculine and in the indicative mood. Inception conveniently remembers the most frequent strings you input in the morphological feature specification window, and will show them to you via a drop down menu as you type. Another way to save time and labor when inputting feature-value strings is to maintain a simple text file listing the most common strings you use during annotation. You can then copy and paste these strings into the window as needed.The Output FileAfter you have finished annotating a text in Inception you need to output the results to a file. Inception allows for a number of output formats, one of them is CONLLU-format. This format conveniently represents the lemmas, syntactic dependencies, and morphological features. A CONLLU-file is a text file where each row represents a single token, and with ten tab-separated columns, each of which specifies linguistic information about that token. The details of the encoding are found at the Universal Dependencies website.The CONLLU file for the annotation of SAA 5, 114 looks as follows:CONLLU file for SAA 5, 114.Note that the CONLLU format encodes token position within a sentence in the first column. Dependency relations are given in the seventh column, and dependency type in the eighth. Thus for the second sentence, the fourth row represents the token Wazana, while the fifth row represents uptahhir. The seventh column of the fourth row has value 5 and the eight column is ‘obl’. That means the token Wazana is syntactically dependent on token uptahhir and the dependency relation is oblique.  You must take care when reading a CONLLU file that you remember the seventh column represents the syntactic head, not the dependent. These are easy to confuse!If your initial text file was broken up into several lines, the CONLLU file output by Inception will consist of multiple ‘blocks’ of indices each starting with 1. Header information and comments are indicated by lines starting with ‘#’. This is illustrated below.CONLLU file representing multiple sentences.Your CONLLU file can itself be imported into Inception for further annotation, become part of a growing treebank project for your language, or serve as training data for a suitable language model.Issues to Consider When AnnotatingLet us step back a minute from Inception and Akkadian, and speak about annotating in cuneiform more generally. If you have not previously annotated a cuneiform text in the specificlanguage at hand it would be good to check how others have annotated inthat language, both in the hopes of making your work compatible withtheirs, as well as becoming familiar with the difficulties they faced inapplying their annotation scheme. Unless you are highly experienced withthe language using modern linguistic categories, you may find that yourinitial label set for morphological features turns out to beinsufficient, or your understanding of a certain syntactic structureturns out to be wrong (or at least problematic). Looking at what othershave done may save you time and effort in the long run. At the sametime, no annotation scheme currently used for a cuneiform language fits the grammatical particularities of that language perfectly, whetherat the level of theoretical description or practical implementation.Depending on the type of text you are working with, you may find yourselfdeviating from the conventions of other annotators or even needing toinvent conventions yourself. What is most important is that you havereasons you can cite for what you do and that you explain those reasonssomewhere in the documentation that accompanies your work. Such documentation often consists of a text file or Markdown file that is meant to accompany the annotation files and processing scripts you bundle together in an online repository such as GitHub (see under Choosing How to Store Results). Being explicit in this file about the corpus you are annotating, the morphosyntactic label sets you use, and use of your various scripts will improve the utility of your work. Not only will it be easier for others to understand and replicate what you did, it will alsohelp you to remember your own policies at a later date and to be consistent in your work.Here are some examples of projects that use linguistic annotations in different languages written in cuneiform: on Anatolian languages like Luwian and Hittite see the eDiAna project and Hittite Festival Rituals, respectively; for Akkadian, Sumerian, Persian, and Urartian see the ORACC lemmatization guidelines, and for Sumerian specifically see the ETCSL project conventions. The article of Luukko, Sahala, Hardiwck, and Linden 2020 outlines a morpho-syntactic annotation scheme for some Neo-Assyrian royal inscriptions, while that of Ong and Gordin 2024 addresses annotation of Neo-Assyrian letters using a spaCy language model. Consulting the last two examples in particular can give you an idea of what the entire annotation workflow for Akkadian consists of.A second issue is whether to approach annotation sub-task by sub-task ordocument by document. If you intend to annotate many documents forseveral features (e.g. syntactic dependencies and morphologicaldependencies and lemmatization), you can either annotate all of thedocuments for a single feature first, and then go back and go over thedocuments for the second feature, etc., or you can do all of thefeatures for a single document while it is still in front of you beforegoing on to the second document, etc. Depending on the tools andknowledge you bring to your work, one method may be more efficient thanthe other. If the syntactic dependencies of a corpus are particularlydifficult to do, you may opt to first go through and do the morphologybefore tackling the syntax alone. If you are working with someone whodoes not know the grammar of the language well but can find the lemmasfor all the tokens in the text on their own, you could delegate the taskof lemmatization to them while you do the dependencies and morphology.While these kinds of considerations are relevant to annotating manysorts of languages beyond cuneiform ones, in the case of cuneiformlanguages there is one thing to keep in mind. Unless the type of textyou are annotating is particularly simple morphosyntactically, it willlikely take you some effort to understand what the text is saying evenin transliteration (or normalization). Thus, going over the text multipletimes for different annotation tasks may be less efficient that doingeverything at once while the meaning of the text is still fresh in yourmind.Finally, be aware of accompanying datasets or machine learning toolsthat can accelerate your work. If your transliterations come from anonline database such as Oracc, they may also come with lemmatization data as part of aJSON file or separate glossary. If you already had to extract the rawtext from such a JSON file, it is only a little more work to grab thelemmas that go with the tokens as well. Similarly, if you are makingannotations of a corpus to train a natural language processing model on it (say a syntactic parser or morphologizer), you can actually start training your model on the portion of the corpus you have annotated early on and thenapply its predictions to the rest of the corpus. Going through andcorrecting the model’s predictions is often faster than going throughthe whole raw corpus unaided. This technique, known as bootstrapping,is particularly effective when repeated multiple times, early on, for alarge corpus.Choosing How to Store ResultsUnless you are annotating cuneiform texts for private purposes and donot want to share your data, you should consider how to make your workavailable to others on the internet. GitHub is an online platformmainly designed for people writing code they wish to share with othersin a controlled, version-specific way. As an annotator, you can use itas a convenient place to store your data and any associated processingscripts, keeping your data project private, open to all, or only toinvited users. The system is designed to synchronize with particularfolders on your local computer, so that the process of backing up dataor uploading newer versions is easy and allows you to compare currentand older versions of files. GitHub is a good place to provide your dataif you envision yourself working on multiple projects in the future, orif you have scripts or other associated code that you need to presentalongside the annotations themselves.If you have annotated your texts in the Universal Dependencies format,you can also make your data available on the UD website (which actually stores its data onGitHub) alongside annotated corpora from dozens of other languages. Youdo need to make sure your annotations conform to their formatspecifications, which are more strict than what has been discussed here. This website already features corpora of Akkadian,Biblical Hebrew, and Hittite, and its general purpose is to facilitate multi-lingual corpus research.A third alternative is Zenodo, a site for publishing academic datasets. The datasets you publish there are linked to your personal account or those of your work group.Final ObservationsWhile annotating itself has some immediate uses, it is often done aspart of a larger language processing task, research program, orpedagogical project. We mentioned some of these uses earlier on, amongwhich were applications to machine learning and language modeldevelopment. The challengesof applying machine learning and developing language models forcuneiform languages are somewhat different from someone working onEnglish. There are fewer pre-existing annotated corpora that one can useto jump-start one’s own annotations. Many natural language processingpackages assume that you are interested in working with a popular modernlanguage, and often come with large datasets from those languagesbuilt-in, without clearly explaining how to apply their code to alow-resource language starting from the ground up. You will likely find it very helpful to haveguidance from someone more experienced in natural language processing ordata science, or to look at some of the online proceedings from NLPworkshops aimed at humanists and specialists in less common languages.One recent workshop at Princetonillustrates how to develop an NLP project for new languages usingspacy. On a broader level, one may also considerthe Digital Humanities Summer Institute with itsvarious course offerings. What may be the most helpful in the beginning,however, is finding someone else who has begun an annotation project inyour language by searching GitHub and getting guidance from them."
    } ,

  

  

  

    
    

    {
      "title"    : "Interview with Nepos Games about their game Nebuchadnezzar",
      "authors" : "Gustav Ryberg Smidt, Michael Wamposzyc",
      "tags"     : "pedagogy, gamification, city-builder",
      "doi"       : "",
      "submit-date" : "2024-02-06",
      "publish-date" : "2024-03-10",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Interview%20with%20Nepos%20Games%20about%20their%20game%20Nebuchadnezzar.html",
      "type"     : "white-paper",
      "summary"     : "Not many games are focused on the ancient Near East, but we've gotten the chance to talk with the programmer of one of them. Nebuchadnezzar is a city-builder game and it's fun. This interview is meant to peak an interest in it and showcase the benefits gamification can have for how we see our research field.",
      "content"     : "Gustav Ryberg Smidt                                                                                                                      1                                                                                                                                                                                                                                                                                                           ;               Michael Wamposzyc                                                                                                                                                                                                                      2                                                                                                                                                                             1 Ghent University              ;                 2 Edinburgh Napier University (UK)                                                      Submitted on: February 06, 2024          Published on: March 10, 2024                                                                          Under peer review                                          Summary: Not many games are focused on the ancient Near East, but we&#39;ve gotten the chance to talk with the programmer of one of them. Nebuchadnezzar is a city-builder game and it&#39;s fun. This interview is meant to peak an interest in it and showcase the benefits gamification can have for how we see our research field.                                      #pedagogy                  #gamification                  #city-builder                        IntroductionWhen we started looking into gamification of the ancient Near East, we wanted to talk to so-called stakeholders and Nepos Games, the creators of Nebuchadnezzar, were first on that list. Since there are only two people behind the game, both of them are well versed with its development. This gives us the unique opportunity to understand all the steps that go into researching and building such a game.Gustav was lucky enough to sit down with Joseph, one of the programmers of the Nebuchadnezzar game. Together with his colleague in Nepos Games, they developed a city builder game based on ancient Mesopotamia. The interview will give you the chance to look behind the curtain through small snippets of videos with commentary and gameplay, to help you better visualize what is going on.The game was published in February 2021 on Steam and currently has one DLC (downloadable content) called the “The Adventures of Sargon.” Our two game-makers met at a company working on Truck simulators, which is exactly what it sounds like: a simulator of driving a long-haul truck. Nebuchadnezzar is different; it is a city builder game where the player must construct a functional city and complete a number of missions tailored to the period of Mesopotamian history they represent.The GameYour main goal is to create a thriving city and economy. There are three basic resources: workforce, goods and money. As they are dependent on each other, the gameplay revolves around a feedback loop between the three. When you build a house, you can accommodate a workforce that creates goods. The goods you accumulate can be used to sustain a bigger population and for the money you earn on trading and taxes you can buy more houses. In conjunction, you need to link the resources with a well-planned infrastructure. This is a city planner game in its simplicity, but it includes features that make it distinctively a city builder game of an ancient Near Eastern city.  You play as the omnipotent ruler, seeing everyone running around doing your bidding! Maybe a slight exaggeration, but you assign the workers and basically define every part of their life. Yet you still have to adhere to an underlying hierarchy imposed by the social structure. There are the low-level workers that can create food and make pottery. They are your worker ants that provide the bare necessities, which is enough for your city to grow in number, not in complexity. For that you need specialized workers. People who can write tablets, be priests or jewelers. It’s a good facsimile of the co-dependency there must have existed between social layers in the ancient Near East.Once you have a few workers to begin with, you better start feeding them, cloth them, and sweeten their life in general. That might be baking bread or milking cattle, but of course this also includes providing the mandatory staple of beer! And you need a diversified economy–bread and butter might be enough for some, but you cannot maintain a thriving community of scribes without clay, or entertain priests without idols of the gods.Trading with other cities was essential for Mesopotamia to flourish, since the area was rather poor in a number of natural resources. In the game you have to source such things like copper from abroad and then you can make that into jewelry that can be traded away. Interestingly, money in itself is not enough to open up new trading routes. You have to practically gain the respect of prospect partners, like we see in the Amarna letters where international power players discuss with each other who is worthy of a certain status.  We really like the effort they put into working on the pantheons in the game. You have a city god per mission who is determined by historical evidence, and then a group of other deities from the same pantheon that you can choose from. Once you open up trading routes to a city from a different pantheon, that city god also becomes available to you. It’s truly a polytheistic society. You can worship your deities by throwing more or less lavish festivals or ultimately by building a temple in their name. The more you honour them, the better the in-game bonuses they give you.  For many people, simply hearing the word infrastructure makes them slightly nod off. But don’t! Stay with us, please. It was as essential back then as it is today. Even though they are misconceptions, the idea that the Romans invented roads and aqueducts is something most men think about on a daily basis apparently. In the game we have to draw water from the rivers via irrigation systems and construct roads that makes the re-distribution of wealth possible. Funnily, you also have to create fire stations.  The new DLC introduces some interesting aspects into a city builder game–mainly warfare. It’s a heavy industry, as it needs materials, food and a workforce. And that’s of course not really different from anything else in the game. So, economically it requires the same considerations as the other parts of the game, but actually invading another city is a whole different ball game.  The InspirationWithout fail, every time an assyriologist meets a new human being, any human being, they fear to be greeted with: “Oh I think I’ve heard about what you study, it’s something to do with the pyramids, right?” And for that reason, it slightly hurt when Joseph told Gustav that a large inspiration for the game was “Pharaoh”, a city builder game set in Egypt. But he then followed that by explaining that the best inspiration for a civilisation based game is the world’s first. As he would say it himself: ‘we demolished the mainstream pyramids and built the hanging gardens’, so now all is good.  It was clear when talking with Joseph that he is fascinated by the ancient Near East and the more details we went into the more excited he seemed to get. His attention to the great structures that were first implemented in the fertile crescent showed, together with the digital art, that he and his co-developer didn’t just consider Mesopotamia through the glasses of exotic otherness. They were trying to showcase the monumentality hidden under the surface in Iraq.They also put a lot of effort into explanatory content. Before every mission, the player is met by an introduction text to the period it’s in. Not surprisingly, the sources for those explanations were Wikipedia and similar sites. Even though Joseph tried to contact a specialist, it was to no avail. This tells us how important it is to help keep those generally available resources up to date.  We really like the artwork as well, which they put some effort into. Instead of mainly taking visual inspiration from existing and oftentimes morally loaded ideas of how the ancient Near East looked like, they did their best to base it on their research and work.ConclusionNebuchadnezzar is not a game that is meant to teach you about the ancient Near East, but one really feels it is based on knowledge of the ancient Near East, and players might accidentally learn something new. What surprised us the most was the ability such a game can have on a more holistic understanding of our field. It helped Gustav to visualise larger Mesopotamian social structures when playing it. Even though it’s not a fully accurate picture of the social structures as he understands them, it can spur ideas of how to potentially model these structures, which elements are important in the social structures, and what happens when you change something specific. McCall in his book from 2023 Gaming the Past: Using Video Games to Teach Secondary History, uses this game as an example of how to critically discuss the agency of individuals in an ancient society (pp. 12–15). In the digital age, a game like Nebuchadnezzar can play a role in how we teach and comprehend the ancient Near East, so we definitely encourage the readers to try it out!"
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts - Challenges and Comparisons of Python and R Resources",
      "authors" : "chatGPT",
      "tags"     : "python, R, pre-processing",
      "doi"       : "10.5281/zenodo.6411251",
      "submit-date" : "2023-04-07",
      "publish-date" : "2023-04-07",
      "peer-review-date" : "2023-04-07",
      "update-date" : "",
      "url"      : "/pages/paper1.html",
      "type"     : "white-paper",
      "summary"     : "This is an example paper created by chatGPT.",
      "content"     : "chatGPT                                                                                                                      1                                                                                                                                                                                                           1 openAI                                                      Submitted on: April 07, 2023          Published on: April 07, 2023                      Peer reviewed: April 07, 2023                                                                        Reviewed by:         chatGPT                                                                                            doi                                                    Summary: This is an example paper created by chatGPT.                                      #python                  #R                  #pre-processing                        About this paperAll the text contained in this paper, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write an example white paper that will include all possible markdown syntax? (different heading types, tip prompts, images, etc.). This will be used as a template example.  The example white paper should be about current challenges in preprocessing ancient texts using available online resources, particularly comparing current resources in python versus current resources in R.  As this is a demo, please include jokes to make it funny and entertaining!Then, for the peer-review process, a new conversation with chatGPT was started where the following prompt was given:  I need you to perform an academic peer review for a white paper submitted to openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Please critically answer the following questions:      should the paper be published in openDANES?    are there any corrections you would suggest to the text, which would make it clearer or strengthen the points the author is making? Please write them out.    are the jokes funny enough? If not, can you improve them?  Then the original version of this paper was copy-pasted.ChatGPT provided the following answers:  Overall, the paper is well-written, informative, and engaging. It addresses an important and timely topic, and provides a useful comparison of Python and R resources for preprocessing ancient texts. Below are my answers to the three questions posed:  (1) Yes, the paper should be published in openDANES. It meets the platform’s criteria of providing tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  (2) There are no major corrections that I would suggest. However, I have a few minor suggestions that may strengthen the points the author is making:      The introduction could benefit from a more specific statement of the problem or research question that the paper addresses.    In the Python section, the author mentions sentiment analysis and questions its usefulness for ancient texts. It may be helpful to provide an example or explanation of why this is the case.    In the R section, the author could expand on the benefits of the tidytext and quanteda packages. What specific features or functionalities do these packages offer?    (3) The jokes are light-hearted and add to the overall tone of the paper. I don’t think they detract from the content, and therefore don’t need to be improved.  Overall, the paper is a valuable contribution to the field of ancient Near Eastern studies and computational methodologies.Then, we asked chatGPT to rewrite the paper with the suggested corrections. The text below is the result of that peer-review.The text has not been adapted whatsoever by a human. This is purely meant as an example of how a formatted paper looks like.Preprocessing Ancient Texts: Challenges and Comparisons of Python and R ResourcesIntroductionAs the field of ancient Near Eastern studies becomes increasingly reliant on computational methodologies, scholars are faced with the challenge of preprocessing ancient texts using available online resources. In this white paper, we will explore the current challenges of preprocessing ancient texts and compare the available resources in Python and R. Specifically, we aim to answer the question: which language is better for preprocessing ancient texts - Python or R?The Challenge of Preprocessing Ancient TextsPreprocessing ancient texts presents a unique set of challenges that are not encountered when working with modern texts. For one thing, ancient texts often come in a variety of languages, many of which are no longer spoken. Additionally, ancient texts may be damaged or incomplete, making them difficult to read or understand. And let’s not forget the challenge of deciphering ancient scripts!But fear not, intrepid scholars! With the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world.Python Resources for Preprocessing Ancient TextsPython has a wealth of resources available for preprocessing ancient texts, including the Natural Language Toolkit (NLTK) and the TextBlob library. These tools can be used to tokenize text and perform part-of-speech tagging. However, while Python also has the capability to perform sentiment analysis, it is unlikely to be useful for ancient texts since we have no way of knowing how the authors were feeling.Of course, as with any programming language, there are some quirks to be aware of. As the saying goes, “Python is the second-best language for everything.” But don’t let that discourage you - with a little perseverance, you can accomplish great things in Python.R Resources for Preprocessing Ancient TextsR is another popular language for preprocessing ancient texts, with resources like the tidytext package and the quanteda package. These tools can be used to perform a variety of text analysis tasks, such as word frequency analysis and topic modeling.The tidytext package offers functionality for performing sentiment analysis specifically for texts in English, which may be useful for some ancient texts written in English. The quanteda package provides a variety of text processing and analysis functionalities, including tokenization, part-of-speech tagging, and keyword extraction. It also offers support for many different languages and scripts.However, R has its own quirks and challenges. As one wise person once said, “R is like a drug. Once you start using it, you can’t stop.” So be careful when delving into the world of R - you may find yourself unable to escape!Comparing Python and R ResourcesAs we have seen, both Python and R offer valuable resources for preprocessing ancient texts. Which language is better for this task ultimately depends on your specific needs and preferences. Python may be more intuitive for those who are new to programming, while R may be better suited for those who are already familiar with statistics and data analysis.ConclusionIn conclusion, preprocessing ancient texts presents a unique set of challenges, but with the help of computational methodologies, we can overcome these challenges and gain new insights into the ancient world. Python and R both have their own strengths and quirks, so the best choice depends on your specific needs and preferences.And remember, when in doubt, just keep laughing - because as the great scholar Confucius once said, “Everything has beauty, but not everyone sees it.”"
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)",
      "authors" : "chatGPT",
      "tags"     : "python, CLTK, ancient Greek",
      "doi"       : "",
      "submit-date" : "2023-04-06",
      "publish-date" : "2023-04-06",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/tutorial1.html",
      "type"     : "tutorial",
      "summary"     : "This is a demo tutorial created by chatGPT.",
      "content"     : "{%- include author_card.html title=page.title item_type=page.type banner-image-source=page.banner-image-source authors=page.authors date=page.submit-date publish-date=page.publish-date peer-review-date=page.peer-review-date update-date=page.update-date tags=page.tags summary=page.summary reviewers=page.reviewers difficulty=page.difficulty doi=page.doi deprecated=page.deprecated -%}About this tutorialAll the text contained in this tutorial, except for this section, was created by chatGPT. It was generated by the following prompt:  We are developing a website called openDANES, a platform for tutorials and white papers about using computational methodologies for ancient Near Eastern studies.  Can you write a demo tutorial that will include all possible markdown syntax? (all heading types, code blocks, images, etc.). This will be used as a template example.  The demo tutorial should be about preprocessing ancient texts using the The Classical Language Toolkit (CLTK) python library.  As this is a demo, please include jokes to make it funny and entertaining!The code in this tutorial has not been validated and the text itself has not been edited only the link to the image displayed in the tutorial has been adapted to display correctly. This is purely meant as an example of how a formatted tutorial looks like.Preprocessing Ancient Texts with the Classical Language Toolkit (CLTK)Are you tired of manually preprocessing ancient texts for your research in Near Eastern studies? Fear not! The Classical Language Toolkit (CLTK) is here to help!What is CLTK?The CLTK is a Python library that provides natural language processing (NLP) tools for ancient languages, including Greek, Latin, Sanskrit, and Old English. With the CLTK, you can tokenize, lemmatize, and part-of-speech tag texts, as well as perform other common NLP tasks.Installing CLTKBefore we get started, make sure you have Python 3 installed. Then, you can install the CLTK using pip:pip install cltkLoading TextsOnce you have the CLTK installed, you can start loading texts. Let’s load the Iliad by Homer:import cltkfrom cltk.corpus.utils.importer import CorpusImportercorpus_importer = CorpusImporter(&#39;greek&#39;)corpus_importer.import_corpus(&#39;tlg&#39;) # import the TLG corpuscorpus_importer.import_corpus(&#39;phi5&#39;) # import the PHI5 corpusfrom cltk.corpus.readers import get_corpus_readerreader = get_corpus_reader(corpus_name=&#39;tlg&#39;, language=&#39;greek&#39;)iliad = reader.raw(&#39;tlg0012.tlg001&#39;) # load the IliadTokenizing TextsNow that we have the Iliad loaded, let’s tokenize it:from cltk.tokenize.word import WordTokenizertokenizer = WordTokenizer(&#39;greek&#39;)tokens = tokenizer.tokenize(iliad)Part-of-Speech TaggingWe can also perform part-of-speech tagging on the tokens:from cltk.tag.pos import POSTagtagger = POSTag(&#39;greek&#39;)tagged_tokens = tagger.tag_ngram_123_backoff(tokens)Lemmatizing TextsFinally, we can lemmatize the tokens:from cltk.lemmatize.backoff import BackoffGreekLemmatizerlemmatizer = BackoffGreekLemmatizer()lemmas = [lemmatizer.lemmatize(token) for token in tokens]And that’s it! With just a few lines of code, we’ve tokenized, part-of-speech tagged, and lemmatized an ancient Greek text. Now, go forth and preprocess with ease!{% include fig_legend.html content=”Figure 1: This is the logo of CLTK” %}  Why did the linguist break up with the CLTK? Because it kept tokenizing everything!Note: This demo tutorial used various markdown syntaxes including headers, code blocks, images, and blockquotes. Use these features to make your tutorials more organized and visually appealing. And don’t forget to add a touch of humor to keep your readers engaged!"
    } ,

  

  

  
    {} ,

  

  

  
    {} 

  

  
]