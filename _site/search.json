[
  

  

    
    

    {
      "title"    : "Assembling Photographed Views of Cuneiform Tablets with Photoshop",
      "authors" : "Luis Sáenz, Louis Happel, Mathilde Touillon-Ricci, Alberto Giannese, Jonathan Taylor, Enrique Jiménez",
      "tags"     : "cuneiform, Photoshop, 2D images",
      "doi"       : "",
      "submit-date" : "2025-03-11",
      "publish-date" : "2025-04-25",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Assembling%20Photographed%20Views%20of%20Cuneiform%20Tablets%20with%20Photoshop.html",
      "type"     : "tutorial",
      "summary"     : "This tutorial provides a step-by-step guide on assembling and processing photographed views of cuneiform tablets using Photoshop, with a focus on automation to ensure efficiency and standardization. It covers essential tasks like converting image formats, adding scales, and stitching photos, while also offering troubleshooting advice for manual corrections.",
      "content"     : "Luis Sáenz                                                                                                                      1                                                                                                                                                                                                                                                                                                                                                                                                           ;               Louis Happel                                                                                                                                                                                                                      2                                                                                                                                                                                                                                                                                                           ;               Mathilde Touillon-Ricci                                                                                                                                                                                                                                                                                                                      3                                                                                                                                                                             ;               Alberto Giannese                                                                                                                                                                                                                                                                                                                      3                                                                                                                                               ;               Jonathan Taylor                                                                                                                                                                                                                                                                                                                      3                                                                                                                                                                             ;               Enrique Jiménez                                                                                                                                                                                                                      2                                                                                                                                                                                                                                                                                                                                                                       1 University of Jena              ;                 2 LMU Munich              ;                 3 The British Museum                                                      Submitted on: March 11, 2025          Published on: April 25, 2025                                                                          Under peer review                                          Summary: This tutorial provides a step-by-step guide on assembling and processing photographed views of cuneiform tablets using Photoshop, with a focus on automation to ensure efficiency and standardization. It covers essential tasks like converting image formats, adding scales, and stitching photos, while also offering troubleshooting advice for manual corrections.                                      #cuneiform                  #Photoshop                  #2D images                                              Difficulty level: beginner                    Documenting archaeological artifacts is a crucial aspect of research, as it provides valuable data for both traditional and computational analysis and preservation. A standardized approach to documenting these artifacts enhances their usability, facilitating more efficient data processing and analysis. Additionally, implementing an automated method to streamline this documentation process is vital. Among these methods, photography has become a standard practice.Cuneiform tablets, as archaeological artifacts, present a particular challenge because the inscribed text must be clearly readable for the preservation method to be useful for any kind of philological or computational analysis.In this tutorial, we will learn how to combine multiple views of cuneiform tablets - front, back, and sides - into a single comprehensive layout. This creates standardized photographic renderings of cuneiform tablets appropriate for publication, following the presentation style used by the British Museum.Figure 1: Rendering of the tablet 1889,0426.114 with front, back and side views housed in the British Museum containing extispicy and ritual directions. © The Trustees of the British Museum.We believe that this is the most cost-effective and efficient way to capture the three-dimensionality of these artifacts and the inscriptions, which often extend to the edges, using a method like photography that can only produce 2D images.The tutorial focuses on automating this workflow with specialized tools, reducing manual intervention while ensuring consistent, high-quality results that meet professional documentation standards.This tutorial utilizes two specialized Photoshop components:  The Photoshop Actions file Babylon_Project_modified.atn, originally created by Alberto Gianese for the British Museum.  The Photoshop JavaScript STITCHING_updated_EJ_LH_LS_Jena.jsx, developed by Mathilde Touillon-Ricci and Jonathan Taylor for the British Museum.These components contain automated sequences that:  Extract the tablet image from its background  Place it on a black background layer  Apply digital processing for stitching the images according to scale  Add a 100-pixel margin to the final image  Save and close the final imageBoth components have been enhanced through subsequent modifications by Prof. Dr. Enrique Jiménez and the other authors of this tutorial. For training purposes, Prof. Johannes Hackl has kindly provided three sets of tablet photographs, for which we extend our sincere thanks. These tablet photographs from the Frau Professor Hilprecht Collection are used throughout this guide and are available for download for practicing the tutorial.  Acknowledgements: These resources were developed over the course of a prolonged period of bulk digitisation at the British Museum, originally for the Ashurbanipal Library Project, then the Ur of the Chaldees project, before being adapted for the Babylon project. The concept is based on a Perl script written by Steve Tinney, which composited scans into CDLI “fat-cross” images. It was not possible to run those scripts using the set-up in place at the British Museum, so an equivalent was developed there that could be run through Photoshop, and would take into account the requirements of working with photos. Resizing and quality control scripts were written by Ana Tam. “Compositor” and “finaliser” scripts were written by Jonathan Taylor, drawing on pieces of code made available by Trevor Morris, the late Mike Hale, and Max Kielland.PrerequisitesBefore applying any action or script, some preliminary steps are required to ensure proper functioning.Preparing Cuneiform ImagesThis tutorial assumes you have already photographed your cuneiform tablets following professional documentation standards. If you do not have images available you can use the ones provided for testing the workflow of this tutorial (see under Preparing Software and Code).If you are using your own images, they should be high-resolution images of all tablet sides, each including a scale and ideally shot against a high-contrast background (preferably black, as shown in Figure 2). A piece of black velvet works excellently as a background material, as it effectively eliminates reflections.Figure 2: A photo of the obverse of HS.2067 (HS.2067_01) with a black backgroundThe most important requirement for your photographs is consistency. Maintain the same camera configuration, lighting setup, and positioning for each tablet. This standardized approach ensures all tablet views will be uniform and scaled the same when assembled into the final rendering.Labeling the photos with the tablets sides:The ideal number of photos is 6, with each file capturing a specific view of the tablet. Using the obverse (front side) of the tablet as your reference point, the photos should be labeled as follows:  TABLET-NUMBER_01 → obverse or front  TABLET-NUMBER_02 → reverse or back  TABLET-NUMBER_03 → top with scale  TABLET-NUMBER_04 → bottom  TABLET-NUMBER_05 → left edge  TABLET-NUMBER_06 → right edgeNote on orientation: Don’t worry if you’re unsure which side is the obverse or reverse - even experienced Assyriologists sometimes can’t determine this at first glance. What’s important is to consistently label one side as the front (01) and the other as the back (02) throughout your documentation.Figure 3: Labeling conventions for tablet views by side.Try to maintain the consistent numbering of photos. In case you don’t have all six photos, the script will still work, but you must preserve the labeling convention (Tablet-number_XX) and content organization shown above. A critical requirement: ensure at least one blue square calibration reference is present, as the script uses it to rescale the final stitched image. This is included in both scale bars we provided for downloading.For a set with only two photos (e.g., Tablet-number_01 and Tablet-number_02), place the scale reference in 02. If you have all photos but 03 is missing, create a placeholder Tablet-number_03 containing only the resized scale reference.  You can use Advanced Renamer to rename the files. This is a software tool designed for renaming multiple files and folders at once. It offers a variety of methods for generating new names and organizing data efficiently.  Make a copy of all photos on your local computer before starting. Never modify the original files!File format and size:The script primarily works with TIFF files, which are optimal for preserving image quality and metadata. While JPEG copies are often required for projects due to their smaller file sizes, making them better suited for web usage and sharing, the primary output remains in TIFF. The script can generate JPEG copies of the TIFF files when specified. If your project requirements don’t specify a format, you can always convert the stitched TIFF files to JPEG format as a separate step after the main processing.  Save your stitched images in either compressed or uncompressed TIFF format based on your project requirements. If no specification is given, save uncompressed - you can always compress files later in batch. See Compress *.TIFF to LZW or ZIP format for details.  You can set your final image resolution to either 600 or 300 DPI based on your project requirements. If no specification is given, use 600 DPI - you can always reduce resolution later in batch. See Reduce the DPI of Stitched Images for details.Additional Images and MetadataInstitution Logo: The script allows you to add an institutional logo to the stitched image, similar to those used by the Hilprecht Collection.Figure 4: Stitched image of HS.2067 with the University of Jena logoYour institution must provide a digital version of their logo. Use a fixed size for the logo, as dynamic scaling can produce unwanted effects. The script will only resize the logo if it exceeds the width of the stitched image, reducing it to 70% of its original size. For reference, use the 5cm scale shown in the HS.2067 image.Alternatively, you can use a simpler approach by incorporating the logo into the scale bar, as demonstrated by the British Museum:Figure 5: British Museum 5cm color scale with logoMetadata:Photoshop can embed basic metadata using its native standards and some IPTC fields, though not all. This script implements minimal metadata using both Photoshop’s standard and selected IPTC fields.For more comprehensive IPTC metadata, we recommend using either Adobe Bridge or ExifTool for batch processing. Adobe Bridge is the preferred option, as ExifTool may encounter issues with special characters and lengthy text entries.Preparing Software and CodeInstall Adobe Photoshop on your computer. This workflow has been tested with Adobe Photoshop versions 2021 through 2025. Both the Photoshop Actions and JavaScript files are Photoshop-specific and are not compatible with other image editing software like GIMP.  Note: If the Actions don’t work as expected in your Photoshop version, please notify the tutorial authors or the OpenDANES team.Download Files:Download the required files from the eBL project’s GitHub repository.Scripts and Actions:  cuneiform_documentation_bm.atn  stitching_script_bm.jsx  Resize Scale 2cm.jsx  Resize Scale 5cm.jsxScales:  BM_5cm scale.tif  BM_2cm scale.tifThe scale bars BM_5cm scale and BM_2cm_scale are essential components of this script, as they contain a blue square that serves as a reference for resizing the final image correctly.For testing this workflow, we provide example sets:  HS.1671: 6 photos for stitching with 5cm scale bar  HS.2067: 6 photos for stitching with 2cm scale bar bar  HS.1662: 2 photos for stitching  mock_logo_tutorial: Institutional logo sampleLoad the ActionsTo load actions in Photoshop:  Open the Action panel:          Navigate to Photoshop &amp;gt; Windows &amp;gt; Actions or      Use the shortcut Alt+F9            Figure 6: Activation of the Actions Panel.    Import the custom actions:          Click the Actions panel menu      Select Load Actions...            Figure 7: Load operation for the custom action Babylon_Project_modified.atn.    Select and load the file:          Locate Babylon_Project_modified.atn      Click Load      The actions will now appear in your Actions panel under the Babylon Project folder.You can modify these actions or create your own as needed. With the actions loaded, we can proceed to pre-processing.Pre-ProcessingConverting Camera Files to TIFF FormatIf your photos are in any of these camera formats:  RAW formats (*.CR2, *.NEF, *.ARW, etc.)  Digital Negative (*.DNG)  Camera-specific formats (*.RW2, *.ORF, *.PEF)Follow these steps to convert them to TIFF:  Note: If your photos are already in TIFF format, you can skip the conversion process. Simply create the _PreProcess folder and place your TIFF files directly there.  File Organization:          Create a folder named _ToProcess      Create another folder named _PreProcess      Copy all your camera files into the _ToProcess folder        Using Adobe Photoshop:          Open Photoshop              Navigate to File &amp;gt; Scripts &amp;gt; Image Processor                Figure 8: Activation of the image processor dialog box.            In the Image Processor dialog box:                  Set the source folder to _ToProcess          Set the destination folder to _PreProcess          Check the “Save as TIFF” option          Click “Run” to begin the conversion                    Figure 9: Selection of TIFF conversion parameters.Rotating PhotosAfter converting to TIFF format, ensure all photos are oriented correctly for reading the cuneiform text. The correct orientation should match how you would naturally read the cuneiform text on the tablet. If multiple photos need the same rotation:In Windows File Explorer:  Navigate to the _PreProcess folder  Select all photos (Press Ctrl + A)  Right-click to open the context menu  Choose either Rotate right or Rotate left as neededSeparation of Photos with Suffix _03For photos showing the scale (those with suffix _03), we need a separate workflow since they require digital scale placement. Follow these steps:File Organization:  Navigate to the _PreProcess folder  Create a new folder named PreProcess_03  Locate all photos with the suffix _03  Move these _03 photos to the PreProcess_03 folderThis separation ensures that scale-containing photos will undergo different processing steps from the rest of the photos.Once you have:  Prepared and labeled the images correctly according to tablet’s sides  Converted your camera files to TIFF format  Downloaded and loaded in Photoshop the necessary actions  Rotated the images if needed  Separated the images with suffix _03You’re ready to proceed with extracting the tablet images from their backgrounds using Photoshop Actions.  You can process multiple tablets simultaneously. The code will combine tablets photos according to their file names.Processing Photos without Scale  Create a folder named _Processed  Open Photoshop      Navigate File &amp;gt; Automate &amp;gt; batch        Figure 10: Batch Dialog Box Operation    Under Play in the Action menu, check the options under Set until you find the name of your project (Cuneiform Documentation or another name assigned by you such as Babylon Project). Then select Views 1-6 BATCH under Action.      Set the source folder to _PreProcess.          Note: If you converted the images to TIFFs make sure your source folder is set to the folder with the actual images in it, not just a parent folder or check include all subfolders.        Set _Processed as the final destination.Figure 11: Action Application Options for &#39;Views 1-6 BATCH&#39;  Important: The action uses the secondary color as the background, so ensure it is set to black. To change the secondary color to black, click on the background and secondary color icon.Figure 12: Background Color Selection in Color Picker  Disable all action toggles to prevent Photoshop from requesting approval before applying each action.Figure 13: Action Toggle DeactivationPhotoshop will then use the Object Selection Tool to automatically separate the tablet images from their background and apply various digital enhancements before saving and closing them. The final output should look like this:Figure 14: Result of &#39;1-6 View BATCH&#39; Action applied to HS_2067_01  The batch process may need to be run multiple times, as some photos might not be processed correctly on the first attempt. In most cases, running the process two or three times will successfully process all remaining images. If any photos remain incorrectly processed, please refer to the troubleshooting section.  Note: For faster access, you can assign a Function Key (such as F6) to the Views 1-6 Batch action. Simply select the action, click the menu icon, select Action Options, and assign your preferred function key.Figure 15: &#39;Views 1-6 BATCH&#39; Action Configuration.Processing Photos with ScaleThis section covers the processing of photos with the prefix _03. It will involving adding the scale bars and resizing them to accurately represent the tablet size.  Note: This step includes a manual adjustment that cannot be automated. However, after performing this manually once you can then perform the process automatically (see Automatic Application of the Action). You can also create a Save and Close action to streamline the process slightly.  Navigate to File &amp;gt; Automate &amp;gt; Batch  Select View 3 HP from the Action dropdown menu  Set the source folder to your _03 photo directory (e.g., PreProcess_03).The process creates two layers for each photo: HP and background, which are essential for subsequent operations.Figure 16: &#39;View 3 HP&#39; Action Configuration.Adding and Resizing the Scale Bar  Open each photo with the _03 suffix      Select the Line Tool (press U on your keyboard)          The Line Tool is under the Shape Tool in the left-side toolbar. If you press U, make sure the shape is set on Lineand not something else. You can right-click on the Shape Tool to change the shape type.                Using the scale visible in the photo, draw a line exactly 1 cm long        Figure 17: Line drawn to 1 cm length for scale bar calibration.    Check that a new layer named Line 1 is created in the Layers panel. If the layer has a different name (this may occur in non-English versions of Photoshop), rename it to Line 1 or change the required Javascript to process another name:          Open Resize Scale 5cm.jsx in any text editor, like notepad.      Find this line (around line 6)var layer = activeDocument.layers.getByName(&quot;Line 1&quot;); //Grab the currently selected layer      and change &quot;Line 1&quot; for the name generated in your local version of photoshop.      Save and close.      Do the same with Resize Scale 2cm.jsx        Repeat steps 2-4 for all remaining images  Save your workManual Set-up of JavaScript Sources  With the _03 image file open, locate and select View 3 Place 5cm Scale in the Actions window      Select the Place action within this group and start Recording        Figure 18: Start Recording of Scale Addition Action    Import the BM_5cm_scale.TIF file into your Photoshop workspace as another layer of the _03 photo using either:          File import      Drag and drop from your source folder (make sure to drop the BM_5cm_scale.TIF on the image so it is added as a layer and not as its own image). After dropping, press enter.            End the recording        Figure 19: Recording Termination of Scale Addition Action    Delete the original Place action (select it and press the bin at the bottom right of the Actions window)Now, you can rescale BM_5cm_scale.TIF to match the cuneiform tablet:  Initiate the action recording, like in Figure 18.  Navigate to File &amp;gt; Scripts &amp;gt; Browse  Locate and select Resize Scale 5cm.jsx (the action will probably fail but that is ok). This should rescale the BM_5cm_scale.TIF layer.  End the recording.  Note: The action may fail at this point - this is expected behavior.After completing the process for the 5cm scale, go back to the beginning of this section and repeat the instructions for the 2cm scale. You should see View 3 Place 2cm Scale in the actions window below the View 3 Place 5cm Scale.Then, for each _03 image you should only keep one scale bar (the 2cm or 5cm).  Choose a scale bar that fits within the tablet’s width. The JavaScript measures the image width, not the tablet width. If the scale bar is wider than the tablet, it will cause distortion in the final image, making that side appear disproportionately small.Position the scale bar approximately 0.5cm below the tablet’s bottom edge.Automatic Application of the ActionAfter completing the manual action above once, you have two options for applying the action and adding the scale bar to all other _03 images:  Individual Processing          Open each _03 image separately      Apply the appropriate action based on scale size (2cm or 5cm)        Batch Processing          Sort your _03 images into two separate folders:                  One folder for tablets using 2cm scale          One folder for tablets using 5cm scale                    Navigate to File &amp;gt; Automate &amp;gt; batch      Select the appropriate action                  View 3 Place 5cm Scale or          View 3 Place 2cm scale.                    After running the action, each image window will remain open with the resized scale bar. Like before, position the scale bar approximately 0.5cm below the tablet’s bottom edge      Final ProcessingRemove the background removal from your _03 photos:  Navigate to File &amp;gt; Automate &amp;gt; batch  Select View 3 Crop in the the Action dropdown menu.  Set your source folder to the directory containing your _03 photosFigure 20: Settings for removing the backgroundThis action will:  Remove the original background  Create a new black background  Merge all layers into one  Save and close the file automaticallyFile Consolidation:Transfer all processed _03 photos to the _PreProcess directory containing your other photo files. Individual subfolder organization is not required, as the script automatically groups photos with matching filenames. This consolidation step ensures all related photos are in place for the automated stitching process.Figure 21: All views of the tablet ready to be stitchedRunning the Stitching ScriptOnce all photos are processed as described earlier:  Open Photoshop  Navigate to File &amp;gt; Script &amp;gt; Browse  Locate and select stitching_script_bm.jsx.Figure 22: Navigation to the script stitching_script_bm.jsxThe script interface has two tabs, Processing Options and Metadata:Processing Options:Figure 23: User interface tab Processing Option.  Source folder: select _PreProcess  Destination folder: Create and select a folder like _Processed or finished.  Never place the destination folder inside the source folder - this will cause a script error as it will attempt to process the new folder  Logo inclusion toggle  Final resolution settings  Compression type (if needed)  JPEG copy creation (optional). For JPG files, a compression setting of 8 provides the optimal balance between quality and file size.Metadata:Figure 24: User interface tab Metadata.  General Metadata section:          Fill in all fields as needed        IPTC Metadata section:          Only modify the Credit Line field      Other fields auto-populate from General Metadata and tablet information        Note: Your settings will be saved for future use, requiring changes only when necessary.Running the Script:  Click OK to start processing.  The script will          Sort through your folder      Classify photos by filename      Create individual folders for each set        Note: For maximum efficiency, preprocess as many photos as possible before running the script.The stitching_script_bm.jsx performs these operations:  Creates black background and places all images  Arranges images based on filenames  Mirrors images with _05 and _06 suffixes for reverse side alignment  Embeds metadata including:          Photographer name      Credit lines      Title        Optional institutional logo placement  Compression options:          LZW or ZIP compression      Optional JPEG export      Post-ProcessingAfter running the script:  Review all assembled photos  Identify any incorrect assemblies  For persistent issues, process problematic images manually (see troubleshooting).Convert from *TIFF to *.JPEGIf required for your project you can convert the \*.TIFF files to \*.JPEG using Photoshop’s Image Processor:  Open Photoshop      Navigate to File &amp;gt; Scripts &amp;gt; Image Processor.        Figure 25: Navigation to Image Processor.    This will open the Image Processor user interface. Specify the settings for converting your files.        Enable Save as JPEG as the file type.        Figure 26: Settings for saving as JPEG.    Set your desired quality  Select destination folderCompress *.TIFF to LZW or ZIP formatDepending on the specifications of your project you can compress the *.TIFF file to LZW or ZIP format:  Open Photoshop  Navigate to File &amp;gt; Scripts &amp;gt; Image Processor  Select source folder containing TIFF files  Choose destination folder  Enable “Save as TIFF”  Check LZW compression.If you need ZIP compression. You can use Actions and Batch Processing:  Record a new action  Open a TIFF file  Save it as TIFF with your desired compression  Stop recording  Go to File &amp;gt; Automate &amp;gt; Batch  Select your action and source folderReduce the DPI of Stitched ImagesIn Photoshop:  Navigate to File &amp;gt; Scripts &amp;gt; Image Processor  Select your source images  Choose destination folder  Select output format          “Save as TIFF” or      Your preferred format or      Both formats        Enable “Resize to Fit”  For 300DPI resolution, enter:          Width: 1500 px      Height: 2000 px        Click Run to process imagesTroubleshootingRemoving the Background ManuallyIn some cases, the action Views 1-6 BATCH might not perform the background removal as expected, particularly if there is insufficient contrast between the tablet and the background. If this happens, you will need to manually cut out the tablet. Follow these steps:  Open each photo individually.  Select the tablet within the photo using the Object Selection Tool (W).  Apply the Views 1-6 action.This action performs the same background removal process as Views 1-6 BATCH, but starts with a predetermined selection, which can be useful when the automatic selection fails.Figure 27: Location of the Obejct Selection Tool.Stitching Photos ManuallyIf the stitching script does not work as expected, you can manually stitch the photos using Photoshop:  Create new file          Open Photoshop      Create a new file with dimensions 8000 x 8000 px      Set resolution to 600 dpi.        Import and arrange photos          Open all source photos      Copy and paste each photo into the new file, one at a time.      Arrange the photos in the correct order, ensuring they align properly.              Important: Adjust the size of the photos according to the dimensions in photo _03 (the one with the scale). Do not alter the photo with scale!        Process reverse edges          Copy and paste the left and right edges      Rotate them 180 degrees      Place adjacent to corresponding reverse sides        Final processing          Select the entire composition with the Object Selection Tool (or W)      Ensure that the entire tablet and scale are included in the selection.      Crop: Image &amp;gt; Crop      Merge layers: Layer &amp;gt; Merge visible or press Ctrl + Shift + E      Save and close the file      Final RemarksWe trust this tutorial has helped you to establish an efficient workflow for creating standardized tablet documentation. While the scripts and actions automate most tasks, manual intervention options ensure you can handle any technical challenges that may arise. For updates, bug reports, or additional assistance, please contact the tutorial authors or the openDANES team."
    } ,

  

  

  
    {} ,

  

  

  

    
    

    {
      "title"    : "Digital Ancient Near Eastern Studies - A Transition to Arts and Crafts",
      "authors" : "Shai Gordin, Avital Romach, Eliese-Sophia Lincke, Hubert Mara, Aleksi Sahala, Marine Béranger",
      "tags"     : "opinion, open access, DANES network",
      "doi"       : "10.5281/zenodo.10894909",
      "submit-date" : "2024-02-16",
      "publish-date" : "2024-02-16",
      "peer-review-date" : "2024-03-29",
      "update-date" : "",
      "url"      : "/pages/Digital%20Ancient%20Near%20Eastern%20Studies%20-%20A%20Transition%20to%20Arts%20and%20Crafts.html",
      "type"     : "white-paper",
      "summary"     : "A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.",
      "content"     : "Shai Gordin                                                                                                                      1                                                                                                                                                                                                           2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ;               Avital Romach                                                                                                                                                                                                                                                                                                                      3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ;               Eliese-Sophia Lincke                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                 ;               Hubert Mara                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      5                                                                                                                                                                                                                                                                             ;               Aleksi Sahala                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6                                                                                                                                                                             ;               Marine Béranger                                                                                                                                                                                                                                                                                                                                                                                                                      4                                                                                                                                                                                                                                                                                                                                                                                                                                         1 Ariel University              ;                 2 Open University of Israel              ;                 3 Yale University              ;                 4 FU Berlin              ;                 5 MLU Halle-Wittenberg              ;                 6 University of Helsinki                                                      Submitted on: February 16, 2024          Published on: February 16, 2024                      Peer reviewed: March 29, 2024                                                                        Reviewed by:         Massimo Maiocchi                                                                                                                              ; Michele Cammarosano                                                                                            doi                                                    Summary: A brief history of computational studies of the ancient Near East, and an introduction to the organizational structure, running actions, and vision of the DANES network to the wider Ancient Near Eastern studies community.                                      #opinion                  #open access                  #DANES network                          “The use of electronic data-processing devices for research in the fields of linguistics and philology is by now fairly common.” (Hans G. Güterbock, “The Hittite Computer Analysis Project”, from the 1967-1968 annual report of the Oriental Institute, currently the Institute for the Study of Ancient Cultures)Digital studies of the ancient Near East are not new. If anything, within the fields of assyriology, egyptology, and archaeology, one can find some of the pioneers embracing the advent of the computer age. Today, there is new interest in applying computational methods to the humanities and social sciences, and to the study of the ancient world specifically, given the potential of such interdisciplinary initiatives. However, there are questions that are fair to ask:  can the computer really teach us something new about our fragmentary, complicated texts and artifacts? Is this a movement of new arts and crafts, providing us with fresh perspectives, or are these arts and crafts a trending hobby, not really worth the scholarly effort?In order to apply computational methodologies, there is a need for shared language, interdisciplinary work, and a willingness to constantly learn outside of your field of specialty. Exploration of texts and artifacts as data, with the plethora of tools and techniques available today, is fundamental, yet figuring out where to start can be daunting. There is a need to design experiments, that might fail, and show no results except (bitter) experience. Even the significance of successful results are not always appreciated by your peers. Where computational analysis ends or should end and humanistic inference begins can at times be opaque. Lack of standards and resources for computational research in ancient Near Eastern (ANE) studies and neighboring disciplines makes the interpretation and significance of such work vague and unclear to the non-specialists.Let us give such an example. Below you can see graphs generated from the network of the Mesopotamian Ancient Placenames Almanac (MAPA); for further information see Gordin, Clark and Romach (2022) and Clark and Gordin (2023). The points on the graph are toponyms attested in texts from the hinterland of the city of Uruk in the first millennium BCE. Their position on the graph is a result of their Degree in the network—how central they are as hubs—and their Betweenness Centrality score—how important they are as a bridge to get from one location to another. Without minimal background in graph theory, it is hard to understand the value of such visualizations, and whether they add new information to our traditional ways of understanding connections and links between geographical locations. The transformation of the texts into data—from sentences to word lists, to toponyms represented in a network, to a graph of their scores—distances us from the artifacts, the original texts. These transformations are so substantial, it is the equivalent of looking at a sword and trying to understand how it was forged without seeing the process or knowing anything about the tools and methods used.In this piece, we argue that it is worthwhile to dedicate the time to learn these arts and crafts. We begin with what resources were created before us, where we stand today, and what we hope to gain in the future. We outline the initiatives of the Digital Ancient Near Eastern Studies (DANES) network, whose purpose is to increase the number of apprentices, by providing both basic literacy to understand computational methods, and advanced practical applications to those who want to become artisans in the field.FoundationsThe early pioneers created large databases which are the baselines for many computational works performed today, such as (in alphabetical order) the Coptic Scriptorium, Cuneiform Digital Library Initiative (CDLI), Hethitologie-Portal Mainz (HPM), the Open Richly Annotated Cuneiform Corpus (ORACC), Papyri.info, the Perseus digital library, and Thesaurus Linguae Aegyptiae (TLA), to name a few. Most of these efforts were placed at digitizing ancient sources. This was a colossal effort with the technology of the time, when storage space was limited, when punch-cards were frustrating, when non-Latin characters were not supported.The first wave of digitization now seems lost to recent history, with many mixed results that cannot be fully surveyed here (see Sahala 2021, ch. 2). Centers like the Centres de mécanographie documentaire pour l’archéologie (CDMA, later CADA) initiated in 1957 by Jean-Claude Gardin, were not only the first to make machine readable versions of Mesopotamian texts (Plutniak 2018), but also published computational historical studies using, for example, graph theory and network analysis (Gardin &amp;amp; Garelli 1961 on Old Assyrian commercial networks). Other foundational projects, that continue until this very day in different iterations, include the Neo-Assyrian letters of Simo Parpola in Helsinki during the 1960’s (now the Neo-Assyrian Text Corpus Project, and the related Munich MOCCI project), and Buccellati’s Cybernetica Mesopotamica (see its many publications, as well as a topical index).Criticism was not lacking towards these revolutionary endeavors. Were digitization and computational studies worth the effort? Was new knowledge obtained that could not have been gathered otherwise? Such setbacks conceptually made it seem as though the computer revolution has not fundamentally touched upon the fields of the ancient Near East. There are still general misunderstandings regarding the inherent differences between print publications (which include PDF versions), and plain-text digital scholarly editions that are also published on online platforms (see Sahle 2016).From the early 2010s, the AI revolution reinvigorated the use of computational methods for the study of the ancient Near East. Breakthroughs in computer science, increased storage capabilities for big data, the development of the internet, the creation of programming languages and environments that are more user-friendly, and the extension of the Unicode standard, opened new lines of research.At the current moment, we think a conscious choice needs to be made on how we envision the field going forward. If we want to promote new methodologies, we cannot dismiss established ones. If we want to promote such new methodologies alongside established ones, computational research needs to be communicated in a way that is still understandable to all relevant fields. We need a shared language that includes basic literacy of terms, common problems and possible solutions (see e.g. Homburg et al. 2023). As basic digital literacy becomes more common, more scholars and students will desire to expand their knowledge, use digital tools, gain more programming experience, etc.This is particularly important as computational methods are shared across the growing field of digital humanities. A shared set of methodologies is a powerful link between fields, creating a shared jargon, and it is important that we propel the study of ancient cultures to current research avenues.Training in the PresentAll the above requires training—both of humans and machine learning algorithms. The barriers to computational studies today for those coming from the humanities, compared to the challenges of the 20th century, are very different. Yet, the big difference is that today it is easier to lower said barriers, due to the ease of sharing information, tools, and knowledge online. There are many digital humanities’ initiatives, platforms, and resources, to train and aid humanists to learn anything from basic digital literacy to developing advanced machine learning applications (such as the Programming Historian, The Digital Orientalist, or DARIAH-Campus). These general resources are invaluable, but sometimes they are insufficient when applying new methodologies on ancient Near Eastern data, either in the form of text, object, image, or landscape.Furthermore, current technologies, especially generative AI models, offer new possibilities. Large language models (LLMs) have transformed the ways in which we interact with texts only in the last few months. Many are wondering what is the place of humans in this future, now that generative models such as GPT and image generation models are revolutionizing the way we write and create. Barriers such as the knowledge of how to write code may seem obsolete, now that anyone can use chatGPT with natural language input to program. Nevertheless, that does not mean that anyone can write useful and functional code, or gain meaningful results.It is easy to trick or ask ChatGPT questions the model was not trained for, and receive incorrect results. When asked to translate a pivotal sentence in Hittite that was used for its identification as an Indo-European language, the translation is only partially correct, with many caveats elaborated upon by the model, such as the incorrect statement that Hittite and Latin are from two different language families!For humanists to be able to understand how to work with such tools, and gain meaningful results from AI models, digital literacy is a prerequisite. In the above example, knowing more about what language models are to begin with, what data they are trained on and how it affects the model’s results, allows humanists to judge their results accordingly and also teaches humanists how to give only reasonable tasks to models to begin with.On the other side of the coin, those with computational background are often not familiar enough with the intricacies of ancient cultures, languages, writing systems, and artifacts in order to design models that will provide historically sound results or meaningful conclusions. Conceptual understanding and structured ontologies are imperative for scholars from the humanities to better contribute to the methodological discussions in the interface between computational methods and humanistic approaches.Furthermore, despite the foundational works of the 20th century, we are still facing issues that are waiting for state-of-the-art innovations when studying ancient cultures computationally. We should update our best-practice methods for preserving and digitizing our objects of study. While the vast majority of image data are photographs and flatbed scans, the number of acquired and available 2D+ and 3D models is increasing. 3D models specifically provide precise representations that allow the exact measurements of minute details such as fingerprints, seals and damaged characters. Standardized high-resolution, high-contrast representations have also been shown to improve AI-based approaches to optical character recognition tasks (Stötzner et al. 2023). To take advantage of these new technological possibilities, 3D scanning needs to be put on the agenda, and its benefits for research and object preservation need to be clearly communicated to the wider research community.For fully transparent research and seamless workflow, these material remains need to be linked to metadata, or text editions if there is any writing on the objects. Linked open data (LOD) is a set of standards for linking data (be it text, artifact, metadata, and more) over the web in certain formats, following specific ontologies agreed upon by a community of experts. The use of LOD in ancient studies is growing (e.g. Pelagios network), and it requires consensus and discussions to keep the data interlinked.Digital monolingualism and the primacy of the Latin script have led to software development lagging behind a fundamental need for full Unicode and font support in order to include non-European scripts and languages. Natural language processing (NLP) models designed primarily for English and maybe some other European languages require adaptations when applied to ancient languages. The need for a lobby of ancient Near Eastern studies and other disciplines sharing these concerns is obvious (see for example the DHd Multilingual Digital Humanities working group), and ancient language processing (ALP) is becoming a growing initiative within the NLP and machine learning community (see e.g. Anderson et al. 2023, Bin and Gordin 2023, and the ML4AL workshop @ ACL 2024).In the next section, we elaborate on the work of the DANES community to furnish some of the prerequisites necessary for meaningful computational and collaborative research, and to promote research on some of the current issues presented above.Current and Future ActivitiesThe DANES network actively creates spaces for the above goals to become a reality. The efforts of the network, which is inclusive and welcoming to all interested scholars and students in relevant fields, are manifest through the following initiatives:ConferencesThe first annual DANES conference took place between 19th-21st of February 2023, organized by three institutions: the Digital Pasts Lab at Ariel University, the TAD AI and Data Science Center at Tel Aviv University, and the School of Computer Science and Engineering at the Hebrew University of Jerusalem. The conference covered many topics, including optical character recognition (OCR) for cuneiform documents, data visualization methods, critical discussion of data analysis and machine learning, computational stylistics, natural language processing models, linguistic annotation, networks, and new online tools and environments and their sustainability. One can view the talks of the first day on YouTube, read the abstracts on Zenodo, and view the conference posters on OpenDANES. The proceedings of the conference are going to be published as a special issue, iDANES, in the it - Information Technology journal.Just as important as the talks themselves, were the discussions in-between sessions, the discovery of shared problems, and arriving at joint solutions. These conversations led to more avenues of further development and collaboration. Such was the main purpose of the conference: not only giving a venue to present interdisciplinary research, but also a space for scholars that usually have few people around them that understand every aspect of their work.The conference included a Q&amp;amp;A session with some of the experts in the field, Eliese-Sophia Lincke, Niek Veldhuis, Hendrik Hameeuw, and Hubert Mara, who received questions from the audience and online participants on some of the current burning questions. Especially important were the round-table discussions, whose purpose was to establish the goals of the new DANES network, common points of contention, future areas of research, and what is needed for the network and this field to thrive in the next decade.DANES conferences are going to regularly occur every year. The exact date of the next conference is still in the works. The following conferences will continue to have a combination of lectures with active participation: hands-on workshops, round-tables, and Q&amp;amp;A sessions.Working groupsThe DANES working groups are usually once-per-month online meetings to discuss or implement a specific computational methodology. In between meetings, the DANES discord server has sub-channels for each group for further discussions. They are meant as an entry point for those who want to learn about computational methodologies and do not know where to start or whom to ask. They are usually led by at least two people from the network.In 2023, the OCR group, led by Hendrik Hameeuw and Eliese-Sophia Lincke, met to discuss current challenges. The ancient language processing (ALP) group, led by Katrien De Graef and Shai Gordin, had guest lectures of various experts in the field to discuss their ongoing projects; the lectures and the recordings are available. The Interoperability and annotation group, led by Adam Anderson and Timo Homburg, discussed the importance of linked open data and how to apply it.Specific topics for groups depend on the initiators. Anyone is welcome to lead a group! Current active groups include a continuation of the ALP group, this year focusing on how to create a framework of under-resourced ANE languages, with a case-study on Elamite. Further activities of the Pedagogy and Gamification group and OCR groups are planned for 2024—sign up for the mailing list or join the Discord for updates!Discord and mailing listThe channels of communication for members of the DANES network is through Discord and a mailing list. To join the mailing list or Discord, send an email to danes@listserv.dfn.de or digpasts@gmail.com. Discord is a communication platform that allows users to create and join communities, known as servers, where they can chat with text, voice, and video. It was originally designed for gamers to communicate while playing online games, but it has since expanded to cater to various interests and communities. The DANES discord server includes various channels for communication of the DANES working groups and general discussions. It is meant as the primary space for anyone in the community to ask questions, give updates, and generally have conversations on DANES and related fields and methodologies.The mailing list is the more formal communication channel of the community. It is used to send out the monthly newsletter and give updates on meetings of the DANES working groups.NewsletterThe mailing list sends out a monthly newsletter, at the beginning of each month, which includes updates on the world of DANES. The newsletter summarizes, while explicating jargon, important articles that were published in the past month, and highlights important previous publications related to DANES. It informs the DANES community of relevant conferences, both in the ANE field and the digital humanities and computer science fields. The purpose of the newsletter is to establish DANES in both the field of computer science and ANE studies, by developing joint language and terminology around important publications, and encourage network members to participate in conferences relating to both worlds.Furthermore, after the publication of the newsletter there is a regular happy-hour meeting on the DANES discord channel for members to discuss and chat about what’s new in the community.OpenDANES platformThe OpenDANES platform is an open access publication platform for pedagogical materials. It aims to create content that will be useful and informative to any who want to learn how to combine computational methods with ancient artifacts, regardless of their previous knowledge. It publishes tutorials, which provide step-by-step instructions on how to apply computational methodologies for beginner, intermediate, and advanced levels. White papers, such as this one, are published on the platform as well. Those can be anything from opinion pieces, updating the community on projects, introducing the community to general initiatives, or efforts of groups and individuals working on digital and computational studies of the ancient world. All contributions go through a peer-review process to ensure high-quality and usefulness to the community, as well as to acknowledge contributors’ academic work. Upon final publication, all contributions receive a DOI.Additionally, OpenDANES includes the DANES resources, a dataset which collects free, online resources which can aid the DANES community members. It is a constantly growing dataset, and anyone is invited to contribute resources! See the instructions on the DANES Resources page.Final NotesTime will tell how the efforts of the DANES network will fare. The goals and activities will likely change and adapt as times goes by, and as technology changes and develops. The one thing that will not change is our overarching goal: being a resource and a hub for the community of those who study or want to study the ancient world computationally. Our focus will remain on sharing knowledge and expertise with the community, spreading the crafts and training new generations of artisans.For that reason, we call out to all who want to take part in the community, whether as passive observers or active participants. Considering our past, present, and future, it is hard to imagine the field not changing drastically in the next decade. We want the ANE community to thrive in new research possibilities, without losing academic rigor to enigmatic methods and trendy AI jargon. In order to build a shared language and decide on our field’s future, we want as many as possible to join us: from the masters of the craft, to those on the fence on whether to become apprentices or not, to those who never thought they could have anything to do with this art. There is a place for everyone within the DANES community.ReferencesAnderson, Adam, Shai Gordin, Stav Klein, Bin Li, Yudong Liu &amp;amp; Marco C. Passarotti (eds.). 2023. Proceedings of the Ancient Language Processing Workshop (RANLP-ALP 2023). Shoumen, Bulgaria: INCOMA Ltd. https://aclanthology.org/volumes/2023.alp-1Clark, Shmuel &amp;amp; Shai Gordin. 2023. ‘The Mesopotamian Ancient Place-Names Almanac (MAPA): A Gazetteer of the Uruk Urbanscape in the Age of Empires.’ Journal of Open Humanities Data 9(1), p. 20. https://doi.org/10.5334/johd.146Gardin, Jean-Claude &amp;amp; Paul Garelli. 1961. ‘Étude des établissements assyriens en Cappadoce par ordinateur’. Annales 16 (5): 837–76.Gordin, Shai, Shmuel Clark &amp;amp; Avital Romach. 2022. ‘MAPA: A Linked Open Data Gazetteer of the Southern Babylonian Landscape’. Interdisciplinary Digital Engagement in Arts &amp;amp; Humanities 3(2). https://doi.org/10.21428/f1f23564.8d442eeaHomburg, Timo, Tim Brandes, Eva-Maria Huber &amp;amp; Michael A. Hedderich. 2023. ‘From an Analog to a Digital Workflow: An Introductory Approach to Digital Editions in Assyriology’. Cuneiform Digital Library Bulletin 2023 (4). https://cdli.mpiwg-berlin.mpg.de/articles/cdlb/2023-4Li, Bin &amp;amp; Shai Gordin (eds.). 2023. Proceedings of ALT2023: First Workshop on Ancient Language Translation (ALT). Macau, China: Asia-Pacific Association for Machine Translation. https://aclanthology.org/2023.alt-1.pdfPlutniak, Sébastien. 2018. ‘Aux prémices des humanités numériques ? La première analyse automatisée d’un réseau économique ancien (Gardin &amp;amp; Garelli, 1961). Réalisation, conceptualisation, réception’. ARCS - Analyse de réseaux pour les sciences sociales / Network Analysis for Social Sciences Volume 2 (September). https://doi.org/10.46298/arcs.9236.Sahala, Aleksi. 2021. ‘Contributions to Computational Assyriology’. Helsinki: University of Helsinki. http://hdl.handle.net/10138/332924.Sahle, Patrick. 2016. ‘What Is a Scholarly Digital Edition?’ In Digital Scholarly Editing: Theories and Practices, edited by Matthew James Driscoll &amp;amp; Elena Pierazzo, 19–39. Digital Humanities Series. Cambridge: Open Book Publishers. https://books.openedition.org/obp/3397Stötzner, Ernst, Timo Homburg &amp;amp; Hubert Mara. 2023. CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation.  IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). https://doi.org/10.1109/ICCVW60793.2023.00183"
    } ,

  

  

  

    
    

    {
      "title"    : "How to Annotate Cuneiform Texts",
      "authors" : "Matthew Ong",
      "tags"     : "cuneiform, annotation, linguistics",
      "doi"       : "10.5281/zenodo.10825347",
      "submit-date" : "2023-10-02",
      "publish-date" : "2023-10-06",
      "peer-review-date" : "2024-03-16",
      "update-date" : "",
      "url"      : "/pages/How%20to%20Annotate%20Cuneiform%20Texts.html",
      "type"     : "tutorial",
      "summary"     : "A high level overview of how to annotate a cuneiform text for linguistic content.",
      "content"     : "Matthew Ong                                                                                                                      1                                                                                                                                                                                                                                                                       1 UC Berkeley                                                      Submitted on: October 02, 2023          Published on: October 06, 2023                      Peer reviewed: March 16, 2024                                                                        Reviewed by:         Heather D. Baker                                                                          ; Chiara Palladino                                                                                                                                                doi                                                    Summary: A high level overview of how to annotate a cuneiform text for linguistic content.                                      #cuneiform                  #annotation                  #linguistics                                              Difficulty level: beginner                    Lesson OverviewThis tutorial presents a high level overview of how to annotate acuneiform text for linguistic content. In general, linguistic annotation is an important first step if one wants to make a written language corpus amenable to more sophisticated linguistic analysis than just key word searches. It is also usually necessary to annotate the corpus if one wants to train a language model on it which can parse new sentences according to the language’s grammar. Exactly what we mean by ‘annotation’, and other reasons why one might want to make annotations of a corpus, will be discussed the Purpose of Annotating. The tutorial will then go through an illustrative example to show the important aspects of annotation.This tutorial uses examples from Akkadian, but the general techniques can adapted to other ancient language corpora.Basic facts about cuneiformCuneiform is a writing system developed in southernMesopotamia towards the endof the fourth millennium BCE. It uses logograms, syllabograms and determinatives. Cuneiform was used to express several languages of the ancient Middle East, includingSumerian,Akkadian,Hittite,Elamite, andHurrian. The most common way to represent cuneiform signsin modern publications is via transliteration. When a signis transcribed using capital letters, it indicates a logogram. When lower case letters are used, it indicateshow the sign is to be pronounced in a syllabic reading. If we write a sign within half-square brackets (‘⸢KUR⸣’), it is only partially visible on the tablet, while if it is written within square brackets (‘[KUR]’) it means the sign is no longer visible at all, and is reconstructed by assyriologists. When we write a sign as a superscript (for print editions) or within curlybrackets (for online editions), it means the sign serves as a determinative.For example, the sign transcribed as KUR looks like three trianglewedges arranged in the shape of a mountain. When written logographically, it usually stands as the word for ‘mountain’ or ‘land’. But this sign can bepronounced in different ways depending on context. It can be read ‘mat’,‘kur’, ‘nat’, and ‘šad’, among other things. When we write {KUR} (in digital editions) or KUR (in print editions), itmeans that the KUR sign functions as a determinative signaling that theword following it is a type of land or mountainous area.Drawing of the KUR sign.We usually use dashes between transliterated signs to indicate theybelong to the same word. Thus the sequence a-me-el šar-ri representstwo words, and in Akkadian it means ‘man of the king’.Normalization takes transliteration and expresses the actual phonological forms behind it. It dispenses with dashes and uses macrons and circumflexes to indicate vowel length. Using the aboveexample, the normalization of a-me-el šar-ri would be amēl šarri. The macron sign above the vowel ‘e’ indicates this vowel is long.The Purpose of AnnotatingLinguistic annotation allows a human editor to add information to a text relating to its linguistic structure, whether thatdeals with semantics, morphology, syntax, or phonology. Providing such linguistic information about a text via annotation has a number of uses, suchas:      Providing linguistic data for machinelearning algorithmsseeking to model properties of the underlying language;        Illustrating for students of the language how to grammaticallyanalyze a text, or as a check against the students’ own analyses;        Providing empirical data to a researcher who wants to ask questionsinvolving the systematic recording of all the linguistic data in alarge number of texts, or questions involving linguistic patternsthat emerge only upon processing the data by a machine;        Giving the annotator themselves an opportunity to work through acorpus in detail, creating an index, much like an editor does when preparing a newprint edition of a text.  The Stages of AnnotatingWhen linguistically annotating a cuneiform text it is convenient to divide the work into a preprocessing stage and an actual annotation stage. We will discuss each of these stages in order.Preprocessing the Raw TextAnnotating a cuneiform text generally requires that it first beavailable in transliteration or transcription/normalization (depending on the specificpurposes of the annotator). If acuneiform text has been edited in a print journal or other scholarlypublication, it will be represented in transliteration. Increasingly,transliterations of cuneiform texts are also available online as part ofdigital databases. Sometimes the database simply displays the text toyou in your browser (i.e. as part of an HTML file) and you will have toscrape the transliteration from thesite using a variety of data processing tools. But often times thedatabase also allows you to download the transliteration in a text file.In fact, you should check if the database has their entire collection oftransliterations available for download (see OpenDANESresourcesfor downloadable datasets), as annotation is usually a process appliedto an entire group of texts rather than just one. One repository that does this is the CDLI.Some online cuneiform corpora, such as those found in Oracc, also include normalized versions of cuneiform texts which can be obtained through suitable preprocessing. Readers interested in learning how to download large sets of normalized texts from Oracc can use the Jupyter notebooks of Niek Veldhuis, which come with substantial documentation. This notebook in particular will allow you to obtain normalized versions of texts from one of the State Archives of Assyria volumes online.In what follows we will use portions of a normalized text obtained from Oracc via the above notebooks for illustrating preprocessing and annotation. This text is SAA 5, 114, an Akkadian letterfrom the royal archives of Sargon II (r. 721-705 BCE).The basic file format of the normalized text you want to ultimately annotate should be a plain text file, with no HTML markup or other metadata. In particular, there should be no punctuation markers like periods or quotation marks. You need to make sure that each word in the text is separated by spaces (ideally one) from the preceding and followingwords, which allows the annotation program to split up the text file intoword-size units. This important preliminary step is calledtokenization, and the broken-up items in the text are called tokens.Note that depending on your purposes, you may choose to reduce oreliminate symbols in the text denoting unreadable signs (e.g. the symbol‘x’) or other comments indicating layout of the text and tablet (such as‘rest of tablet broken’). Generally, removing broken or uninterpretable symbols makes little difference for training morphosyntactic language models. But if you wish to have your annotations retain more fidelity to the original text (e.g. by indicating how far apart fragments of linguistically parsable material are from each other), it is better to retain these tokens.In our case (SAA 5, 114), our desired plain text file looks like  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazanauptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša{KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupušarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lātašpuraNote that in contrast to the online edition of this text, there are no special marks and commentsdescribing line numbers, breakage in the tablet, obverse and reverse, or reconstructed parts of the text. This is because our interest in the text is linguistic, without regard to where the words appear on the tablet. Moreover, such clean up makes the resulting annotation more useful for machine learning purposes. However, depending on the annotation program you use it is often possible to include special comment lines at the top of the file preceded by a special symbol, such as ‘#’. Such lines can be useful in allowing you or a processing script to identify certain features of the text (such as publication information).In general, we should understand that what to remove from an online edition of the text is a choice we need toexplain when presenting the results of our annotations. If your projectneeds to treat these features of online editions differently, you may haveto adjust later stages of your workflow accordingly. Note in general that it might be harder for you to read and understand the text if you remove markers of the physical aspects of the tablet (e.g. where the obverse/reverse begins, rulings, line breaks). Thus it is probably a good idea to have an edition of the text handy for you to consult while parsing the text.Note also that unclear signs or partial words like {KUR}x and x+x-pa are still left in transliterated form with dashes and brackets to indicate which signs function as determinatives or go together to form a word. According to Assyriological convention, a single ‘x’ sign generally refers to one illegible sign on the tablet. You may wish to remove these tokens according to your needs. Sometimes the fragmentary tokens are still useful for linguistic annotation, particularly if they come with a determinatives. In that case one can often deduce the part of speech or semantic class of the original token, a useful piece of information for annotation. In our example, the fragmentary token {KUR}x has the determiner for a land or country. Thus we know to mark it as a (proper) noun during the annotation phase.We need to make two more comments about the preprocessing stage. First, the above text file for SAA 5, 114 consists of one line, even though the text itself consists of multiple sentences. Keeping the entire text as a single line somewhat simplifies the annotation stage. But depending on the length of the text you are annotating or its particular layout, you may find it convenient to break up the text into several lines based on sentence breaks. This can make the annotation phase somewhat simpler since the annotation program we recommend below will regard each line as its own unit, allowing you to analyse the entire text piece by piece. An example of how this could be done is given below.  ana šarri bēlīya urdaka Gabbu-ana-Aššur Urarṭaya emūqēšu ina Wazana uptahhir o bēt pānīšūni lā ašme Melarṭua māršu Abaliuqunu pāhutu ša {KUR}x x+x-pa adi emūqēšunu x x+x x x x x x x x x mātāti izaqqupu šarru bēlī lū ūda šarru bēlī lū lā iqabbi mā kî tašmûni mā atâ lā tašpuraIn particular, if the text you are annotating is in verse form, where each line represents its own sentence, introducing such line divisions into the text file may be a judicious choice. Alternatively, introducing line breaks where the cuneiform tablet has a line ruling (indicating a strong section break) or where there is a clear shift in topics within the text (which tends to be represented in English translation by a paragraph indentation) can also be judicious. In general, however, the choice of where to break up a prose cuneiform text into sentences can be somewhat subjective. Furthermore, once you start annotating separate lines of a text you cannot easily ‘merge’ the lines back together (say, if you made a mistake in where to divide the text) without having to start over from scratch again. For this reason, we recommend being conservative in introducing line breaks in the text file.If you are annotating multiple cuneiform texts, a good practice is to haveeach text in its own file, where the file is titled in a way that you(or the computer, if using a processing script) can easily identify whatthe text is. This may mean using numerical indices in the file nameswhose interpretation you record in a separate list. The image below illustrates this schema. It is a directory listing of many files, each of which containes a single cuneiform text in transcription. Each file is labelled with the P-number that uniquely identifies the text it contains within the CDLI catalogue.Example of files names.Alternatively, you can often put all of the texts in a single file, separated byempty lines and special comment lines identifying the text.Each approach has its advantages depending on your purpose for annotating and the toolsyou use. Generally if you are scraping your texts from an online repository it is easier to store them in separate files.Making the AnnotationsTo create the annotation metadata for a text you need aprogram that will allow you to view the text and add special symbols andnotation to it. By now there are a number of free programs aimed athumanists seeking to add all sorts of metadata to digitized texts,freely available for download or use online as a web application. Someof these programs are mainly used for highlighting thematic relationsbetween passages or phrases in a text or for connecting entitiesmentioned in the text to an external specialized vocabulary such as adatabase of maps or biographies. In our case (doing linguisticannotations), you want to make sure your tool can annotate lemmas,syntactic dependencies, and morphological features at the minimum.One very reasonable annotator program we recommend for Akkadian is Inception. It is freely downloadable as a Javaapplet that works directly through your internet browser. It is capable of handling lemmas, syntactic dependencies, and morphological feature specification in a fairly intuitive manner. The following screen shot shows what it looks like when annotating the normalized version of SAA 5, 114 introduced above.Annotating SAA 5, 114 in Inception.The rest of this tutorial will discuss annotations done in Inception.Generally, if you are doing your own morpho-syntactic annotations from the ground up, by hand, you start by importing the raw text file of the cuneiform text in transcription into Inception. This looks something like this:A raw transcription file imported into Inception.Annotation layersOne should think of annotating a text as a collection of more or less disjoint labeling tasks. Each task can be thought of as applying a ‘layer’ of labels to all of the tokens of the text, and indeed, Inception uses the term ‘layer’ to describe the various annotation tasks you can use the program for (including custom labelling tasks). For the sake of efficiency, you want to tackle each specific labeling task in turn rather than trying to do them all at the same time. For example, you first go through and assign the part of speech tags to each token in the text. Then you go back and assign lemmas to all the tokens. Then you provide morphological parses for all the tokens, etc. This ‘serial-wise’ way of doing things in Inception is easier because you must switch layers in the graphical interface whenever you wish to switch from one labeling task to another.Visually, Inception displays the labels you apply as colored layers stacked above each token. In the SAA 5, 114 example above, the lemma for a given token is given in the orange box right above the token. For instance, the lemma for the form uptahhir is pahāru. Above that are the morphological parses, and then the part of speech tags. Syntactic dependencies are indicated by directed arrows among the tokens.Some of the default layers used in Inception will now be discussed in turn.Part of speech layerInception provides two fields for part of speech tags. The first is the universal part of speech tag (UPOS). It is based on the UniversalDependencies (UD) framework and is meant to apply for all languages. It appears on the left-hand cell of the part of speech tag above a token. The second kind of tag is provided for language-specific or more customized part of speech tags and is abbreviated XPOS. It appears on the right-hand side of the part of speech tag. In the case of Akkadian and Sumerian, a convenient set of XPOS tags to use is provided by Oracc and is listed here (along with corresponding UPOS tags).Syntactic layerIn terms of syntax, Inception uses the UD framework because it is well-suited for working with many different languages. Unlike other grammatical formalisms, UD is based on the idea of syntactic dependency.Syntactic dependencies indicate grammatical dependencies among words andphrases such as subject and object of a verb, an adjective modifying anoun, or a conjunction connecting two full sentences. Overall, one makes semantically ‘light’ terms like prepositions and particles dependent on semantically more important terms like nouns and verbs. Syntactic dependencies are asymmetric relations betweentwo words or phrases, one of them being the head and the other thedependent. One may think of annotating the syntactic dependencies of a sentence asessentially constructing a directed graph or tree, where the dependencies between tokens in a text are expressed by directed arrows. Inception reflects this fact visually in an intuitive manner.In the second sentence within the SAA 5, 144 example figure, there is an arrow going from the verb uptahhir to the noun Urarṭaya ‘the Urartian’. The arrow’s direction indicates that the noun is dependent on the verb, and the label ‘nsubj’ indicates the noun is the nominal subject of the verb. Creating such a dependency is a simple manner of selecting the Syntax layer and drawing an arrow from the head token to the dependency token. You select the label for the dependency in the lower right.Although the program will not prevent you from doing so, it is considered semantically anomalous in the basic UD framework to have a form syntactically dependent on more than one head form. This is because of what syntactic dependence means in UD (putting aside the so-called enhanced dependencies system). In Inception, this anomaly would be represented by having arrows from two different forms leading into a third. Although the program will not prevent you from doing this, such ‘improper’ graph structures will not only be unclear to human readers, it will also create problems for machine learning algorithms using your annotations for training data. You should thus strive to avoid making such mistakes in your annotation (remembering that it is perfectly normal for a single form to have multiple forms dependent on it, i.e. to have multiple arrows leading out of it). A more subtle problem arises when you have a cycle in your annotations, i.e. where there is a chain of arrows starting from one form and ultimately leading back to the same form. This type of error can arise when you are annotating a very complex or long sentence. It, too, creates problems for machine learning algorithms but can be hard to discover until you gain experience in ‘reading’ Inception dependency graphs.Note also that because most cuneiform texts do not mark sentence divisions, it is up to the annotator to decide whether they want to indicate sentence divisions at the layer of the base text file (by separating the text into separate lines) before annotating. If you wish to annotate the whole text in Inception without first making line breaks, the only clear marker of sentences or clauses will be where one annotation graph stops and the other begins. In other words, sentences are defined graph-theoretically by connected components. What you should remember is if you have broken your base text into separate lines (where each line represents its own sentence), you cannot draw arrows across sentence boundaries in Inception. If you initially made a line break in the text thinking it was a sentence boundary but while annotating realize you made a mistake, you cannot easily undo your mistake. You must either go back and fix the base text and start annotating from scratch again, or tolerate the error in Inception and continue. For this reason, it is sometimes better to be conservative and not make breaks in the underlying text unless you are sure there is a sentence break.Morphological layerWhen doing annotations for morphological forms in Inception, you should think in terms offeature-value pairs. A feature is a certain grammatical or semantic category such asgrammatical number or gender, verb tense, or definiteness, and a valueis what variant of the category the word expresses via a particularmorpheme. Thus in the English word cats, the plural suffix -s signalsthe value ‘plural’ within the category of grammatical number. Inliked, the -d suffix indicates the value ‘past’ in the category oftense. In Inception, a morphological parse for a token consists of a text string made up of feature-value pairs, where a given pair consists of the feature label on the left, the value label on the right, and an equal sign joining the two sides together. Examples include‘Gender=Masculine’ or ‘Tense=Past’ or ‘Subjunctive=Yes’. The feature-value pairs are concatenated together by the bar sign ‘|’. For example, a noun in Akkadian might have feature-value string ‘Gender=Masculine|Number=Singular|Case=Nominative’. For cuneiform there are currently no strict conventions for what label sets to use for morphological annotation. However, if you wish your annotation scheme to be compatible with other annotation projects, you should check to see what they use and match their schema (or establish some one to one correspondence in labels) before beginning annotation.In Inception, feature-value strings are input on the right side of the screen once you’ve designated a token for morphological annotation. In the second sentence of the SAA 5, 114 figure, you can see the Features window showing the morphological feature specification of the form uptahhir. It begins ‘Gender=Masc|Mood=Ind|…’, which means the form is masculine and in the indicative mood. Inception conveniently remembers the most frequent strings you input in the morphological feature specification window, and will show them to you via a drop down menu as you type. Another way to save time and labor when inputting feature-value strings is to maintain a simple text file listing the most common strings you use during annotation. You can then copy and paste these strings into the window as needed.The Output FileAfter you have finished annotating a text in Inception you need to output the results to a file. Inception allows for a number of output formats, one of them is CONLLU-format. This format conveniently represents the lemmas, syntactic dependencies, and morphological features. A CONLLU-file is a text file where each row represents a single token, and with ten tab-separated columns, each of which specifies linguistic information about that token. The details of the encoding are found at the Universal Dependencies website.The CONLLU file for the annotation of SAA 5, 114 looks as follows:CONLLU file for SAA 5, 114.Note that the CONLLU format encodes token position within a sentence in the first column. Dependency relations are given in the seventh column, and dependency type in the eighth. Thus for the second sentence, the fourth row represents the token Wazana, while the fifth row represents uptahhir. The seventh column of the fourth row has value 5 and the eight column is ‘obl’. That means the token Wazana is syntactically dependent on token uptahhir and the dependency relation is oblique.  You must take care when reading a CONLLU file that you remember the seventh column represents the syntactic head, not the dependent. These are easy to confuse!If your initial text file was broken up into several lines, the CONLLU file output by Inception will consist of multiple ‘blocks’ of indices each starting with 1. Header information and comments are indicated by lines starting with ‘#’. This is illustrated below.CONLLU file representing multiple sentences.Your CONLLU file can itself be imported into Inception for further annotation, become part of a growing treebank project for your language, or serve as training data for a suitable language model.Issues to Consider When AnnotatingLet us step back a minute from Inception and Akkadian, and speak about annotating in cuneiform more generally. If you have not previously annotated a cuneiform text in the specificlanguage at hand it would be good to check how others have annotated inthat language, both in the hopes of making your work compatible withtheirs, as well as becoming familiar with the difficulties they faced inapplying their annotation scheme. Unless you are highly experienced withthe language using modern linguistic categories, you may find that yourinitial label set for morphological features turns out to beinsufficient, or your understanding of a certain syntactic structureturns out to be wrong (or at least problematic). Looking at what othershave done may save you time and effort in the long run. At the sametime, no annotation scheme currently used for a cuneiform language fits the grammatical particularities of that language perfectly, whetherat the level of theoretical description or practical implementation.Depending on the type of text you are working with, you may find yourselfdeviating from the conventions of other annotators or even needing toinvent conventions yourself. What is most important is that you havereasons you can cite for what you do and that you explain those reasonssomewhere in the documentation that accompanies your work. Such documentation often consists of a text file or Markdown file that is meant to accompany the annotation files and processing scripts you bundle together in an online repository such as GitHub (see under Choosing How to Store Results). Being explicit in this file about the corpus you are annotating, the morphosyntactic label sets you use, and use of your various scripts will improve the utility of your work. Not only will it be easier for others to understand and replicate what you did, it will alsohelp you to remember your own policies at a later date and to be consistent in your work.Here are some examples of projects that use linguistic annotations in different languages written in cuneiform: on Anatolian languages like Luwian and Hittite see the eDiAna project and Hittite Festival Rituals, respectively; for Akkadian, Sumerian, Persian, and Urartian see the ORACC lemmatization guidelines, and for Sumerian specifically see the ETCSL project conventions. The article of Luukko, Sahala, Hardiwck, and Linden 2020 outlines a morpho-syntactic annotation scheme for some Neo-Assyrian royal inscriptions, while that of Ong and Gordin 2024 addresses annotation of Neo-Assyrian letters using a spaCy language model. Consulting the last two examples in particular can give you an idea of what the entire annotation workflow for Akkadian consists of.A second issue is whether to approach annotation sub-task by sub-task ordocument by document. If you intend to annotate many documents forseveral features (e.g. syntactic dependencies and morphologicaldependencies and lemmatization), you can either annotate all of thedocuments for a single feature first, and then go back and go over thedocuments for the second feature, etc., or you can do all of thefeatures for a single document while it is still in front of you beforegoing on to the second document, etc. Depending on the tools andknowledge you bring to your work, one method may be more efficient thanthe other. If the syntactic dependencies of a corpus are particularlydifficult to do, you may opt to first go through and do the morphologybefore tackling the syntax alone. If you are working with someone whodoes not know the grammar of the language well but can find the lemmasfor all the tokens in the text on their own, you could delegate the taskof lemmatization to them while you do the dependencies and morphology.While these kinds of considerations are relevant to annotating manysorts of languages beyond cuneiform ones, in the case of cuneiformlanguages there is one thing to keep in mind. Unless the type of textyou are annotating is particularly simple morphosyntactically, it willlikely take you some effort to understand what the text is saying evenin transliteration (or normalization). Thus, going over the text multipletimes for different annotation tasks may be less efficient that doingeverything at once while the meaning of the text is still fresh in yourmind.Finally, be aware of accompanying datasets or machine learning toolsthat can accelerate your work. If your transliterations come from anonline database such as Oracc, they may also come with lemmatization data as part of aJSON file or separate glossary. If you already had to extract the rawtext from such a JSON file, it is only a little more work to grab thelemmas that go with the tokens as well. Similarly, if you are makingannotations of a corpus to train a natural language processing model on it (say a syntactic parser or morphologizer), you can actually start training your model on the portion of the corpus you have annotated early on and thenapply its predictions to the rest of the corpus. Going through andcorrecting the model’s predictions is often faster than going throughthe whole raw corpus unaided. This technique, known as bootstrapping,is particularly effective when repeated multiple times, early on, for alarge corpus.Choosing How to Store ResultsUnless you are annotating cuneiform texts for private purposes and donot want to share your data, you should consider how to make your workavailable to others on the internet. GitHub is an online platformmainly designed for people writing code they wish to share with othersin a controlled, version-specific way. As an annotator, you can use itas a convenient place to store your data and any associated processingscripts, keeping your data project private, open to all, or only toinvited users. The system is designed to synchronize with particularfolders on your local computer, so that the process of backing up dataor uploading newer versions is easy and allows you to compare currentand older versions of files. GitHub is a good place to provide your dataif you envision yourself working on multiple projects in the future, orif you have scripts or other associated code that you need to presentalongside the annotations themselves.If you have annotated your texts in the Universal Dependencies format,you can also make your data available on the UD website (which actually stores its data onGitHub) alongside annotated corpora from dozens of other languages. Youdo need to make sure your annotations conform to their formatspecifications, which are more strict than what has been discussed here. This website already features corpora of Akkadian,Biblical Hebrew, and Hittite, and its general purpose is to facilitate multi-lingual corpus research.A third alternative is Zenodo, a site for publishing academic datasets. The datasets you publish there are linked to your personal account or those of your work group.Final ObservationsWhile annotating itself has some immediate uses, it is often done aspart of a larger language processing task, research program, orpedagogical project. We mentioned some of these uses earlier on, amongwhich were applications to machine learning and language modeldevelopment. The challengesof applying machine learning and developing language models forcuneiform languages are somewhat different from someone working onEnglish. There are fewer pre-existing annotated corpora that one can useto jump-start one’s own annotations. Many natural language processingpackages assume that you are interested in working with a popular modernlanguage, and often come with large datasets from those languagesbuilt-in, without clearly explaining how to apply their code to alow-resource language starting from the ground up. You will likely find it very helpful to haveguidance from someone more experienced in natural language processing ordata science, or to look at some of the online proceedings from NLPworkshops aimed at humanists and specialists in less common languages.One recent workshop at Princetonillustrates how to develop an NLP project for new languages usingspacy. On a broader level, one may also considerthe Digital Humanities Summer Institute with itsvarious course offerings. What may be the most helpful in the beginning,however, is finding someone else who has begun an annotation project inyour language by searching GitHub and getting guidance from them."
    } ,

  

  

  

    
    

    {
      "title"    : "Interview with Nepos Games about their game Nebuchadnezzar",
      "authors" : "Gustav Ryberg Smidt, Michael Wamposzyc",
      "tags"     : "pedagogy, gamification, city-builder",
      "doi"       : "",
      "submit-date" : "2024-02-06",
      "publish-date" : "2024-03-10",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Interview%20with%20Nepos%20Games%20about%20their%20game%20Nebuchadnezzar.html",
      "type"     : "white-paper",
      "summary"     : "Not many games are focused on the ancient Near East, but we've gotten the chance to talk with the programmer of one of them. Nebuchadnezzar is a city-builder game and it's fun. This interview is meant to peak an interest in it and showcase the benefits gamification can have for how we see our research field.",
      "content"     : "Gustav Ryberg Smidt                                                                                                                      1                                                                                                                                                                                                                                                                                                           ;               Michael Wamposzyc                                                                                                                                                                                                                      2                                                                                                                                                                             1 Ghent University              ;                 2 Edinburgh Napier University (UK)                                                      Submitted on: February 06, 2024          Published on: March 10, 2024                                                                          Under peer review                                          Summary: Not many games are focused on the ancient Near East, but we&#39;ve gotten the chance to talk with the programmer of one of them. Nebuchadnezzar is a city-builder game and it&#39;s fun. This interview is meant to peak an interest in it and showcase the benefits gamification can have for how we see our research field.                                      #pedagogy                  #gamification                  #city-builder                        IntroductionWhen we started looking into gamification of the ancient Near East, we wanted to talk to so-called stakeholders and Nepos Games, the creators of Nebuchadnezzar, were first on that list. Since there are only two people behind the game, both of them are well versed with its development. This gives us the unique opportunity to understand all the steps that go into researching and building such a game.Gustav was lucky enough to sit down with Joseph, one of the programmers of the Nebuchadnezzar game. Together with his colleague in Nepos Games, they developed a city builder game based on ancient Mesopotamia. The interview will give you the chance to look behind the curtain through small snippets of videos with commentary and gameplay, to help you better visualize what is going on.The game was published in February 2021 on Steam and currently has one DLC (downloadable content) called the “The Adventures of Sargon.” Our two game-makers met at a company working on Truck simulators, which is exactly what it sounds like: a simulator of driving a long-haul truck. Nebuchadnezzar is different; it is a city builder game where the player must construct a functional city and complete a number of missions tailored to the period of Mesopotamian history they represent.The GameYour main goal is to create a thriving city and economy. There are three basic resources: workforce, goods and money. As they are dependent on each other, the gameplay revolves around a feedback loop between the three. When you build a house, you can accommodate a workforce that creates goods. The goods you accumulate can be used to sustain a bigger population and for the money you earn on trading and taxes you can buy more houses. In conjunction, you need to link the resources with a well-planned infrastructure. This is a city planner game in its simplicity, but it includes features that make it distinctively a city builder game of an ancient Near Eastern city.  You play as the omnipotent ruler, seeing everyone running around doing your bidding! Maybe a slight exaggeration, but you assign the workers and basically define every part of their life. Yet you still have to adhere to an underlying hierarchy imposed by the social structure. There are the low-level workers that can create food and make pottery. They are your worker ants that provide the bare necessities, which is enough for your city to grow in number, not in complexity. For that you need specialized workers. People who can write tablets, be priests or jewelers. It’s a good facsimile of the co-dependency there must have existed between social layers in the ancient Near East.Once you have a few workers to begin with, you better start feeding them, cloth them, and sweeten their life in general. That might be baking bread or milking cattle, but of course this also includes providing the mandatory staple of beer! And you need a diversified economy–bread and butter might be enough for some, but you cannot maintain a thriving community of scribes without clay, or entertain priests without idols of the gods.Trading with other cities was essential for Mesopotamia to flourish, since the area was rather poor in a number of natural resources. In the game you have to source such things like copper from abroad and then you can make that into jewelry that can be traded away. Interestingly, money in itself is not enough to open up new trading routes. You have to practically gain the respect of prospect partners, like we see in the Amarna letters where international power players discuss with each other who is worthy of a certain status.  We really like the effort they put into working on the pantheons in the game. You have a city god per mission who is determined by historical evidence, and then a group of other deities from the same pantheon that you can choose from. Once you open up trading routes to a city from a different pantheon, that city god also becomes available to you. It’s truly a polytheistic society. You can worship your deities by throwing more or less lavish festivals or ultimately by building a temple in their name. The more you honour them, the better the in-game bonuses they give you.  For many people, simply hearing the word infrastructure makes them slightly nod off. But don’t! Stay with us, please. It was as essential back then as it is today. Even though they are misconceptions, the idea that the Romans invented roads and aqueducts is something most men think about on a daily basis apparently. In the game we have to draw water from the rivers via irrigation systems and construct roads that makes the re-distribution of wealth possible. Funnily, you also have to create fire stations.  The new DLC introduces some interesting aspects into a city builder game–mainly warfare. It’s a heavy industry, as it needs materials, food and a workforce. And that’s of course not really different from anything else in the game. So, economically it requires the same considerations as the other parts of the game, but actually invading another city is a whole different ball game.  The InspirationWithout fail, every time an assyriologist meets a new human being, any human being, they fear to be greeted with: “Oh I think I’ve heard about what you study, it’s something to do with the pyramids, right?” And for that reason, it slightly hurt when Joseph told Gustav that a large inspiration for the game was “Pharaoh”, a city builder game set in Egypt. But he then followed that by explaining that the best inspiration for a civilisation based game is the world’s first. As he would say it himself: ‘we demolished the mainstream pyramids and built the hanging gardens’, so now all is good.  It was clear when talking with Joseph that he is fascinated by the ancient Near East and the more details we went into the more excited he seemed to get. His attention to the great structures that were first implemented in the fertile crescent showed, together with the digital art, that he and his co-developer didn’t just consider Mesopotamia through the glasses of exotic otherness. They were trying to showcase the monumentality hidden under the surface in Iraq.They also put a lot of effort into explanatory content. Before every mission, the player is met by an introduction text to the period it’s in. Not surprisingly, the sources for those explanations were Wikipedia and similar sites. Even though Joseph tried to contact a specialist, it was to no avail. This tells us how important it is to help keep those generally available resources up to date.  We really like the artwork as well, which they put some effort into. Instead of mainly taking visual inspiration from existing and oftentimes morally loaded ideas of how the ancient Near East looked like, they did their best to base it on their research and work.ConclusionNebuchadnezzar is not a game that is meant to teach you about the ancient Near East, but one really feels it is based on knowledge of the ancient Near East, and players might accidentally learn something new. What surprised us the most was the ability such a game can have on a more holistic understanding of our field. It helped Gustav to visualise larger Mesopotamian social structures when playing it. Even though it’s not a fully accurate picture of the social structures as he understands them, it can spur ideas of how to potentially model these structures, which elements are important in the social structures, and what happens when you change something specific. McCall in his book from 2023 Gaming the Past: Using Video Games to Teach Secondary History, uses this game as an example of how to critically discuss the agency of individuals in an ancient society (pp. 12–15). In the digital age, a game like Nebuchadnezzar can play a role in how we teach and comprehend the ancient Near East, so we definitely encourage the readers to try it out!"
    } ,

  

  

  

    
    

    {
      "title"    : "Musical Instruments in Ancient Mesopotamia (MIAM) A Semantic Media Wiki Database and Lexicon",
      "authors" : "Dahlia Shehata, Benedetta Bellucci, Tomash Shtohryn",
      "tags"     : "database, lexicon, music, open access, Semantic MediaWiki",
      "doi"       : "",
      "submit-date" : "2025-04-03",
      "publish-date" : "2025-04-08",
      "peer-review-date" : "",
      "update-date" : "",
      "url"      : "/pages/Musical%20Instruments%20in%20Ancient%20Mesopotamia%20(MIAM)%20A%20Semantic%20Media%20Wiki%20Database%20and%20Lexicon.html",
      "type"     : "white-paper",
      "summary"     : "A research project presentation with the scope of collecting, classifying and interpreting musical instruments, their terminology, iconography, and contexts from texts, images and original finds. Its outcome is presented in online database and lexicon based on Semantic Media Wiki-technology (SMW).",
      "content"     : "Dahlia Shehata                                                                                                                      1                                                                                                                                                                                                                                               ;               Benedetta Bellucci                                                                                                                      1                                                                                                                                                                                                                                               ;               Tomash Shtohryn                                                                                                                      1                                                                                                                                                                                                           2                                                                                                                                                                                                           1 University of Würzburg              ;                 2 Incorporated notes to this white paper                                                      Submitted on: April 03, 2025          Published on: April 08, 2025                                                                          Under peer review                                          Summary: A research project presentation with the scope of collecting, classifying and interpreting musical instruments, their terminology, iconography, and contexts from texts, images and original finds. Its outcome is presented in online database and lexicon based on Semantic Media Wiki-technology (SMW).                                      #database                  #lexicon                  #music                  #open access                  #Semantic MediaWiki                        IntroductionMusical instruments are cultural artefacts inextricably linked with the social, historical, religious, and political forces of their eras. They instantiate equal progress in technical and scientific knowledge and developments in trade, commerce, and society. Their production, propagation, and use in private and institutional structures testify to interconnectivity and exchange in a variety of cultural matters (Eichmann et al. 2019; Lawson 2020). All of this makes research into musical instruments a vital component for the study of the multicultural and multi-ethnic background of ancient Mesopotamia.The musical instruments of Mesopotamia have been addressed in numerous articles and monographs. Many of these have focused on individual questions, written corpora, artefacts, or epochs with a noticeable concentration on the 3rd and 2nd millennia BCE (e.g. Krispijn 1990; Schauensee 2002; Ziegler 2007; Dumbrill 2007; Mirelman 2014; Shehata 2014; Shehata 2021b; Cheng 2001; Gabbay 2010, 2014a/b). General overviews are provided in the works by Schmidt-Colinet (1981), Rashīd (1984), and Dumbrill (1998, 2005). In recent years, these have become outdated as a result of new finds, publications, and investigations, among these e.g. Ziegler (2007), Orlamünde (2011), Strommenger/Miglus (2010). Additionally, important aspects of Mesopotamia’s music cultures have so far been neglected; unsatisfactorily analyzed are, for example, smaller sound tools such as rattles and whistles, which remain unnoticed in important museum collections and are seldom studied (e.g. Pruß 1999). Entries on musical instruments in common dictionaries of Akkadian and Sumerian (name = musical instrument) still only contain broad and unspecific identifications (e.g. EPSD2 ŋešdim-nun “lyre?”), inconsistencies, or, given the lack of both philological and musicological investigations, no identification at all (e.g. CAD B 134b; CDA 40a baṣillatu (ḫabaṣillatu) “a musical instrument”).The need to create a reference work based on Assyriological and archaeological research standards, considering the variety of philological terms and the details of their probable interpretation, while being informed by differentiated modern musicological terminology, appears evident. It further allows to carry forward basic research themes:  The identification of musical instruments through their names and shapes in texts and images and from physical remains;  The evaluation of the contexts of musical instruments and the identification of related classifications from an emic perspective;  A comprehensive cultural-historical outline highlighting traditions and innovations in Mesopotamia’s sound and music cultures.An important basis for further investigations is the identification of musical instruments, which remains the primary goal of Mesopotamian music research. This provides necessary connecting points for interdisciplinary research questions. Facing this endeavor, a first evaluation of the different limitations in the available evidence strongly suggests that ancient Mesopotamian instrument designations as well as their classification are based less on organological features or playing techniques as specified by Hornbostel and Sachs (1914), but rather on other qualities such as function, status, sound, origin, or use (see e.g. Kartomi 1990; Koch/Kopal 2014; Franklin 2015). Moreover, dealing with challenges of polysemy and ambiguity necessitates systematic documentation and differentiation of contextual information, which needs to be supplemented by musically relevant data such as material and construction, music setting, sound description, ensemble type, function, symbolic value, and players. Finally, musical instruments, their shapes, names, and classifications, are, of course, subject to constant change. An evaluation of their interconnectivity within cultural traditions and changes diachronically throughout all of Mesopotamia’s history and synchronically in various regions thus constitutes the third desideratum identified above.MIAM: Team and SettingThe project titled Musical Instruments in Ancient Mesopotamia-—in short, MIAM—-started in April 2024 as a four-year project and is funded by the Deutsche Forschungsgesellschaft (DFG). The project is hosted at the University of Würzburg in close collaboration between the department of Ancient Near Eastern Studies and the Centre for Philology and Digitality “Kallimachos” (in short ZPD). As a central academic institution, bridging the gap between the Humanities and Computer Sciences, the ZPD was founded in 2023 with the aim to provide the best possible support for developing digital topics in humanities research.MIAM’s core team consists of an archaeologist (Benedetta Bellucci), a philologist (Dahlia Shehata), and a part-time IT-specialist (Tomash Shtohryn), as well as student helpers (Konstanze Hofbauer and Bilind Shushe). The project further counts on several associated researchers in Würzburg as well as international researchers; among them Christian Reul who supervises the digital background at the ZPD, the musicologists Oliver Wiener (Würzburg) and Salah ed-Din Maraqa (Freiburg), as well as representatives of the main discipline of Ancient Near Eastern Studies, Uri Gabbay (Jerusalem), Sam Mirelman (London), and Ricardo Eichman (Berlin). Equally included as research advisors due to their specialized practical knowledge are instruments builders and practical musicians, e.g. Ralf Gehler (Schwerin) at the Zentrum für Traditionelle Musik, specialized in recreating ancient musical instruments.Research ObjectivesAs given in its full title, Musical Instruments in Ancient Mesopotamian Records: Terminology, Iconography, and Context, the project’s main focus are musical instruments, their shapes, types, organology, names and terminology as well as their cultural and historical setting. Given the above sketched desiderata in the study of ancient Mesopotamia’s music cultures, MIAM’s main goal is to unlock previously unanswered questions based on the following investigative concerns:  How far are we able to correlate and identify musical instruments from their given names, their depiction, and original sound tools (i.e. musical instruments and other sound-producing artefacts)?  What kind of typologies and classification systems may be developed and identified for Mesopotamia’s musical instruments, both from emic and etic perspectives?  What are the geographical distribution and historical developments of individual as well as groups of musical instruments?In view of the generally limited evidence for musical instruments in Mesopotamia, the only possible basis for addressing the presented research questionnaire is the creation of a collection of primary sources that is as comprehensive as possible, and on which the contextual and historical evaluation can take place. The project’s objectives are three-stepped and take their start from compilation work:      Compilation: Creating a comprehensive collection of records on musical instruments in texts, images and sound tools.        Analysis and evaluation: Interpreting and correlating archaeological and philological data with regard to terminology, iconography, and context.        Presentation: Feeding the results from steps (1) and (2) into an online open-access digital database and lexicon.  The project’s primary objective is to provide a comprehensive collection of records with any sort of reference to musical instruments. Most importantly, there is no restriction to the record’s type: MIAM collects information on musical instruments detected in cuneiform manuscripts (texts), from iconography (images), and as original finds of sound tools.As to the texts and their content, references to musical instruments are detectable in all text genres. In contrast to Lexical Lists, which provide a differentiated terminology yet not fully deciphered and understood, Akkadian or Sumerian narratives may contain extended descriptions of music performances with information on the instruments’ handling, their players, or the sounds produced.Depending on the type of the visual medium, images can provide information about the players, the contexts, and, in the case of fine visual works, also about organological features of a musical instrument. Already at this early stage of the project, new as well as previously unknown or unnoticed representations of musical instruments have been discovered, which significantly revise our previous understanding of, for example gender distribution of the instrumentalists or the construction of single instruments. Equally first-step research into sound tools has revealed that the majority of original instrument finds are clay rattles which are found in all regions of Mesopotamia with an astonishing consistency in shape and type.The analyses and (re-)evaluations of each single record collected will further include new editions and publications of hitherto unknown as well as re-examined material in different PR journals.Lastly, the results from these examinations and evaluations will feed a database and a lexicon with individual entries to ancient Sumerian and Akkadian lemmas, as well as modern instrument names and related terminology, created and presented as an online and open access Wiki-platform (Figure 1).Figure 1: Start page of MIAM created based on MediaWiki and Semantic MediaWiki-frameworks.Scope and interdisciplinarityA special and innovative feature of MIAM is its primary thematic focus, which sets it aside from digital and online editing projects aimed at a specific corpus or genre of texts or visual media. Since MIAM primarily pursues content-related research objectives, its research material is not limited to a specific type of image or text genre but merges all types of records and, thus, heterogeneous source material. This approach is key for linking and correlating archaeological and philological data of various types and genres, combining the different contextual information to obtain as comprehensive a picture as possible of the examined material.The collection of sources will cover the archaic and early Uruk periods (ca. 3900–3500 BCE) to the Seleucid period (320-63 BCE). In geographical terms, it will be limited to Akkadian and Sumerian sources, mainly from the Sumerian and later Babylonian and Assyrian spheres of influence, which include Mesopotamia and parts of Syria. Within these boundaries, we aim to present all hitherto published textual and iconographic material referencing musical instruments. However, the detection and edition of new and previously undetected sources is limited to sifting through and researching the collections of a selection of important museums in Europe and the United States.Recent investigations, among them Gabbay (2014a/b), Mirelman (2014), Shehata (2017, 2021a/b), and Sánchez Muñoz (2021) have demonstrated that the treatment of questions of terminology, identification, and typology requires an interdisciplinary approach based on a well-founded study of textual as well as archaeological primary sources. MIAM is therefore inevitably interdisciplinary. Not only does it digitally exploit and present philological and archaeological research connecting different analyzing methods. The evaluation and interpretation of this specialized area of ancient Near Eastern history further requires the knowledge of music experts from music history, ethnomusicology, or music archaeology. The core team, therefore, constantly exchanges results and questions with external specialists, which leads to the project’s widespread networking and perception.The MIAM online database and lexiconSemantic Media WikiMIAM’s electronic and online database and lexicon were created based on MediaWiki and Semantic MediaWiki (SMW)-frameworks. Since its initial release in 2005, Semantic MediaWiki has been under active development with more than 1600 active Wikis today. It is ideally suited for the implementation of semantic database projects and offers numerous ready-to-use components to be easily adjusted to the special needs of the project.Figure 2: Pros and limitations of SMWsThere are many advantages to MediaWiki in bundle with SemanticMediaWiki: the foremost is its widespread community connected to long-term support and maintenance. A basic MediaWiki-Instance is quickly and easily installed making it usable out of the box. Once the basis is installed, which is equally achieved with ready-to-go forms and page formats, entering data takes its start. Particularly valuable for our project is the high flexibility in organizing different datasets and page hierarchies, which can be changed at any time and thus adapted at regular intervals to the requirements of the constantly growing material and emerging questions.Every entering field within a set of data may be individually defined and adjusted to the needs of its contents, e.g. whether it is mandatory, needs special characters, free text, or prescribed values such as time periods, collections or provenances. Additional components for enhancing data management, usability and the overall design are dropdowns, uploads of images and other visual data, and tokens. Other features are downloads, either of single pages, files or a specific selection of a record or lexicon information. It is also possible to define the templates that change the overall frontend of respective pages. Accordingly, this allows the previously entered data to be wrapped in numerous design containers, which improve the design and usability of the Wiki. Various ready-to-use extensions may be additionally implemented, for example, to visualize different statistics and developments, such as the temporal distribution of a particular instrument or the spatial distribution of an instrument type using a map. SemanticMediaWiki thus enables data recording and managing, enriched with semantic attributes and categories, and can be imagined as a large network which is visualized and may be explored through special queries.However, working with the Wiki throughout the project’s first year has already revealed problematic issues and limitations. Even though the input of transliterations of cuneiform text with all the necessary specifications, such as line numbering and scores, is perfectly feasible, there are no solutions for implementing annotated text, such as ATF (Annotated text Format), as is provided, e.g., in ORACC or eBL. The final presentation of the texts will, therefore, rely on linking and cross-references to the major text editing projects to enable the user single word-based exploration. The great benefit from including different extensions for visualization and advanced search are offset by inconsistencies that may appear in their implementation and the data modelling. Gladly, thanks to the great support community it was so far always possible to find alternative solutions out of the many predefined SemanticMediaWiki features and extensions. Lastly, due to the ready-to-use characteristics of SMW, special design features cannot be integrated. All this must be taken into consideration when developing a WIKI-project which is optimally adapted to the needs of the question.Since MIAM’s goals are primarily content-related and the focus is on analyzing and interpreting the records, the project’s output is not diminished by omitting annotation and lemmatization of images or texts. On the contrary, collaboration with other projects within the ever-growing digital ANE community allows for solutions to be developed more effectively. Ultimately, collaboration also prevents the loss of time due to repeating work steps that have already been successfully completed by third-party research projects. Priority was given to simple programming and design to avoid complications caused by complexity and data volume while at the same time facilitating interactions and links with other digital formats through simplicity.Setting and entering of meta dataIn creating appropriate and necessary data sets we were initially facing the challenge of including an input mask that works for all types of records, be them texts, i.e. cuneiform manuscripts of all types and contents; images, as there are depictions on various supports; or original musical instruments, the sound tools.Figure 3: Sketch of meta data sets sorted to three tabs headed “View”, “Subject”, and “Object data”The main category Record therefore provides input fields that suit all source types (Figure 3) and is presented under three different tabs. The first “View” tab presents primary information, such as the place of the first publication, but most importantly all data that are the concern of the project’s main focus, e.g. “instruments”, “technical terms”, “professions” in music or “ensemble size”. The “View” further includes detailed photographs from different perspectives, also in RTI-format, as well as technical designs, and—where deemed necessary—3D-scans of objects examined during several museum visits. Copyright issues are clarified with each museum before publication and launching on the MIAM-website. The second tab, “Subject”, presents the detailed content and discussion of the record, while additional general information is provided under “Object data”.While some of the entry fields are mandatory for reasons of the database’s hierarchical organization, such as “Signature”, an internal MIAM-numbering, or the “Type” of a record, others disappear when left empty (marked in grey in Figure 3). This is for example the case for most iconographic records where the field “Text transliteration” is blanked out in the page’s final presentation.The activated “Visual editor” offers a menu with all kinds of special characters and formatting options for facilitating text input. Even though the corresponding rendering in the “wiki editor” looks complex, it gives access to the HTML coding and allows individual adjustments.            Sound tools (ca. 464)      4th/3rd mill. BCE      Late 3rd mill. BCE      2nd mill. BCE      1st mill. BCE                  Chordophones      10                                   Idiophones      8      30      100      250              Aerophones      4      10      20      30              Membranophones      2                                       Images (ca. 332)      4th/3rd mill. BCE      Late 3rd mill. BCE      2nd mill. BCE      1st mill. BCE                  Seals      20      5      15      10              Figurines and Statues      2      60      70      60              Reliefs      10      25      30      25                  Texts (ca. 472)      4th/3rd mill. BCE      Late 3rd mill. BCE      2nd mill. BCE      1st mill. BCE                  Lists      8             21      19              Literary      4      2      55      24              Administrative      25      45      85      25              Letters                    40      15              Liturgy                    10      25              Rituals/Incantations      3             5      32              Inscriptions      4      2      13      10              In total      44      49      229      150      Table 1: Extrapolation of relevant Records, different types of sources with references to musical instrumentsOf the approximately 1450 records projected for the 2023 application (Table 1), 615, thus ca. 40%, have now been entered and transferred into Record pages in the MIAM online database. One year into the project, slight shifts in the distribution of sources are evident. This is particularly noticeable in the “Sound tools”, especially within the group of idiophones: Clay rattles, which have been widespread throughout the ancient Near East since the Neolithic period, are a veritable mass product. Contrary to previous estimates, their number already exceeds all other source groups in our project. Similarly, the number of images is increasing due to hitherto unnoticed objects in recent excavation reports and private collections. The number of texts remains stable, so far supplemented only by a few newly discovered manuscripts with textual parallels.The input mask, adjusted and defined individually for MIAM, can ultimately be seen as a cumulation of general information, such as collected for example by museum websites for all types of archaeological artifacts, but additionally presenting artefact-type-specific data, such as text transliterations or detailed image descriptions, which results in a collection of single full editions including substantive discussions.Lexicon of Akkadian and Sumerian terms and namesFigure 4: Provisional presentation of a Lexicon entry in MIAMThe research output achieved from collecting end (re-)editing Records as well as the insights gained from their systematic evaluation and interpretation will feed into the compilation of the Lexicon of Sumerian and Akkadian Instruments names as well as Technical Terms related to musical instruments, such as their parts (e.g. Sumerian a2 “arm” for the instrument’s neck), materials (e.g. Sumerian siki “wool”), their handling and sound, and additional accessories (e.g. plectrum). Lexical entries present name variants, spellings and plausible identifications, next to a general description and an outline giving historical developments, playing contexts and uses (Figure 4). This comprehensive presentation is rounded off by a bibliography, a list of attestations and a visualization of their geographical distribution using a map.Even though many terms are attested in both Sumerian and Akkadian ‘counterparts’, still where such equivalents vary and are not recognizable as uniform translations, separate entries are created for each of the Sumerian and Akkadian terms and related through internal links. Also here, changes in the meaning of names and their possible identification are evident, which is why a precise differentiation of each term, also with regard to linguistic evidence and its diversity, may indicate such developments.Navigating tools and GlossariesMIAM is not only aimed at specialists in ANE studies but is intended to be equally understandable and navigable for non-specialist scientists, as well as non-experts, be they musicians, instrument makers or undergraduates. The SMW-website therefore provides different tools to facilitate search, browsing, and visualization.Among these tools are different Glossaries, presenting proper names, places, and professions, maps displaying the distribution of instrument types in images and names in text sources. Different searching tools, such as a drop-down menu and single-word-searches that enable browsing and searching for individual modern and ancient terms, authors, or abbreviated publications. A comprehensive Bibliography contains all titles referred to on the site as well as literature relevant for all kinds of Mesopotamian instrument studies.Figure 5: Provisional overview page for Records in MIAMEach overview page presents the collection of page entries, be it in Records or Lexicon, in the form of tables displaying the basic and relevant information of a data and enhancing browsing through an alphabetical list (Figure 5).All data, whether entries in the Lexicon or text translations and discussions in Records are linked internally to Glossaries and the Bibliography, and externally to online Assyriological corpora including Archibab, BDTNS, CDLI, ETCSL, ORACC, SEAL, eBL, EPSD2, eSAD and CAD for quick access to further information.ConclusionThe Project Musical Instruments of Ancient Mesopotamia (MIAM) makes an important contribution to the cultural history of ancient Mesopotamia by unlocking previously unanswered quarrels and questions. It addresses these research gaps by drawing on the widest possible range of source materials. This will be achieved by preparing a comprehensive edition of previously unknown materials in museum collections on the one hand and the merging and evaluation of already known but mostly neglected data on the other. We expect the insights to be gained from the evaluation of these materials to contribute to a better understanding of Mesopotamia’s history and cultural development in that they will enhance collaborative research between academics in several associated fields, such as music history, anthropology, and music ethnology, as well as musicians and instrument builders. The results from this research will be presented in an online and open-access digital database created based on Semantic MediaWiki-frameworks. This MIAM webpage is designed to equally address ANE scholars as well as non-experts, undergraduate students, lecturers and teachers.As to the form of the online presentation of the project, including general and commonly understandable but also specific knowledge, MIAM could provide a prototype for topic-related digital data collecting and managing, including varied source types and genres, rather than conventional single-source-type related databases and online editions.The MIAM webpage’s launch is scheduled for spring 2028. Maintenance is guaranteed by the Center for Philology and Digitality “Kallimachos” in Würzburg with IT support that cares for updates and minor adjustments. In case the project is extended, we aim at including further collections as well as peripheral territories, such as Anatolia and the Levant, at the same time creating a scholarly community that takes care of keeping the database up-to-date.Bibliography and AbbreviationsArchibab = Archives Babyloniennes XXe – XVIIe siècles av. J.-C. (http://www.archibab.fr).BDTNS = Database of Neo-Sumerian Texts (BDTNS) (http://bdtns.filol.csic.es/).CAD = A. L. Oppenheim et al., The Assyrian Dictionary of the Oriental Institute of the University of Chicago, Glückstadt: J. J. Augustin and Chicago: Oriental Institute, Chicago, 1956–2010.CDLI = Cuneiform Digital Library Initiative (https://cdli.mpiwg-berlin.mpg.de/).Cheng, J., 2001, Assyrian Music as Represented and Representations of Assyrian Music, Cambridge, Mass. Harvard Univ., Diss.Dumbrill, R. J., 1998, The Musicology and Organology of the Ancient Near East, London: Tadema Press.–– 2005, The Archaeomusicology of the Ancient Near East, Bloomington: Trafford Publishing.–– 2007, Idiophones of the ancient Near East in the collections of the British Museum, Morrisville, NC: British Museum.eBL = electronic Babylonian Literature (https://www.ebl.lmu.de/).Eichmann, R., M. Howell &amp;amp; G. Lawson, 2019, Introduction: Music, Social Identity, Political Cohesion, in: R. Eichmann, M. Howeel, G. Laswon (eds.), Music and politics in the Ancient World: exploring identity, agency, stability and change through the records of Music Archaeology, Berlin, Edition Topoi, 15–30.EPSD2 = The Pennsylvania Sumerian Dictionary (http://oracc.museum.upenn.edu/epsd2/).eSAD = Supplement to the Akkadian Dictionaries (https://www.gkr.uni-leipzig.de/altorientalisches-institut/forschung/supplement-to-the-akkadian-dictionaries).ETCSL = The Electronic Text Corpus of Sumerian Literature (http://etcsl.orinst.ox.ac.uk/).Franklin, J. C., 2015, Kinyras: The Divine Lyre, Hellenistic Studies 70, Washington DC: Center for Hellenistic Studies, trustees for Harvard University.Gabbay, U., 2010, The Ancient Mesopotamian Sistrum and Its References in Cuneiform Literature: The Identification of the šem and the meze, in: I. L. Finkel and R. Dumbrill (eds.), ICONEA 2008: Proceedings of the International Conference of Near Eastern Archaeomusicology, London: ICONEA Publications 2010, 23-28–– 2014a, The Balaĝ Instrument and its Role in the Cult of Ancient Mesopotamia, in: J. Goodnick-Westenholz, Y. Maurey, and E. Seroussi (eds.), Music in Antiquity: The Near East and the Mediterranean, Yuval – Studies of the Jewish Music Research Centre 8, 129-147.–– 2014b, Pacifying the Hearts of the Gods: Sumerian Emesal Prayers of the First Millennium BC, Heidelberger Emesal-Studien 1, Wiesbaden: Harrassowitz.Galpin. F. W., 1936, The Music of the Sumerians and their Immediate Successors the Babylonians and Assyrians: Described and Illustrated from Original Sources, 1st edition, Cambridge University Press. Following editions: 1955 and 1972: Verlag Valenton Koerner: Baden-Baden.von Hornbostel, E. M. &amp;amp; C. Sachs, 1914, Systematik der Musikinstrumente, in: Zeitschrift für Ethnologie 46, Heft 4-5, 553–590.Kartomi, M., 1990, On Concepts and Classifications of Musical Instruments, Chicago: University of Chicago Press.Kilmer, A. D., 1993-1997, Musik. A.I. In Mesopotamia, in: Reallexikon der Assyriologie und Vorderasiatischen Achäologie (RlA) 8, Berlin, New York: De Gruyter, 463–482.Koch, L.-C. &amp;amp; R. Kopal, 2014, Klassifikation von Musikinstrumenten – Zum 100-jährigen Bestehen der Hornbostel-Sachs-Systematik, Zeitschrift für Ethnologie, Band 139, Heft 2, 281–302.Krispijn, Th. J. H., 1990, Beiträge zur altorientalischen Musikforschung. 1. Šulgi und die Musik, in: Akkadica 70, 1–27.Lawson, G., 2020, The Mammoth in the Room. Did Musical Necessity Drive Innovation in Ancient Technology? In: G. Kolltveit, R. Rainio (eds.), The Archaeology of Sound, Acoustics &amp;amp; Music. Studies in Honour of Cajsa S. Lund, Berlin: ēkhō, 117–150.Mirelman, S., 2014, The Ala-instrument: Its Identification and Role, in: J. Goodnick-Westenholz, Y. Maurey, &amp;amp; E. Seroussi (eds.), Music in Antiquity: The Near East and the Mediterranean, Yuval – Studies of the Jewish Music Research Centre 8, 148–171.ORACC = The Open Richly Annotated Cuneiform Corpus (http://oracc.museum.upenn.edu/).Orlamünde, J., 2011, Die Obeliskenfragmente aus Assur, Wissenschaftliche Veröffentlichungen der Deutschen Orient-Gesellschaft 135, Wiesbaden: Harrassowitz.Pruß A., 1999, Glöckchen, Rasseln, Pfeifen. Musikinstrumente aus Ton, Hallesche Beiträge zur Orientwissenschaft 28/140, 56−87.Rashīd, S. A., 1984, Musikgeschichte in Bildern, Mesopotamien, Band 2, Leipzig: Dt. Verl. für Musik.Sánchez Muñoz, D. 2021. Encore des percussions! Observations sur /tigi/ et /adab/, Pallas 115, 149–170.Schauensee, M. de, 2002, Two Lyres from Ur. Philadelphia: University of Pa. Museum of Archaeology and Anthropology.Schmidt-Colinet, C., 1981, Die Musikinstrumente in der Kunst des Alten Orients: Archäologisch-Philologische Studien, Bonn: Bouvier.SEAL = Sources of Early Akkadian Literature (http://www.seal.uni-leipzig.de/)Shehata, D., 2014, Sounds from the Divine: Religious Musical Instruments in the Ancient Near East, in: J. Goodnick-Westenholz, Y. Maurey and E. Seroussi (eds.), Music in Antiquity: The Near East and the Mediterranean, Yuval – Studies of the Jewish Music Research Centre 8, 102–128.–– 2017, Eine Mannshohe Leier im altbabylonischen Ištar-Ritual aus Mari (FM 3, no. 2), Altorientalische Forschungen 44/1 (2018), 68–81.–– 2021a, En marge d’ARCHIBAB, 36: à propos des instruments de musique halhallatum de CUSAS 40 1963. Nouvelles assyriologiques brèves et utilitaires 4 (103).–– 2021b, Musikinstrumente im Lederarchiv von Isin, in: C. Bühring et al. (eds.), Klänge der Archäologie. Festschrift für Ricardo Eichmann. Wiesbaden: Harrassowitz, 2021, 417–429.Strommenger, E &amp;amp; P.A. Miglus, 2010, Altorientalische Kleinfunde, Wissenschaftliche Veröffentlichungen der Deutschen Orient-Gesellschaft 126, Wiesbaden: Harrassowitz.Ziegler, N., 2007, Les musiciens et la musique d’après les archives de Mari, FM IX / Mémoires de N.A.B.U. 10, Paris: SEPOA."
    } ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} ,

  

  

  
    {} 

  

  
]